{"meta":{"title":"彩音のBlog","subtitle":null,"description":"彩音","author":"彩音","url":""},"pages":[{"title":"about","date":"2020-05-30T15:58:46.000Z","updated":"2020-09-28T14:12:00.000Z","comments":true,"path":"about/index.html","permalink":"/about/index.html","excerpt":"","text":"与&nbsp; 彩音 对话中... bot_ui_ini()"},{"title":"bangumi","date":"2019-02-10T13:32:48.000Z","updated":"2019-11-15T16:29:48.000Z","comments":false,"path":"bangumi/index.html","permalink":"/bangumi/index.html","excerpt":"","text":"","keywords":null},{"title":"美术馆","date":"2021-04-22T07:53:46.000Z","updated":"2021-07-11T07:23:22.000Z","comments":true,"path":"gallery/index.html","permalink":"/gallery/index.html","excerpt":"","text":"美术馆内容均摘自画师的社交帐号。作品仅供欣赏，不得用于商业用途。 BUNBUN传送门：@BUNBUN922 光崎传送门：@kousaki_r ぽんかん⑧传送门：@ponkan_8 Tiv传送门：@tiv_ wlop传送门：@wlopwangling"},{"title":"rss","date":"2018-12-20T15:09:03.000Z","updated":"2019-11-15T16:29:48.000Z","comments":true,"path":"rss/index.html","permalink":"/rss/index.html","excerpt":"","text":""},{"title":"相册","date":"2021-08-27T14:10:00.000Z","updated":"2021-09-22T04:48:52.000Z","comments":true,"path":"photo/index.html","permalink":"/photo/index.html","excerpt":"","text":"福冈 北海道 鹿儿岛 熊本 长崎 ……"},{"title":"theme-sakura","date":"2019-01-04T14:53:25.000Z","updated":"2019-11-15T16:29:48.000Z","comments":false,"path":"theme-sakura/index.html","permalink":"/theme-sakura/index.html","excerpt":"","text":"Hexo主题Sakura修改自WordPress主题Sakura，感谢原作者Mashiro","keywords":"Hexo 主题 Sakura 🌸"},{"title":"歌单","date":"2020-05-09T07:58:46.000Z","updated":"2021-04-30T07:53:50.000Z","comments":true,"path":"musiclist/index.html","permalink":"/musiclist/index.html","excerpt":"","text":"女声粤语谢安琪 薛凯琪 日语Aimer LiSA Uru 福原遥 Collection 男声国语陈奕迅 李宗盛 粤语麦浚龙 * 因为版权限制及试听限制现在能听的歌真的好少啊😢"},{"title":"tags","date":"2018-12-12T14:14:16.000Z","updated":"2019-11-15T16:29:48.000Z","comments":true,"path":"tags/index.html","permalink":"/tags/index.html","excerpt":"","text":""},{"title":"video","date":"2018-12-20T15:14:38.000Z","updated":"2019-11-15T16:29:48.000Z","comments":false,"path":"video/index.html","permalink":"/video/index.html","excerpt":"","text":"var videos = [ { img: 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '放送时间: 2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' }, { img : 'https://lain.bgm.tv/pic/cover/l/0e/1e/218971_2y351.jpg', title: '朝花夕誓——于离别之朝束起约定之花', status: '已追完', progress: 100, jp: 'さよならの朝に約束の花をかざろう', time: '2018-02-24 SUN.', desc: ' 住在远离尘嚣的土地，一边将每天的事情编织成名为希比欧的布，一边静静生活的伊欧夫人民。在15岁左右外表就停止成长，拥有数百年寿命的他们，被称为“离别的一族”，并被视为活着的传说。没有双亲的伊欧夫少女玛奇亚，过着被伙伴包围的平稳日子，却总感觉“孤身一人”。他们的这种日常，一瞬间就崩溃消失。追求伊欧夫的长寿之血，梅萨蒂军乘坐着名为雷纳特的古代兽发动了进攻。在绝望与混乱之中，伊欧夫的第一美女蕾莉亚被梅萨蒂带走，而玛奇亚暗恋的少年克里姆也失踪了。玛奇亚虽然总算逃脱了，却失去了伙伴和归去之地……。' } ] .should-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:95%;}.should-ellipsis-full{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;width:100%;}.should-ellipsis i{position:absolute;right:24px;}.grey-text{color:#9e9e9e !important}.grey-text.text-darken-4{color:#212121 !important}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}img{border-style:none}progress{display:inline-block;vertical-align:baseline}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{-webkit-box-sizing:inherit;box-sizing:inherit}ul:not(.browser-default){padding-left:0;list-style-type:none}ul:not(.browser-default)>li{list-style-type:none}.card{-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2);box-shadow:0 2px 2px 0 rgba(0,0,0,0.14),0 3px 1px -2px rgba(0,0,0,0.12),0 1px 5px 0 rgba(0,0,0,0.2)}.hoverable{-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s}.hoverable:hover{-webkit-box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19);box-shadow:0 8px 17px 0 rgba(0,0,0,0.2),0 6px 20px 0 rgba(0,0,0,0.19)}i{line-height:inherit}i.right{float:right;margin-left:15px}.bangumi .right{float:right !important}.material-icons{text-rendering:optimizeLegibility;-webkit-font-feature-settings:'liga';-moz-font-feature-settings:'liga';font-feature-settings:'liga'}.row{margin-left:auto;margin-right:auto;margin-bottom:20px}.row:after{content:\"\";display:table;clear:both}.row .col{float:left;-webkit-box-sizing:border-box;box-sizing:border-box;padding:0 .75rem;min-height:1px}.row .col.s12{width:100%;margin-left:auto;left:auto;right:auto}@media only screen and (min-width:601px){.row .col.m6{width:50%;margin-left:auto;left:auto;right:auto}}html{line-height:1.5;font-family:-apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif;font-weight:normal;color:rgba(0,0,0,0.87)}@media only screen and (min-width:0){html{font-size:14px}}@media only screen and (min-width:992px){html{font-size:14.5px}}@media only screen and (min-width:1200px){html{font-size:15px}}.card{position:relative;margin:.5rem 0 1rem 0;background-color:#fff;-webkit-transition:-webkit-box-shadow .25s;transition:-webkit-box-shadow .25s;transition:box-shadow .25s;transition:box-shadow .25s,-webkit-box-shadow .25s;border-radius:2px}.card .card-title{font-size:24px;font-weight:300}.card .card-title.activator{cursor:pointer}.card .card-image{position:relative}.card .card-image img{display:block;border-radius:2px 2px 0 0;position:relative;left:0;right:0;top:0;bottom:0;width:100%}.card .card-content{padding:24px;border-radius:0 0 2px 2px}.card .card-content p{margin:0}.card .card-content .card-title{display:block;line-height:32px;margin-bottom:8px}.card .card-content .card-title i{line-height:32px}.card .card-reveal{padding:24px;position:absolute;background-color:#fff;width:100%;overflow-y:auto;left:0;top:100%;height:100%;z-index:3;display:none}.card .card-reveal .card-title{cursor:pointer;display:block}.waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent;vertical-align:middle;z-index:1;-webkit-transition:.3s ease-out;transition:.3s ease-out}.waves-effect img{position:relative;z-index:-1}.waves-block{display:block}::-webkit-input-placeholder{color:#d1d1d1}::-moz-placeholder{color:#d1d1d1}:-ms-input-placeholder{color:#d1d1d1}::-ms-input-placeholder{color:#d1d1d1}[type=\"radio\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"radio\"]:not(:checked)+span{position:relative;padding-left:35px;cursor:pointer;display:inline-block;height:25px;line-height:25px;font-size:1rem;-webkit-transition:.28s ease;transition:.28s ease;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border-radius:50%}[type=\"radio\"]:not(:checked)+span:before,[type=\"radio\"]:not(:checked)+span:after{border:2px solid #5a5a5a}[type=\"radio\"]:not(:checked)+span:after{-webkit-transform:scale(0);transform:scale(0)}[type=\"checkbox\"]:not(:checked){position:absolute;opacity:0;pointer-events:none}[type=\"checkbox\"]:not(:checked):disabled+span:not(.lever):before{border:none;background-color:rgba(0,0,0,0.42)}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):before{width:0;height:0;border:3px solid transparent;left:6px;top:10px;-webkit-transform:rotateZ(37deg);transform:rotateZ(37deg);-webkit-transform-origin:100% 100%;transform-origin:100% 100%}[type=\"checkbox\"].filled-in:not(:checked)+span:not(.lever):after{height:20px;width:20px;background-color:transparent;border:2px solid #5a5a5a;top:0px;z-index:0}input[type=checkbox]:not(:disabled) ~ .lever:active:before,input[type=checkbox]:not(:disabled).tabbed:focus ~ .lever::before{-webkit-transform:scale(2.4);transform:scale(2.4);background-color:rgba(0,0,0,0.08)}input[type=range].focused:focus:not(.active)::-webkit-slider-thumb{-webkit-box-shadow:0 0 0 10px rgba(38,166,154,0.26);box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-moz-range-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)}input[type=range].focused:focus:not(.active)::-ms-thumb{box-shadow:0 0 0 10px rgba(38,166,154,0.26)} 番组计划 这里将是永远的回忆 window.onload = function(){ videos.forEach(function(video, i){ $('#rootRow').append(` ${video.title} ${video.jp} ${video.status} ${video.title} ${video.jp} 放送时间: ${video.time} ${video.desc} ${video.status} `) }) }","keywords":"B站"}],"posts":[{"title":"CS224N - 语言模型","slug":"CS224N - 语言模型","date":"2021-11-26T11:50:00.000Z","updated":"2021-11-26T12:42:31.960Z","comments":true,"path":"2021/11/26/CS224N - 语言模型/","link":"","permalink":"/2021/11/26/CS224N - 语言模型/","excerpt":"","text":"CS224N - 语言模型##","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"python","slug":"python","permalink":"/tags/python/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"CS224N - 单词的含义表示","slug":"CS224N - 单词的含义表示","date":"2021-11-23T11:00:00.000Z","updated":"2021-11-26T07:21:55.869Z","comments":true,"path":"2021/11/23/CS224N - 单词的含义表示/","link":"","permalink":"/2021/11/23/CS224N - 单词的含义表示/","excerpt":"","text":"Lecture 11 单词的含义表示1.1 单词的数据库 - WordNet待解决问题：如何表示单词的含义 描述：WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. 优点： 词性标注 同义归类 词间关系 缺点： 缺少语境 缺少新的词汇和已有词汇的新含义 依赖人工标注 无法计算单词相似度（无法向量化） 1.2 单词的稀疏向量表示 - One-hot Vector待解决问题：如何表示单词的含义 描述：在传统NLP中，我们将不同的单词视为独立的个体。我们可以用 one-hot 编码的形式去表示他们，如下所示。向量的维度将会是词汇量的大小。 motel = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]hotel = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] 缺点： one-hot 编码的向量都是正交的，无法计算相似度。 1.3 单词的稠密向量表示 - Word Vector待解决问题：如何表示单词的含义-如何计算单词相似度 分布式语义：假设一个单词的含义取决于其高频出现的上下文。 描述：通过多个含有单词 w 的上下文建立起单词 w 的表示。这个表示是一个稠密向量，可被称为词表示或者词嵌入，这是一种分布式表示。 优点： 在这种分布式语义中，所得到的稠密向量可以用于计算单词相似度，即在向量空间上同义词距离近，反义词距离远。 1.4 Word Vector 的具体实现 - word2vec待解决问题：如何表示单词的含义-如何计算单词相似度 描述： 根据 word vector 的思想，我们将基于上下文用稠密向量表示一个单词，这样可以在表示单词含义的同时保留单词之间的相关性信息。 对于我们的语料库中任意位置的单词 c（center），有上下文单词集合 o（outside）。 用单词 c 以及上下文单词集合 o 中的单词的 word vector 的相似度去计算 p(c | o) 以及 p(o | c)。同一单词，对于出现在上下文中的该单词以及出现在 center 位的该单词，我们用两个向量 *1 分别表示。 调整各单词的词向量使概率最大。 *1 可以使用单个向量表示，但对于同一单词同时出现在 center 和上下文，会出现 $ x^{T}x $ 的情况，会增加梯度下降难度。 损失函数：$$\\begin{equation}J(\\theta)=-\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m \\atop j \\neq 0} \\log P\\left(w_{t+j} \\mid w_{t} ; \\theta\\right)\\end{equation}$$损失函数会计算每一个位置 t 周围的 2m 个单词出现的概率。损失函数越小，概率越大。 条件概率：$$\\begin{equation}P(o \\mid c)=\\frac{\\exp \\left(u_{o}^{T} v_{c}\\right)}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}\\end{equation}$$ 条件概率的计算中，对上下文单词和中心单词词向量的点乘做了softmax正则化，即对于同一个中心单词，上下文单词词向量与中心单词词向量的点乘结果越大，其条件概率占整个上下文单词概率空间的比重越多。 优点： 模型可以继承 1.3 单词的稠密向量表示 - Word Vector 的思路进行自我迭代。 迭代后得到的词向量能够计算单词相似度，且单词之间一定程度上具有自然语言的逻辑叠加性质，即经典示例 $ V_{king} - V_{man} + V_{woman} = V_{queen} $ Lecture 21 单词的含义表示1.5 word2vec 的两种形式 - Skip-grams 和 Continuous Bag of Words待解决问题：如何表示单词的含义-如何计算单词相似度 word2vec 的两种形式： Skip-grams：给出中心单词预测上下文单词。（常见） CBoW：给出上下文预测中心单词。 1.6 word2vec 的高效实现 - Negative Sampling待解决问题：如何表示单词的含义-如何计算单词相似度-如何提高 word2vec 损失函数计算效率 描述： 在 1.4 Word Vector 的具体实现 - word2vec 中给出的条件概率计算方式在单词维度很大的时候，计算开销极大。在实际操作中使用随机负采样单词和中心单词配对，提升上下文单词词向量与中心单词词向量的点乘大小，降低随机采样单词与中心单词的点乘大小。 损失函数：$$J(\\theta)=\\frac{1}{T} \\sum_{t=1}^{T} J_{t}(\\theta)$$其中，$$J_{t}(\\theta)=-\\log \\sigma(u_{o}^{T} v_{c})-\\sum_{i=1}^{k} E_{j \\sim P(w)}[\\log \\sigma(-u_{j}^{T} v_{c})]$$ $$\\sigma(x)=\\frac{1}{1+e^{-x}}$$ 损失函数的第一部分，上下文单词词向量与中心单词词向量点乘越大，损失函数越小。损失函数第二部分，随机负采样的k个单词的词向量与中心单词词向量点乘越小损失函数越小。 优点： 相比原先的softmax需要在每次条件概率中计算整个单词表，负采样后的损失函数只需要计算上下文单词和经过随机负采样选出的单词，计算开销大幅下降。 1.7 Word Vector 的简单实现 - 共现矩阵待解决问题：如何表示单词的含义-如何计算单词相似度 描述： 通过单词间相邻次数（出现在对方相邻 n 个单词中的次数）构建整个词汇表中单词间的共现矩阵，并用矩阵的 行向量/列向量 表示该单词。 示例（语料库）： I like deep learning. I like NLP. I enjoy flying. 出现次数 I like enjoy deep learning NLP flying . I 0 2 1 0 0 0 0 0 like 2 0 0 1 0 1 0 0 enjoy 1 0 0 0 0 0 1 0 deep 0 1 0 0 1 0 0 0 learning 0 0 0 1 0 0 0 1 NLP 0 1 0 0 0 0 0 1 flying 0 0 1 0 0 0 0 1 . 0 0 0 0 1 1 1 0 各单词对应词向量： 单词 向量 I [0, 2, 1, 0, 0, 0, 0, 0] like [2, 0, 0, 1, 0, 1, 0, 0] enjoy [1, 0, 0, 0, 0, 0, 1, 0] deep [0, 1, 0, 0, 1, 0, 0, 0] learning [0, 0, 0, 1, 0, 0, 0, 1] NLP [0, 1, 0, 0, 0, 0, 0, 1] flying [0, 0, 1, 0, 0, 0, 0, 1] . [0, 0, 0, 0, 1, 1, 1, 0] 最终，根据上下文单词出现的频率，相似的单词（词法/含义 相似）会具有高相似度的词向量。 优点： 实现逻辑简单，不需要训练。 缺点： 空间占用高，随着词汇量增大而增大。（解决方法：数学降维方法 / 预处理词库） 1.8 GloVe - 共现矩阵和word2vec的结合待解决问题：如何表示单词的含义-如何计算单词相似度-如何建立一个时空高效且效果好的模型 现有模型优缺点： 共现矩阵 word2vec 特点 基于单词出现次数 基于概率预测 优点 基于统计学，无需训练，时间高效 能够捕捉单词相似度之外的复杂特征 能在 downstream 任务中获得更好的效果 缺点 仅可用于捕捉单词相似度 随着语料库的增加，训练过程耗时加长，效率降低 对高频词汇分配了过多的权重 GloVe核心思想： 通过共现概率之比编码一些有用信息 用词向量点乘拟合共现概率 $ x = $ $ solid $ $ gas $ $ water $ $ random $ $ \\begin{equation}P(x&#124;ice)\\end{equation} $ 高 低 高 低 $ P(x&#124;steam) $ 低 高 高 低 $ \\frac{P(x&#124;ice)}{P(x&#124;steam)} $ 高 低 ~1 ~1 &amp;#124; 可用于转义 | 。 共现概率之比可以编码一些成分信息（类比），如 $ V_{king} - V_{man} + V_{woman} = V_{queen} $ 为了在训练中使得向量具有这种可叠加的特点，我们让词向量点乘拟合共现概率。$$w_{i} \\cdot w_{j}=\\log P(i \\mid j) \\tag{1}$$ $$w_{x} \\cdot\\left(w_{a}-w_{b}\\right)=\\log \\frac{P(x \\mid a)}{P(x \\mid b)}\\tag{2}$$ 通过 公式1 对于共现概率的拟合，词向量将拥有 公式2 的可叠加性，并将共现概率之比的信息保留在词向量中。 损失函数：$$J=\\sum_{i, j=1}^{V} f(X_{i j})(w_{i}^{T} w_{j}+b_{i}+b_{j}-\\log X_{i j})^{2}$$ $ f(x) $ 是一个权重函数：$$\\begin{equation}f(x) =\\begin{cases}(x / x_{\\max })^{\\alpha} &amp; \\text { if } x&lt;x_{\\max }\\newline 1 &amp; \\text { otherwise }\\end{cases}\\end{equation}$$其中，$ x_{\\max } $ 是共现次数的最大值，$ \\alpha $ 的典型值为 $ \\frac{3}{4} $ 。我们希望给予高频共现单词对更高的权重，但不像 1.7 Word Vector 的简单实现 - 共现矩阵 中高频共现词汇对那样过高的权重。 1.9 如何评价 Word Vector 的好坏？ Intrinsic Evaluation：基于直接的词汇对，如 比较级 / 过去式。 Extrinsic Evaluation：基于真实的任务场景，如 命名实体识别。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"/tags/NLP/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"Python的内置类型","slug":"Python的内置类型","date":"2021-11-17T03:46:00.000Z","updated":"2021-11-26T07:32:12.378Z","comments":true,"path":"2021/11/17/Python的内置类型/","link":"","permalink":"/2021/11/17/Python的内置类型/","excerpt":"","text":"本文参考[1] 官方 Python3 教程[2] RUNOOB Python3 教程 Python的内置类型逻辑值 被定义为假值的常量: None 和 False。 任何数值类型的零: 0, 0.0, 0j, Decimal(0), Fraction(0, 1) 空的序列和多项集: &#39;&#39;, (), [], {}, set(), range(0) 逻辑运算 逻辑运算 描述 x or y 或 x and y 与 not x 非 比较运算 比较运算 描述 &lt; 严格小于 &lt;= 小于等于 &gt; 严格大于 &gt;= 大于等于 == 等于 != 不等于 成员运算及身份运算 运算 描述 in 包含于 not in 不包含于 is 相同对象 is not 不相同对象 &gt;&gt;&gt; a = [1, 2, 3] &gt;&gt;&gt; b = a &gt;&gt;&gt; b == a True &gt;&gt;&gt; b is a True &gt;&gt;&gt; b = a[:] &gt;&gt;&gt; b == a True &gt;&gt;&gt; b is a False b = a 中，列表的赋值采用的是引用赋值，两者不光数值相同，指针指向也相同。其中之一的列表元素发生改变，另一个也会跟着改变。此时，a 和 b 为相同对象。 b = a[:] 中，[:] 表示只取值，此时列表的赋值只是单纯的复制了数值，两者的数值虽然相同，但指针指向不同。其中之一的列表元素发生改变，另一个不会跟着改变。此时，a 和 b 为不同对象。 数字类型及运算符数字类型 intx = 5 floatx = 5.0 complexx = 5 + 5.0j &gt;&gt;&gt; x = [] &gt;&gt;&gt; x.append(5);x.append(5.1);x.append(5+5.1j) &gt;&gt;&gt; x [5, 5.1, (5+5.1j)] 数字运算 数字运算 描述 x + y 加 x - y 减 x * y 乘 x / y 除 x // y 整除（向下取整） x % y 取余 x ** y x 的 y 次幂 pow(x, y) x 的 y 次幂 abs(x) 取绝对值 int() 整数强制类型转换 float() 浮点数强制类型转换 complex(real, imag) 复数 divmod(x, y) (x // y, x % y) round(x[, n]) 将x保留n位小数（四舍五入） math.floor(x) &lt;=x的最大整数 math.ceil(x) &gt;=x的最小整数 位运算 位运算 描述 x y x ^ y 按位异或 x &amp; y 按位与 x &lt;&lt; n 左移n位 x &gt;&gt; n 右移n位 ~x 按位取反 序列类型通用序列操作 通用序列操作 描述 x in s 如果 s 中的某项等于 x 则结果为 True，否则为 False x not in s 如果 s 中的某项等于 x 则结果为 False，否则为 True s + t s 与 t 相拼接 s * n 相当于 s 与自身进行 n 次拼接 s[i] s 的第 i 项，起始为 0 s[i:j] s 从 i 到 j 的切片 s[i:j:step] s 从 i 到 j 步长为 k 的切片 len(s) s 的长度 min(s) s 的最小项 max(s) s 的最大项 s.index(x[, i[, j]]) x 在 s 中首次出现项的索引号（索引号在 i 或其后且在 j 之前） s.count(x) x 在 s 中出现的总次数 可变序列操作 可变序列操作 描述 s[i] = x 将 s 的第 i 项替换为 x s[i:j] = t 将 s 从 i 到 j 的切片替换为可迭代对象 t 的内容 del s[i:j] 等同于 s[i:j] = [] s.append(x) 将 x 添加到序列的末尾 (等同于 s[len(s):len(s)] = [x]) s.clear() 从 s 中移除所有项 (等同于 del s[:]) s.copy() 创建 s 的浅拷贝 (等同于 s[:]) s.extend(t) 用 t 的内容扩展 s (基本上等同于 s[len(s):len(s)] = t) s.insert(i, x) 在由 i 给出的索引位置将 x 插入 s (等同于 s[i:i] = [x]) s.pop(*[, i]) 提取在 i 位置上的项，并将其从 s 中移除 s.remove(x) 删除 s 中第一个 s[i] 等于 x 的项目。 s.reverse() 就地将列表中的元素逆序。 列表列表是可变序列，通常用于存放同类项目的集合（其中精确的相似程度将根据应用而变化）。 class list([iterable])可以用多种方式构建列表： 使用一对方括号来表示空列表: [] 使用方括号，其中的项以逗号分隔: [a] / [a, b, c] 使用列表推导式: [x for _ in iterable] 使用类型的构造器: list() / list(iterable) list.sort(\\*, key=None, reverse=False)此方法会对列表进行原地排序，保证稳定。 &gt;&gt;&gt; list = [1, 5, 3, 4, 6, 2] &gt;&gt;&gt; list.sort() &gt;&gt;&gt; list [1, 2, 3, 4, 5, 6] &gt;&gt;&gt; list = [[1, 5, 3], [4, 6, 2]] &gt;&gt;&gt; list.sort(key=lambda x:x[2]) &gt;&gt;&gt; list [[4, 6, 2], [1, 5, 3]] 元组元组是不可变序列，通常用于储存异构数据的多项集（例如由 enumerate() 内置函数所产生的二元组）。 元组也被用于需要同构数据的不可变序列的情况（例如允许存储到 set 或 dict 的实例）。 class tuple([iterable])可以用多种方式构建元组： 使用一对圆括号来表示空元组: () 使用一个后缀的逗号来表示单元组: a, / (a,) 使用以逗号分隔的多个项: a, b, c / (a, b, c) 使用内置的tuple(): tuple() / tuple(iterable) range 对象range 类型表示不可变的数字序列，通常用于在 for 循环中循环指定的次数。 class range(stop)class range(start, stop[, step])range 构造器的参数必须为整数。 如果省略 step 参数，其默认值为 1。 如果省略 start 参数，其默认值为 0。 如果 step 为零则会引发 ValueError。 &gt;&gt;&gt; list(range(10)) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] &gt;&gt;&gt; list(range(1, 11)) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] &gt;&gt;&gt; list(range(0, 30, 5)) [0, 5, 10, 15, 20, 25] &gt;&gt;&gt; list(range(0, 10, 3)) [0, 3, 6, 9] &gt;&gt;&gt; list(range(0, -10, -1)) [0, -1, -2, -3, -4, -5, -6, -7, -8, -9] &gt;&gt;&gt; list(range(0)) [] &gt;&gt;&gt; list(range(1, 0)) [] 文本序列类型在 Python 中处理文本数据是使用 str 对象，也称为 字符串。字符串是由 Unicode 码位构成的不可变序列。 class str(object=&#39;&#39;)class str(object=b&#39;&#39;, encoding=&#39;utf-8&#39;, errors=&#39;strict&#39;)可以用多种方式构建： 使用引号：&#39;&#39; /&quot;&quot; /&#39;&#39;&#39; &#39;&#39;&#39; / &quot;&quot;&quot; &quot;&quot;&quot; 使用内置的str()：str() / str(object) 使用raw禁止转义：r&#39;\\n&#39; str的方法 描述 str.capitalize() 将str首字母大写 str.find(s) 检测字符串s是否在str中，真返回True，假返回False str.is?() 检查str是否为?，真返回True，假返回False str.join(s) 以str为分割，将s中的元素合并为一个新的字符串 str.lower() / str.upper() 将str全部字符转为小写/大写 str.lstrip(s) / str.rstrip(s) / str.strip(s) 将str 左侧/右侧/两侧 的字符串s去掉 str.replace(old, new[, max]) 将str中的字符串old替换为new，最大次数为max str.split(sep, maxsplit=-1) 将str以sep中的元素为基准进行分割，最多分割maxsplit次，得到maxsplit段 str.startwith(startstr, beg=0, end=len(string)) 检查str是否在(beg, end)区间以startstr作为开头 str.swapcase() 将str的 大/小 写字母转换为 小/大 写字母 str.title() 将str中所有单词首字母大写 str.is?()： str.is?() 中的 ? 描述 alnum 是否只包含字母或数字 alpha 是否只包含字母 digit 是否只包含数字 lower 是否只包含小写字母（只考虑区分大小写的） numeric 是否只包含数字字符 space 是否只包含空格 upper 是否只包含大写字母（只考虑区分大小写的） 字符串格式化str.format() 二进制序列类型（待补充）集合类型 - set, frozenset 对象是由具有唯一性的 hashable 对象所组成的无序多项集。常见的用途包括成员检测、从序列中去除重复项以及数学中的集合类计算，例如交集、并集、差集与对称差集等等。目前有两种内置集合类型：set 和 frozenset。 set 类型是可变的。其内容可以使用 add() 和 remove() 这样的方法来改变。由于是可变类型，它没有哈希值，且不能被用作字典的键或其他集合的元素。 frozenset 类型是不可变并且为 hashable 。其内容在被创建后不能再改变；因此它可以被用作字典的键或其他集合的元素。 class set([iterable])class frozenset([iterable])返回一个新的 set 或 frozenset 对象，其元素来自于 iterable。集合的元素必须为 hashable 要表示由集合对象构成的集合，所有的内层集合必须为 frozenset 对象。如果未指定 iterable，则将返回一个新的空集合。集合可用多种方式来创建： 使用花括号内以逗号分隔元素的方式:{&#39;jack&#39;, &#39;sjoerd&#39;} 使用集合推导式:{c for c in &#39;abracadabra&#39; if c not in &#39;abc&#39;} 使用类型构造器:set() / set(&#39;foobar&#39;) / set([&#39;a&#39;, &#39;b&#39;, &#39;foo&#39;]) set的操作 描述 len(set) 返回set中的元素数量 element in set / element not in set 判断element是否出现在set中 set.add(element) 将element加入set set.clear() 清空set set.copy() 返回set的浅拷贝 setx.difference(sety) 返回包含在setx但不包含在sety的元素 set.discard(element) 移除set中的指定element（element不存在不报错） set.intersection(set) 返回两个集合的交集 set.isjoint(set) 判断两个集合是否有交集 setx.issubset(sety) 判断setx是否为sety的子集 setx.issuperset(sety) 判断setx是否为sety的父集 set.pop() 随机移除set中的元素，返回该元素 set.remove(element) 移除set中的指定element（element不存在报错） setx.symmetric_difference(sety) 返回setx和sety中不重复的元素 set.union(set) 返回两个集合的并集 setx.update(sety) 将setx置为两个集合的并集 映射类型mapping 对象会将 hashable 值映射到任意对象。 映射属于可变对象。目前仅有一种标准映射类型 字典。字典的键几乎可以是任何值。非 hashable 的值，即包含列表、字典或其他可变类型的值（此类对象基于值而非对象标识进行比较）不可用作键。数字类型用作键时遵循数字比较的一般规则：如果两个数值相等 (例如 1 和 1.0) 则两者可以被用来索引同一字典条目。 class dict(\\*\\*kwarg)class dict(mapping, \\*\\*kwarg)class dict(iterable, \\*\\*kwarg)返回一个新的字典，基于可选的位置参数和可能为空的关键字参数集来初始化。字典可用多种方式来创建： 使用花括号内以逗号分隔 键: 值 对的方式: {&#39;jack&#39;: 4098, &#39;sjoerd&#39;: 4127} or {4098: &#39;jack&#39;, 4127: &#39;sjoerd&#39;} 使用字典推导式: {}, {x: x ** 2 for x in range(10)} 使用类型构造器: dict() / dict([(&#39;foo&#39;, 100), (&#39;bar&#39;, 200)]) / dict(foo=100, bar=200) dict的操作 描述 list(dict) 返回dict所有key的列表 len(dict) 返回dict的长度 dict[key] 返回dict中key的value，若key不存在则报错 dict[key] = value 将key增加至dict中并赋值value del dict[key] 移除dict中的key，若key不存在则报错 key in dict / key not in dict 判断key是否在dict中 dict.clear() 清空dict dict.copy() 返回dict的浅拷贝 dict.fromkeys(iterable[, value]) 通过iterable类型的元素建立列表 dict.get(key[, default]) 如果key存在则返回key对应的value，如果key不存在则返回default的值，default默认为None dict.items() 以((key, value), ( , ), …)的形式返回dict的键值对 dict.keys() 以视图对象dict_keys([key1, key2, …])的形式返回dict的keys dict.pop(key) 返回dict中key的value，并将该key移除（key不存在则报错） dict.popitem() 以LIFO（Last In First Out）顺序返回dict中key的value，并将其移除 dict.values() 以视图对象dict_values([value1, value2, …])的形式返回dict的values 字典视图对象由 dict.keys(), dict.values() 和 dict.items() 所返回的对象是 视图对象。该对象提供字典条目的一个动态视图，这意味着当字典改变时，视图也会相应改变。可以使用 list(dict.keys()) 将字典视图对象转换为list。 上下文管理器类型（待补充）Python 的 with 语句支持通过上下文管理器所定义的运行时上下文这一概念。此对象的实现使用了一对专门方法，允许用户自定义类来定义运行时上下文，在语句体被执行前进入该上下文，并在语句执行完毕时退出该上下文。详情请参考 with 语句。 union类型（待补充）","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"python","slug":"python","permalink":"/tags/python/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"「突撃！南島原情報局」南岛原市宣传片字幕翻译","slug":"「突撃！南島原情報局」南岛原市宣传片字幕翻译","date":"2021-07-11T08:30:38.000Z","updated":"2021-07-11T08:30:38.000Z","comments":true,"path":"2021/07/11/「突撃！南島原情報局」南岛原市宣传片字幕翻译/","link":"","permalink":"/2021/07/11/「突撃！南島原情報局」南岛原市宣传片字幕翻译/","excerpt":"","text":"「突撃！南島原情報局」作品简介日语宣传片的个人翻译练习。看到朋友搬运了这个视频感觉很有趣，就当作训练素材了（神经翻译系统训练ing）。这个宣传视频主要介绍了日本长崎县南岛原市的风光以及特产。 突撃！南島原情報局【神回】 勘误有些地方把 一揆（いっき） 听成了 域（いき）。因为之前并不知道 一揆（いっき） 这个词，导致字幕里面有的地方翻译成了 地区 忘了改成 起义。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"},{"name":"翻译","slug":"翻译","permalink":"/tags/翻译/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"「先輩とみなみちゃん」JA共済CM合集字幕翻译","slug":"「先輩とみなみちゃん」JA共済CM合集字幕翻译","date":"2021-07-11T07:31:46.000Z","updated":"2021-07-11T07:31:46.000Z","comments":true,"path":"2021/07/11/「先輩とみなみちゃん」JA共済CM合集字幕翻译/","link":"","permalink":"/2021/07/11/「先輩とみなみちゃん」JA共済CM合集字幕翻译/","excerpt":"","text":"「先輩とみなみちゃん」（前辈和美波酱）作品简介日语广告片的个人翻译练习。合集主要介绍了JA共済的相关保障服务，类似于保险业务。 # 1 ＪＡ共済とは（JA共济是什么） # 2 ＪＡ共済の保障とは（JA共济的保障是什么） # 3 ライフアドバイザーとは（生活顾问是什么） # 4 医療共済（医疗共济） # 5 そなエール（防范准备） # 6 終身共済（终身共济） # 7 メディフル（一時金）（Mediful 一次性补贴） # 8 メディフル（祝金）（Mediful 健康红包） # 9 My家財プラス（我的家产plus）","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"},{"name":"翻译","slug":"翻译","permalink":"/tags/翻译/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语课总结（1）","slug":"日语课总结（1）","date":"2021-05-26T15:31:42.000Z","updated":"2021-05-26T15:31:42.000Z","comments":true,"path":"2021/05/26/日语课总结（1）/","link":"","permalink":"/2021/05/26/日语课总结（1）/","excerpt":"","text":"日语课总结（1）表記（ひょうき） どんなことも、まずやってみようという精神（せいしん）が大事だ。 ごみの処理（しょり）にはお金がかかる。 みんな同じコップを使いますから、自分のコップに印（しるし）を付けてください。 胸に記（しる）して忘れない。 日記（にっき） 今日の夜、テレビでサッカーの試合を放送します。 私の趣味はお菓子を作ることです。 彼は三十七歳のとき、始めて小説を著（あらわ）した。 悲しみの感情を表（あらわ）した音楽。 危険（きけん）な症状を現す。 父の財産を兄と私で半分ずつ相続（そうぞく）した。 この腕時計（うでどけ）は一日に一分ずつ進む。 大雨でサッカーの決勝戦（けっしょうせん）は延期されることになった。 現場から犯人のものと思われる足跡（あしあと）が発見された。 このゲームは非常（ひじょう）に集中力が要（い）る。 言い換え類義 母親は息子の部屋のドアを そっと／静かに 閉めた こっそり／こそこそ きちんと 整洁、恰当 ちゃんと 认真、规矩、整齐 確り（しっかり） 可靠、牢固 地震（じしん）の時は 冷静／落ち着いて に行動しなければならない。 急いで／慌（あわ）てて 今年の梅雨（ばいう）が 長引（ながび）ている／なかなか終わらない 。 土地の問題をめぐる住民（じゅうみん）の対立の 根（ね）／原因 は深い。 専門用語（せんもんようご）の多い英語（えいご）を翻訳（ほんやく）するのは 厄介（やっかい）／面倒（めんどう） なので、専門家に頼むんだ。 この争（あらそ）いの 根（ね）／原因 は一つだけではない。 からかうのは よして／やめて ください。 車を止める。 道路（どうろ）の石を退（ど）ける。 結婚式（けっこんしき）の招待状（しょうたいじょう）を送ったら、 続々（ぞくぞく）と／次々（つぎつぎ）に 返事が届いた。 順々（じゅんじゅん）に仕事をかたづける。 転々（てんてん）と各地（かくち）を巡業（じゅんぎょう）する。 度々注意したが聞き入れない。 選手（せんしゅ）たちはゴールに向かって 徐々（じょじょ）に／少しずつ 走るスピードを上げていった。 工事は 着々（ちゃくちゃく）と／順調に 進んでいる。 さんまがどっと市場（いちば）へ出回（でまわ）る。 彼女は 穏（おだ）やかな／静かな 表情（ひょうじょう）で話している。 語形成（ごけいせい） おいしいと評判（ひょうばん）のケーキを買いに行ったが、売り 切れて いて買えなかった。 日本で 少子化（しょうしか） が進んでいる原因の一つとして、結婚の時期が遅くなっていることがあげられる。 今回のエベントは、この会社に入って初めての 大 仕事だから、みんな張り切っている。 自転車で走っていたら、横道から子供が飛び 出して きて、びっくりした。 母はいつも、肉、魚、野菜をバランスよく取り 入れて 料理を作ってくれる。 もう少しで頂上（ちょうじょう）というところまで行ったが、急に天気が悪くなったので仕方なく引き 返した。 子どもを引き 寄せる。 以前は英語なんて話せないと思い 込んで いたが、海外からの客が増えて、どうしても話さないわけにはいかなくなった。 地球（ちきゅう） 温暖化（おんだんか） の影響（えいきょう）だろうか、異常（いじょう）気象（きしょう）が続いている。 この仏像（ぶつぞう）は国宝（こくほう）だが、一般的（いっぱんてき）には 非 公開になっている。 タバコはやめたはずなのに、無 意識 に灰皿（はいざら）のある場所を探してしまう。 この会社では、新製品を発売するための準備が 着々と 進んでいる。 文脈規定（ぶんみゃくきてい） 呉呉（くれぐれ）もお大事に。 栄養（えいよう）の バランス を取るには、野菜をたくさん食べることです。 朝飯の 献立（こんだて）／メニュー はみそ汁・たまご焼き・のり・香の物だ。 世界の何処（どこ）かで戦争（せんそう）が起こっている。平和（へいわ）を 維持 するのは難（むずか）しいことだ。 結論（けつろん）を出すために、話し合いの 焦点（しょうてん） を絞（しぼ）りましょう。 首相（しゅしょう）はインタビューで政府の重要（じゅうよう）課題（かだい）に対する 見解 を明（あき）らかにする。 会費は5万円くらいと 見当をつける／を推測（すいそく）する。 事故（じこ）は 思いがけない ところで起こることが多いから、一瞬の注意も怠（おこた）ることができない。 手入れを怠（おこた）ると故障（こしょう）する。 この国の女性の 平均寿命（へいきんじゅみょう） は八十歳である。 この料理は野菜が たっぷり 入っているので、体にいいですよ。 青空に山が くっきり／はっきり 見える。 こっそり跡をつける。 ぐっすりと眠っている。 彼女は、この仕事では二十年のキャリアがある ベテラン（veteran） だ。 テレビのアンテナ（antenna）を取り付ける。 この道は狭いから、車が 擦（す）れ違う のは難（むずか）しい。 レポートを書くために、世界の人口の 分布（ぶんぷ） を調べてみた。 社会人は自分の 行動（こうどう） に責任を持たなげればならない。 この荷物（にもつ）は上下（じょうげ）が 逆（さか）さま にならないように注意して運（はこ）んでください。 入選作は 各々（おのおの）／銘々（めいめい）／それぞれ 優（すぐ）れている。 試験の前日はしっかり睡眠（すいみん）をとったほうがいい。 文法（ぶんぽう） Vる + ほかない含义：除此 该动作 之外没别的办法。「Vる + しかない」的official版本。例句：だれにも頼めないから、自分で やる ほかない。 Vる + ものだ／ものではない含义：认为 该动作 为一般常识，这样的动作更好。语气较为强烈。例句1： 学生は 勉強する ものです。遊んでばかりいてはいけません。例句2：日本では、女性に年齢を 聞く ものではない。 V／いAい／なAな + わけではない含义：认为 该动作/形容 不完全正确。例句：日本人が全員、敬語が上手に 話せる わけではない。 Vる + ことはない含义：认为 该动作 没有必要。用于主观性的建议。例句：彼のほうが悪いのだから、君が 謝る ことはない。 Vる／N + どころではない含义：表示现在不是 该动作/名词 发生的时候。语气较为强烈，带有责难或质疑。例句：A：こんばん、映画に行かない？B：え？明日、テストなんだから、 行く／映画 どころではないよ！ Vる、Vない + ことだ含义：认为 该动作 比较好、重要。用于主观性建议。例句：日本語が上手になりたかったら、毎日、日本人と 練習する／会話する ことだ。 V／いAい／なAな／Nの + わけがない含义：表示对于 该动作/形容/名词 的强烈否定。语气带有强烈怀疑、质疑。例句：A：旅行に行かない？B：えっ？今、コロナなんだから、旅行に 行く わけがないでしょう。 Vる、Vない + ことになっている含义：表示 该动作 是被规则、计划等决定好的。例句1：日本では、二十歳（はたち）未満でお酒を飲んでは いけない ことになっている。例句2：能力試験は、7月は中止になったら、12月は 行（おこな）う ことになっている。 N + からして含义：表示仅仅看 该名词 ，不看其他方面，就可以得出一些结论。用于表达 该名词 的重要性亦或是做事的最低限度。例句：あのコンビニのバイト、新人だね。 あいさつ からして、きちんとしていないよ。 V／いAい／なAな／Nの + あまり含义：表示 该动作/形容/名词 过分到超过了一般情况。常用于表示 该动作 做的过了， 该名词（情感） 过剩。*余（あま）り。例句：田中さんは、彼にふられて、 悲しみ のあまり、ご飯を食べなくなった。 V／いAい／なAである／Nである + ことから含义：表示 该动作/形容/名词 是后文的原因。例句：ここから、よく星が 見える ことから、ここは星見町という名前になった。 V／いAい／なAである／Nである + からには含义：表示 该动作/形容/名词 是后文的原因（当然でしょう）。语气中包含着强烈的 決意（けつい），可理解为「既然 … 就一定要 … 」。例句：約束した からには、守らなければならない／守るべきだ。 Nの + かわりに含义：表示后文动作本应由 该名词 完成，作为 该名词 的替代，主语完成了该动作。例句：学会（がっかい）には、忙しい先輩のかわりに私が出席（しゅっせき）した。 Nの + ことだから含义：在较为了解 该名词 的情况下，推测 该名词 很有可能导致后文的动作发生。例句：よく遅（おく）れる 田中さん のことだから、今日も、遅（おく）れるだろう。 Vます／N + がちだ含义：表示 该动作/名词 发生的次数很多或者很容易发生。例句：彼女は小さいときからよく病気する。→彼女は 病気 がちだ。 N + だらけ含义：表示 该名词 大量出现。使用场景仅限于眼见的实体。例句：この作文は間違いがたくさんある。→この作文は まちがい だらけだ。 Vて／N + ばかり含义：表示只会一直进行 该动作/名词 不做别的。常用于责难的时候，带有不满的语气。例句：彼はいつもゲームを して ばかりいる。＝彼はいつも ゲーム ばかりしている。 Vる + べきだ含义：表示 该动作 是当然应该做的。常用于提出强烈的主张和忠告。例句1：子供でも、悪いことをしたら、 謝る べきだ。例句2：結婚式に白い服を 着る べきではない。 V／いAい／なAな + わけだ含义：表示 该动作 是有理由的。常用于表示前后文关系的合理性。例句：A：ジョンさんはアメリカ人だけど、お母さんが日本人なんだって。B：それで、日本語が わかる／話せる わけだ。 N1をN2として含义：把 名词1 当作是 名词2。例句：今日は、 ベトさん を 先生 として、ベトナム語を勉強しましょう。 Vる、Vている、Vない／いAい／なAな／Nの + うちに含义：趁着 该动作/形容/名词 的时间，去做后文的动作。* の間に例句1：子供が ねている うちに、そうじしよう／買い物に行こう。例句2：覚えている／忘れない うちに、メモしよう。 Vる、Vた／Nの + ついでに含义：该动作/名词 的顺便做了后文的动作。例句：スーパーへ 行く ついでに、銀行／郵便局に寄った。 V／いAい／なAなである／Nである + わりに含义：表示在 该动作/形容/名词 的前提下，后文的事件与预想不符。通常用于前后文的正负相关性与预想不符。例句：あのレストランは、値段が 安い わりに、量が多い。 Vます形 + 得る含义：表示 该动作是可能发生的。official版本。不能用于表示能力。例句1：コロナは、だれでも、 なり／かかり うる病気だ。例句2：私は1キロ（泳げる・泳ぎうる）。 Vます形 + っこない含义：表示 该动作 绝对不可能发生。口语。例句：こんなにたくさんの言葉、一日じゃ、 覚えられ っこないよ。 Vる／Nの + おそれがある含义：表示 该动作 恐怕有可能发生。通常 该动作 为负面结果。例句：地震の後は、津波（つなみ）が くる おそれがあります。 Vます形 + つつある含义：表示正在逐渐向 该动作 的方向变化。该动作 通常是具有改变、成为等含义的动词。例句：台風で、風が強くなりつつあります。 Vた + すえに含义：表示后文是 该动作 的结果。这个结果的发生通常需要 该动作 经过较长时间。例句：考えた すえに、国に帰ることに決めました。 Vる、Vている、Vた + ところ含义：表示 即将要、正在、刚刚 做了 该动作 的时候，就发生了后文的动作。 附录这一节课不会的东西也太多了，全记成电子笔记的话工程量有点遭不住啊。。。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"「天気の子」翻译（人物介绍、序章）","slug":"「天気の子」翻译（人物介绍、序章）","date":"2021-05-08T15:58:00.000Z","updated":"2021-05-08T15:58:00.000Z","comments":true,"path":"2021/05/08/「天気の子」翻译（人物介绍、序章）/","link":"","permalink":"/2021/05/08/「天気の子」翻译（人物介绍、序章）/","excerpt":"","text":"原文为自购的「天気の子」@角川つばさ文庫新海誠（しんかいまこと） 作ちーこ 挿絵 本文仅用于日语学习用途，请勿用于商业用途！ 「天気の子」翻译人物紹介（じんぶつしょうかい） 天野陽菜（あまのひな）：帆高（ほだか）が出会った女の子。”祈る”と空を晴れにできる。 森嶋帆高（もりしまほだか）：家出して、東京にやってきた高校生（こうこうせい）。 夏美（なつみ）：須賀（すが）の事務所（じむしょ）で働く女子（じょし）大生（だいせい）。 須賀圭介（すがけいすけ）：帆高を雇（やと）う小さな編集（へんしゅう）プロダクション（production）のライター（writer）。 天野渚（あまのなぎ）：陽菜の弟（おとうと）。女子に人気の小学生（しょうがくせい）。 序章（じょしょう） 君に聞いた物語三月（さんがつ）の雨空（あまぞら）に、フェリーの出港（しゅっこ）を知らせる汽笛（きてき）が長く響く。 在一个三月的雨天，预示着轮渡出港的汽笛声长鸣。 巨大（きょだい）な船体（せんたい）が海水（かいすい）を押しのけていく重い振動（しんどう）が、尻から全身（ぜんしん）に伝わってくる。 巨大的船体推开海水产生的剧烈震动，从屁股传向全身。 僕のチケットは船底（せんてい）に最（もっと）も近い二等船室（にとうせんしつ）。東京までは十時間以上（いじょう）の船旅（ふなたび）で、到着（とうちゃく）は夜になる。このフェリーで東京に向かうのは、人生で二度目だ。僕は立ち上げり、デッキテラスへの階段（かいだん）に向かう。 我的船票是离船底最近的二等房间。到东京是要十小时以上船旅的，到了的时候已经是晚上了。坐这班轮渡去往东京，已经是人生中的第二次了。我站起来，面向甲板的台阶。 「あいつには前科（ぜんか）があるらしい」とか、「今でも警察（けいさつ）に追（お）われているらしい」とか、僕が学校（がっこう）でそんな噂（うわさ）をされるようになたのは、二年半前の東京での出来事がきっかけだった。噂（うわさ）をされること自体（じたい）はどうということもなかったけれど（実際（じっさい）、噂（うわさ）になるのは当然（とうぜん）だったと思う）、僕はあの夏の東京での出来事を、島の誰にも話さなかった。断片的に語（かた）ったことはあるけれど、本当に大事なことは親にも友人（ゆうじん）にも警察（けいさつ）にも話さなかった。あの夏の出来事をまるごと抱えたまま、僕はもう一度東京に行くのだ。 「那个人好像有前科」、「好像现在还在被警察追查」什么的，我在学校被那样的传闻缠身，正是因为两年前在东京的变故。被传闲话这样的事，自己倒是觉得没什么大不了的（实际上，我认为会变成传闻也是当然的）。那个夏天在东京的事故，我对这个岛上的任何人都没提起过。虽然有过只言片语，但真正重要的事情我就算对父母、朋友还是警察都没提起过。因为背负着那个夏天的变故，我再一次前往东京了。 十八歳になった今、今度こそあの街に住むために。 已经18岁的现在，这次一定要为了住在那个街区。 もう一度あの人に会うために。 为了再一次和那个人相遇。 そのことを考えると、いつでも肋骨（ろっこつ）の内側（うちがわ）が熱を持つ。頬（ほお）がじわりと火照（ほて）る。早く海風（うみかぜ）に当たりたくて、僕は階段を登る足を速める。 考虑那个事情的话，不论何时肋骨的内侧都会（感觉）温热。脸颊一点点发烫。想要赶快感受海风，我加快脚步登上台阶。 デッキテラスに出ると、冷たい風が雨と共にどっと顔を打った。その全部を飲み込むようにして、僕は大きく息を吸い込む。風はまだ冷たいけれど、そこには春の気配（けはい）がたっぷりと含（ふく）まれている。ようやく高校を卒業（そつぎょう）したんだーーその実感（じっかん）が、遅（おく）れた通知（つうち）のように今さらに胸に届（とど）く。僕はデッキの手すりに肘（ひじ）を乗せ、遠ざかっていく島を眺（なが）め、風巻（しま）く空に目を移（うつ）す。視界の遥か彼方（かなた）まで、数（かぞ）え切れない雨粒（あまつぶ）が舞（ま）っている。 一从甲板阳台中出来，刺骨的寒风混合着雨点一下子打到我的脸上。像是要把他们全部吞下去一样，我大口吸气。虽然寒风依旧，那里面却包含着春的气息。终于高中毕业了啊——那个实感像是迟来的通知一样，现在才传达到我的心中。我靠在甲板的扶手上，眺望着远去的海岛，将视线移向刮着狂风的空中。直到视线的远方，数不清的雨点在（空中）飞舞。 その途端（とたん）ーーぞわりと、全身の肌（はだ）が粟立（あわだ）った。 正当这时——一阵寒颤，全身的汗毛都竖了起来。 まただ。思わずきつく目を閉じる。じっとしている僕の顔を雨が叩（たた）き、耳朶（じだ）には雨音（あまおと）が響き続ける。この二年半、雨は常（つね）にそこにあった。どんなに息を殺しても決（けっ）して消せない鼓動（こどう）のように。どんなに強く瞑（つむ）っても完全な闇（やみ）には出来ない瞼（まぶた）のように。どんなに静（しず）めても片時も沈黙（ちんもく）出来ない心のように。 还没到。禁不住紧紧闭上眼睛。雨点拍打着我一动不动的脸，耳中一直回响着雨点的声音。这两年半中，经常下雨。像是不论怎么屏住呼吸也绝对抹不掉的脉搏一样。像是不论如何紧闭双眼也不能做到完全黑暗的眼睑一样。像是不论如何镇静也练片刻沉默都做不到的心脏一样。 ゆっくりと息を吐（は）きながら、僕は目を開ける。 一边慢慢的吐气，我睁开了眼睛。 雨。 雨。 呼吸（こきゅう）をするようにうねる黒い海面に、雨が際限（さいげん）なく吸い込むまれていく。まるで空と海が共謀（きょうぼう）して、いたずらに海面を押し上げようとしているかのようだ。僕は怖くなる。身体（からだ）の奥底（おくそこ）から震（ふる）えが湧（わ）き上がってくる。引き裂かれそうになる。ばらばらになりそうになる。僕は手すりをぎゅっと掴（つか）む。鼻から深（ふか）く息を吸う。そしていつものように、あの人のことを思い出す。彼女の大きな瞳（ひとみ）や、よく動（うご）く表情（ひょうじょう）や、ころころ変わる声のトーンや、二つに結んだ長い髪を。そして、大丈夫だ、と思う。彼女がいる。東京で彼女が生きている。彼女がいる限（かぎ）り、僕はこの世界にしっかりと繋ぎ止められている。 雨被像是呼吸一样起伏的黑色的海面无限制地吸进去。完全是天空和大海的同谋，像是要没有意义的将海平面抬高一样。我开始感到害怕。恐惧从身体的深处涌了上来。像是要变得被撕裂一样。像是要变得破碎一样。我紧紧握住扶手。用鼻子深深地吸气。就这样像往常一样，回想起那个人。想起她的大眼睛、多变的表情、轻快转变的声调、扎成两束的长发。之后就会觉得没关系的。她还活着。她还在东京好好的生活着。只要她还活着，我就能和这个世界紧紧相连。 「ーーだから、泣かないで、帆高」 「——所以，别哭啊，帆高」 と、あの夜、彼女は言った。逃げ込んだ池袋（いけぶくろ）のホテル。天井（てんじょう）を叩（たた）く雨の音が、遠い太鼓（たいこ）のようだった。同じシャンプーの香りと、何もかもを許したような彼女の優し声と、闇に青白（あおじろ）く光る彼女の肌（はだ）。それらは余（あま）り鮮明（せんめい）で、僕は不図（ふと）、今（いま）も自分があの場所（ばしょ）にいるような気持ちに襲（おそ）われる。本当（ほんとう）の僕たちは今（いま）もあのホテルにいて、僕はたまたまのデジャヴ（dejavu）のように、未来の自分がフェリーに乗っている姿（すがた）を想像しただけなのではないか。昨日の卒業式もこのフェリーも全部錯覚（さっかく）で、本当の僕の隣にいて、世界はいつもと同じ姿のまま、変わらぬ日常が再開するのではないか。 那个晚上，她这么说道。逃进池袋的旅馆。拍打着天花板的雨声像是远处的太鼓（声）。同样的洗发水的香气、像是什么都原谅的她温柔的声音、在黑暗中显出苍白血色的她的肌肤。因为这些（场景）都太深刻了，突然之间，像是自己也在那个地方的感觉向我袭来。像是我们真的在那个旅馆一样，像是我偶然（脑中）出现了似曾相识感一样，这不只是我在想象未来的自己乘上轮渡的样子吗？昨天的毕业典礼和这个轮渡全都是错觉，实际上我的周围，实际还是和往常相同的的样子，改变不了的日常再次上演，难道不是吗？ 汽笛（きてき）が鋭（するど）く鳴（な）った。 汽笛声尖锐的响起。 違う、そうじゃない。僕は手すりの鉄の感触（かんしょく）を確かめ、潮（しお）の匂いを確かめ、水平線（すいへいせん）に消えかかっている島影（しまかげ）を確かめる。そうじゃない、今はあの夜ではない。あれはもうずっと前のことだ。フェリーに揺られているこの自分が、今の本当の僕だ。きちんと考えよう。最初（さいしょ）から思い出そう。雨をにらみながら僕はそう思う。彼女に再会する前に、僕たちに起きたことを理解しておかなければ。いや、たとえ理解は出来なくても、せめて考え尽（つ）くさなければ。 不是的，不是这样的。我确认了扶手金属的触感，确认了海潮的味道，确认了眼看就要消失在水平线的海岛的样子。不是这样的，现在不是那个晚上了。那已经是很久以前的事情了。随着轮渡摇摆的自己，才是真正的我。好好的思考啊。从最初开始回忆。一边盯着雨点，我这么想着。和她再遇之前，必须要预先理解因为我们而起的事情。不不，比如说就算不能理解，也必须尽力考虑。 僕たちになにが起きたのか。僕たちはなにを選んだのか。そして僕は、これから彼女にどういう言葉（ことば）を届けるべきなのか。 因为我们而发生了什么是吗？我们选择了什么是吗？还有就是，我从这开始应该向她传递怎样的话语呢？ すべてのきっかけはーーそう、たぶんあの日だ。 所有的原因就是——对，大概就是那天了。 彼女が最初（さいしょ）にそれを目撃（もくげき）した日。彼女が語（かた）ってくれたあの日の出来事が、すべての始まりだったんだ。 她最开始目击到那个的日子。她对我讲述的那天偶然发生的事，是一切的开始（强烈解释性语气）。 彼女の母親は、もう何カ月も目を覚まさないままだったそうだ。 她的母亲好像已经好几个月一直没有醒过来了。 小さな病室（びょうしつ）を満（み）たしていたのは、バイタルモニター（vital monitor）の規則的（きそくてき）な電子音（でんしおん）と呼吸器（こきゅうき）のシューという動作音（どうさおん）と、執拗（しつよう）に窓をたたく雨音（あまおと）。それと、長く人の留（とど）まった病室に特有（とくゆう）の、世間（せけん）と切り離されたしんとした空気（くうき）。 生命检测器规律的电子声音、呼吸器“咻”的工作声音以及执拗的敲在窗户上的雨声充满了小小的病房。以及（病）人住了很久的病房特有的，与世隔绝的寂静气氛。 彼女はベッドサイドの丸椅子に座ったまま、すっかり骨張（ほねば）ってしまった母親の手をきゅっと握（にぎ）る。母親の酸素（さんそ）マスクが規則的（きそくてき）に白く濁（にご）るさまを眺（なが）め、ずっと伏（ふ）せられたままの睫毛（まつげ）を見つめる。不安に押しつぶされそうになりながら、彼女はただただ祈っている。お母（かあ）さんが目を覚ましますように。ピンチの時のヒーローみたいな風が力強（ちからづよ）く吹（ふ）きつけて、憂鬱（ゆううつ）とか心配とか雨雲（あまぐも）とか暗くて重い物をすっきりと吹き飛ばし、家族三人で、もう一度青空（あおぞら）の下を笑いながら歩けますように。 她一直坐在床边的圆凳上，紧紧的握着母亲已经（瘦弱到）完全皮包骨头的手。注视着母亲规律性被白色雾气笼罩的氧气面罩，紧盯着一直被垂下的睫毛。被不安（的情绪）压倒，她也唯有祈祷。母亲好像睁开了眼睛。紧急关头伴随着像是英雄一样的强风，将忧郁、担心、阴雨和黑暗沉重的心情一下子吹飞，为了一家三人能再一次在蓝天下笑着散步。 ふわり、と彼女の髪が揺られた。ぴちょん、と耳元で微（かす）かな水音が聞こえた。 她的头发轻柔地摇摆着。咚的一声，耳边听到了微弱的水声。 彼女は顔を上げる。閉（し）め切ったはずの窓（まど）のカーテンがかすかに揺れている。窓ガラス越しの空に、彼女の目は引き寄せられる。いつの間にか陽が射（さ）している。雨は相変わらず本降（ほんぶ）りだけど、雲に小さな隙間（すきま）が出来ていて、そこから伸（の）びた細（ほそ）い光が地上（ちじょう）お一点を照らしている。彼女は目を凝（こ）らす。視界の果てまで敷（し）き詰（つ）められた建物。そのうちの一つのビルの屋上（おくじょう）だけが、スポットライトを浴（あ）びた役者（やくしゃ）みたいにぽつんと光っている。 她仰着头。本应紧闭着的窗户的窗帘正微微地摇摆着。她的眼神被吸引到穿过窗户玻璃的天空上。不知何时，阳光照射了出来。虽然雨还是老样子下得很大，阳光却从云间的缝隙中钻出，并顺着这个缝隙延伸开来照在了地面上的一点。她聚精会神。视野的尽头被建筑物覆盖。只有那里面一座高楼的屋顶在孤零零的发光，就像沐浴在聚光灯下的演员。 誰かに呼ばれたかのように、気づけば彼女は病室から駆け出していた。 像是被谁呼唤着一样，不经意间，她已经从病房中跑了出去。 そこは廃ビルだった。周囲の建物はぴかぴかに真新しいのに、その雑居（ざっきょ）ビルだけは時間に取り残されたかのように茶色く朽（く）ちていた。「ビリヤード（billiard）」とか「金物店（かなものてん）」とか「鰻（うなぎ）」とか「麻雀（マージャン）」とか、錆（さ）び付いて色褪（いろあ）せた看板がビルの周囲にいくつも貼（は）り付いていた。ビニール傘越（がさご）しに見上げると、陽射しは確かにここの屋上（おくじょう）を照らしている。ビルの脇（わき）を覗（のぞ）くと小さな駐車場（ちゅうしゃじょう）になっていて、ぼろぼろに錆（さ）び付いた非常階段（ひじょうかいだん）が屋上まで伸びていた。 那是废弃的高楼。明明周围的建筑物都崭新到闪闪发光，只有那个杂堆中的高楼像是落后于时代一样在茶色中腐朽。有很多「台球馆」、「五金店」、「鳗鱼」、「麻将」这样的生锈褪色的看板贴在高楼的周围。透过塑料伞向上看，阳光确实正在照射着这里的屋顶。稍微看向高楼的周围，能看到一个面积不大的停车场，破损不堪的生锈的（百度翻译：锈蚀不堪的）紧急楼梯延伸到屋顶。 ーーまるで光の水たまりみたい。 简直就像是光的水塘。 階段を昇りきった彼女は、一時（いっとき）、眼前（がんぜん）の景色（けしき）に見とれた。 登上楼梯的她一时间被眼前的景色吸引住了。 手すりに囲（かこ）まれたその屋上（おくじょう）は二十五メートルプールのちょうど半分くらいの広さで、床（ゆか）のタイルはぼろぼろにひび割（わ）れ、一面緑の雑草（ざっそう）に覆（おお）われていた。その一番奥に茂（しげ）みに抱（だ）き抱（かか）えるようにして小さな鳥居（とりい）を真っ直ぐに照らしている。鳥居の朱色（あかいろ）が、陽射しのスポットライトの中で雨粒（あまつぶ）と一緒（いっしょ）にきらきらと輝いていた。雨に濁（にご）った世界の中で、そこだけが鮮（あざ）やかだった。 被扶手包围的屋顶差不多正好有25米游泳池的一半大，地板的瓷砖已经破损不堪出现裂纹，被一片绿色的杂草所覆盖。（阳光）直直的照射着那最深处像是被繁茂草木簇拥着的⛩（神社牌坊）。⛩（神社牌坊）的朱红色在阳光聚光灯束中和雨点一起闪闪发光。被雨点遮住的世界中，只有这里是鲜艳亮丽的。 ゆっくりと、彼女は鳥居に向かって屋上を歩いた。さくさくという柔（やわ）らかい音（おと）と心地（ここち）好い弾力（だんりょく）がある。雨のカーテンの向こうには、幾（いく）つもの高層（こうそう）ビルが白く霞（かす）んで立っている。何処（どこ）かに巣（す）があるのか、小鳥の囀（さえず）りがあたりに満ちている。そこに微かに、まるで別の世界から聞こえてくるような山手線（やまのてせん）の遠い音が混じっている。 慢慢的，她面向神社走在屋顶上。有沙沙的柔软的声音和令人舒服的弹性。雨帘的另一边，有几座高楼在白雾中矗立着。像是哪里有鸟巢一样，到处都是小鸟的啼叫。在那里，完全像是从别的世界传来山手线遥远的声音微弱的混杂（在小鸟的叫声中）。 傘を地面に置（お）いた。雨の冷たさが彼女の滑（なめ）らかな頬（ほお）を撫（な）でる。鳥居の奥には小さな石の祠（ほこら）があり、その周囲には紫色（むらさきいろ）の小さな花が茂（しげ）っていた。そこに埋もれるように、誰が置いたのか盆飾（ぽんかざ）りの精霊馬（しょうりょううま）が二体（にたい）あった。竹（たけ）籤（ひご）を刺したキュウリとように。ゆっくりと目を閉じ、願いながら鳥居を潜（くぐ）る。お母さんが目を覚まして、青空の下を一緒に歩けますように。 把伞放在地面上。雨的寒冷抚摸着她光滑的脸颊。神社的里面有一个石头（搭的）小祠堂，那（祠堂）周围紫色的小花繁茂（生长着）。在哪里埋没着一样，像是被谁放置（在那里）的装饰盂兰盆节的精灵马（一种祭祀用品）有两个。像是竹签刺入的黄瓜一样。慢慢闭上眼睛，一边许愿一边走过神社牌坊。为了能在母亲醒来之后一起走在蓝天之下。 鳥居を抜けると、不意（ふい）に空気が変わった。 穿过神社牌坊，不经意间气场改变了。 雨の音が、ぷつりと途切（とぎ）れた。 雨声，（像绳子一样）啪的一下中断了。 目を開くとーーそこは青空の真ん中だった。 张开双眼——这里是蓝天的正中央。 彼女は強い風に吹かれながら、空のずっと高い場所に浮（う）かんでいた。いや、風を切り裂（さ）いて落ちていた。聞いたこともないような低（ひく）くて深（ふか）い風の音が周囲に渦巻（うずま）いていた。息は吐くたびに白く凍（こお）り、濃紺（のうこん）の中でキラキラと瞬（またた）いた。それなのに、恐怖（きょうふ）はなかった。目覚（めざ）めたまま夢を見ているような奇妙な感覚（かんかく）だった。 她一边被强风吹拂，一边漂浮在空中相当高的地方。不对，是把风劈开向下落。从没听过的低沉深邃的风声在周围卷起漩涡。吐气冻成白雾，在深蓝（的天空）中闪闪发亮。就算如此，也并没有（感到）害怕。（而是）一直睁着眼睛做梦一样的奇妙感觉。 足元を見下（みお）ろすと、巨大（きょだい）なカリフラワーのような積乱雲（せきらんうん）が幾つも浮（う）かんでいた。一つ一つがきっと何（なん）キロメートルもの大きさの、それは壮麗（そうれい）な空の森のようだった。 看向脚下，几块巨大的菜花一样的积雨云漂浮着。每一个都一定有几公里那么大，那就像是壮丽的空中森林。 ふと、雲の色が変化していることに彼女は気付（きづ）いた。雲の頂上（ちょうじょう）、大気（たいき）の境目（さかいめ）で平らになっている平野（へいや）のような場所（ばしょ）に、ぽつりぽつりと緑が生まれ始めている。彼女は目を見張（みは）る。 突然，她注意到云的颜色正在变化。云的顶端，在大气的分界线处变得平坦的像平原一样的地方，绿色开始一点一点（墨水滴到纸上一样）生长。她惊讶的睁大了眼睛。 それは、まるで草原だった。地上からは決（けっ）して見えない雲の頭頂（とうちょう）に、さざめく緑が生まれては消えているのだ。そしてその周囲（しゅうい）に気付けば生き物のような微細（びさい）な何かが群（むら）がっている。 那简直就是草原。在从地面上绝对看不到的云的顶端，沙沙作响生长出来的绿色消失了。之后，稍微注意就会发现，在那周围有像是活物一样微小的什么东西在聚集。 「……魚？」 「……鱼？」 幾何学的（きかがくてき）な渦（うず）を描いてゆったりとうねるその群体（ぐんたい）は、まるで魚の群（む）れのように見えた。彼女は落下（らっか）しながら、じっとそれを見つめる。雲の上の平原（へいげん）を、無数（むすう）の魚たちが泳いでいるーー。 画着几何学漩涡顺畅蜿蜒的那个群体，不如说像是看到鱼群一样。她一边下落，一边聚精会神的盯着那个（鱼群）。无数的鱼正在云上的平原游泳。 突然、指先（ゆびさき）に何かが触れた。驚（おどろ）いて手を見る。やはり魚だ。透明な体（からだ）を持つ小さな魚たちが、重さのある風のように指や髪をすり抜けている。長いひれをなびかせているものや、くらげのように丸いものや、メダカのように細（こま）かなもの。様々な姿形（すがたかたち）の魚たちは、太陽の光を透（す）かしてプリズマみたいに輝いている。気付けば彼女は空の魚に囲（かこ）まれている。 突然，指尖碰到了什么。吓得（赶紧）看向手。果然是鱼。有着透明身体的小鱼们像是劲风一样穿过手指和头发。浮动着长鳍的东西，像是水母一样的圆的东西，像是青鳉鱼一样纤细的东西。各种各样形态的鱼透过阳光像棱镜一样发光。回过神来，她已经被空中的鱼包围起来。 空の青と、雲の白と、さざめく緑と、七色に輝く魚たち。彼女がいるのは、聞いたことも想像したこともない不思議で美しい更（さら）の世界だった。やがて彼女の足元を覆（おお）っていた雨雲（あまぐも）が解（ほど）けるように消えていき、眼下（がんか）にはどこまでも広がる東京の街並みが姿を現（あらわ）した。ビルの一つひとつ、車の一台（いちだい）いちだい、窓ガラスの一枚いちまいが、太陽を浴びて誇（ほこ）らしげに光っている。雨に洗（あら）われて生まれ変わったようなその街に、彼女は風に乗ってゆっくりと落ちていく。次第（しだい）に、不思議な一体感（いったいかん）が全身に満ちてくる。自分がこの世界の一部であることが、言葉以前（ことばいぜん）の感覚（かんかく）として彼女にはただ分かる。自分が風であり水であり、青であり白であり、心であり願いである。奇妙な幸せと切なさが全身に広がっていく。そしてゆっくりと、深く布団（ふとん）に沈み込むように意識が消えていくーー。 天空的蓝，云彩的白，喧嚣的绿，以及发出彩色光芒的鱼群。她所在的是闻所未闻、想所未想过的不可思议的美丽新世界。将将盖住她脚底的雨云像是解散一样消失了，眼下显现出的是向着各处伸展的东京的街道。一座座高楼，一台台汽车，一块块窗户玻璃在阳光的沐浴下自豪的发光。在这个被大雨洗刷后再生一般的街道上，她乘着风慢慢的落下来。（看到这个场景后）立刻，不可思议的一致感充满了全身。自己是这个世界的一部分，（这种）难以言表的感觉只有她能明白。风与水，青与白，心与愿都是我。奇妙的幸福感与苦闷传向全身。之后慢慢的，像是深深坠入被窝一样，意识消失了。 「あの景色。あの時私が見たものは全部夢だったのかもしれないけどーー」と、嘗（かつ）て彼女は僕に語（かた）った。 「那个风景。虽然当时我看到的东西说不定全部都是梦境。」，她曾经这样对我说。 でも、夢ではなかったのだ。僕たちは今ではそれを知っているし、僕たちはその後、ともに同じ景色を目の当たりにすることになる。誰も知らない空の世界を。 但是，那不是在梦中啊。我们现在是知道了这（件事），我们在那之后一起看到的一样的眼前的景色。（知道了）谁也不知道的空中世界。 彼女と共に過ごした、あの年の夏。 和她一起度过的，那年的夏天。 東京の空の上で僕たちは、世界の形を決定的（けっていてき）に変えてしまったのだ。 在东京的上空，我们不经意间让世界的形态发生了决定性的改变啊。 附录仨礼拜我翻完了14页哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈。我快吐了。 本书一共296页，预计共需要 $ \\frac{296页}{14页/3周} = 63.43周 &gt; 1年 $ 的时间。希望之后我能提速！ 也可能放弃提速转向填充附录的单词注释？","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"NLP with GTX1060（完形填空）","slug":"NLP with GTX1060（完形填空）","date":"2021-04-17T07:06:14.000Z","updated":"2021-04-17T07:06:14.000Z","comments":true,"path":"2021/04/17/NLP with GTX1060（完形填空）/","link":"","permalink":"/2021/04/17/NLP with GTX1060（完形填空）/","excerpt":"","text":"NLP with GTX1060（完形填空）1 环境准备我的软件环境： Windows10 python3.7 cuda11.0 + pytorch1.8.0 + cudnn（对应版本） cuda和cudnn需要和驱动版本搭配，cudnn安装需要注册英伟达账号。 我的硬件环境： Intel I7-8750H Nvidia GTX1060(6G) python依赖的几个库： pytorch（官网命令行安装） transformers（官网安装引导） 可能还要按需安装numpy（已经被pytorch依赖）、sklearn、matplotlib等。（直接pip） 2 完型填空 with GTX10602.1 介绍2.1.1 任务介绍完型填空概要： 已知条件：在一段文本中，几个单词被替换为了[MASK]符号。 先验知识：一个未被替换任何单词的文本集合。 目标：对给定一段被替换了n个单词的任意文本进行还原，使整段文本语义通顺，符合逻辑。 上下文填空其实是模型在预训练时使用的我认为最主要的获取单词含义、上下文意思的一种方式。实际上这种任务就是Bert中的Masked Language Model，详情可以参考 Masked LM 。这种任务的本质也是一种分类，只不过在该任务中，类别有vocab_size种，因为字典中的每个token都有填入这里的可能性，我们只是挑选概率高的作为候选。 2.1.2 模型介绍使用模型：DistilBERT特点：详见 2.1.2 模型介绍 。 2.2 代码2.2.1 DistilBERT模型预测# 库 import torch import torch.nn as nn from decimal import Decimal from transformers import DistilBertTokenizer, DistilBertForMaskedLM # 定义 mask位置寻找函数 # input_ids列表 - [index1, index2, ... , index] # 返回的列表包含所有mask在输入文本中的index位置 def find_mask_index(input_tensor): output_list = [] count = 0 for ids in input_tensor[0]: if ids == 103: # mask的id为103 output_list.append(count) count = count + 1 return output_list # 定义 bert完型预测函数 def bert_maskedlm_prediction(input_text): # 设备选择 device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # 选择已经预训练好的模型和tokenizer tokenizer = DistilBertTokenizer.from_pretrained(&#39;./distilbert-base-uncased&#39;) model = DistilBertForMaskedLM.from_pretrained(&#39;./distilbert-base-uncased&#39;) model = model.to(device) # 将输出logits转化为对应vocab的归一化概率 inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;) mask_list = find_mask_index(inputs[&#39;input_ids&#39;]) inputs = inputs.to(device) outputs = model(**inputs) logits = outputs.logits softmax = nn.Softmax() # 选择归一化函数 output_prob_list = [] for mask_index in range(len(mask_list)): output_prob_list.append(softmax(logits[0][mask_list[mask_index]][:])) # 得到归一化概率 # 将概率排序后输出 out_prob_dict_sorted_list = [] # 降序排列 for mask_index in range(len(mask_list)): output_prob_dict = {} for ids in range(output_prob_list[mask_index].shape[-1]): output_prob_dict[tokenizer.convert_ids_to_tokens(ids=ids)] = output_prob_list[mask_index][ids] out_prob_dict_sorted = sorted(output_prob_dict.items(), key=lambda x:x[1], reverse=True) out_prob_dict_sorted_list.append(out_prob_dict_sorted) # 打印 for mask_index in range(len(mask_list)): count = 1 print(&#39;###### MASK {} ######&#39;.format(mask_index)) for (key, val) in out_prob_dict_sorted_list[mask_index]: if count &gt; 5: # 打印概率前5的结果 break else: val = float(val) * 100 val = Decimal(val).quantize(Decimal(&#39;0.00&#39;)) print(&#39;|{:&lt;15s}|{:&gt;5s}%|&#39;.format(key, str(val))) count = count + 1 # 定义 主函数 def main(): input_text = &quot;Beijing is my [MASK], I have lived in [MASK] for many [MASK]. &quot; bert_maskedlm_prediction(input_text) 2.2.2 DistilBERT模型预训练 2.2.1 中使用的是DistilBERT原作者release的预训练好的模型。若想要重新或者进一步预训练，可能需要极强的GPU资源。 预训练代码之后有时间的话会补上。 待续。。。 2.3 实验2.3.1 输入文本范例Beijing is my [MASK], I have lived in [MASK] for many [MASK]. 其中，[MASK]为被盖住的token，实际中没有绝对的标准答案，通常将原文视为标准答案。 2.3.2 预测结果###### MASK 0 ###### Prediction Probability hometown 49.86% birthplace 16.05% home 5.62% homeland 4.64% capital 2.64% ###### MASK 1 ###### Prediction Probability beijing 68.04% china 11.14% peking 2.20% shanghai 1.52% tianjin 1.49% ###### MASK 2 ###### Prediction Probability years 74.87% generations 9.23% decades 7.81% centuries 5.62% months 0.64% 预测结果：Beijing is my hometown / birthplace / home, I have lived in Beijing / China / Peking for many years / generations / decades. 2.3.3 总结说实话，模型的三个预测都是我原本想的词。从结果上说，学习了大量语料的BERT还是很擅长完形填空的。不知道在需要先验知识、情感分析、长距离上下文联系的时候，模型还能不能表现出这样的效果。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"/tags/NLP/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"NLP with GTX1060（文本分类）","slug":"NLP with GTX1060（文本分类）","date":"2021-04-17T07:05:58.000Z","updated":"2021-04-17T07:05:58.000Z","comments":true,"path":"2021/04/17/NLP with GTX1060（文本分类）/","link":"","permalink":"/2021/04/17/NLP with GTX1060（文本分类）/","excerpt":"","text":"NLP with GTX1060（文本分类）1 环境准备我的软件环境： Windows10 python3.7 cuda11.0 + pytorch1.8.0 + cudnn（对应版本） cuda和cudnn需要和驱动版本搭配，cudnn安装需要注册英伟达账号。 我的硬件环境： Intel I7-8750H Nvidia GTX1060(6G) python依赖的几个库： pytorch（官网命令行安装） transformers（官网安装引导） 可能还要按需安装numpy（已经被pytorch依赖）、sklearn、matplotlib等。（直接pip） 2 文本分类 with GTX10602.1 介绍2.1.1 任务介绍文本分类概要： 已知条件：每段文本都有且只有一个标签与之对应，标签集合已知。 先验知识：一个标注好标签的文本集合。 目标：对给定一段任意文本进行正确的分类。 文本分类的实际应用： 文章分类 文本情感分析（消极/积极） 2.1.2 模型介绍使用模型：DistilBERT特点：DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark. 一句话概括：又快又好 2.2 代码2.2.1 DistilBERT模型训练# 库 import torch from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification from transformers import Trainer, TrainingArguments from sklearn.metrics import accuracy_score, precision_recall_fscore_support # 定义 读取tsv文件函数 # tsv文件 - [text1, text2, ... , text], [label1, label2, ... , label] # 其中返回的label会转换为Lookup Table中的序号 def read_tsv_file(file_dir): # Create Lookup Table str2label = [&quot;cs.AI&quot;, &quot;cs.CE&quot;, &quot;cs.cv&quot;, &quot;cs.DS&quot;, &quot;cs.IT&quot;, &quot;cs.NE&quot;, &quot;cs.PL&quot;, &quot;cs.SY&quot;, &quot;math.AC&quot;, &quot;math.GR&quot;, &quot;math.ST&quot;] texts = [] labels = [] with open(file_dir, &#39;r&#39;, encoding=&#39;utf-8&#39;, newline=&#39;&#39;) as tsv_file: for line in tsv_file.readlines(): if line == &#39;&#39; or line == &#39;\\n&#39;: continue line_list = line.split(&#39;\\t&#39;) texts.append(line_list[-1]) # 文本在tsv中的最后一栏 labels.append(str2label.index(line_list[-2])) # 标签在tsv中的倒数第二栏 tsv_file.close() return texts, labels # 定义 bert训练函数 def bert_classification_training(train_file_name, eval_file_name, epochs): # 设备选择 device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # 文件输入 train_texts, train_labels = read_tsv_file(train_file_name + &#39;.tsv&#39;) train_num = len(train_labels) eval_texts, eval_labels = read_tsv_file(eval_file_name + &#39;.tsv&#39;) eval_num = len(eval_labels) # 模型、tokenizer选择 tokenizer = DistilBertTokenizerFast.from_pretrained(&#39;./distilbert-base-uncased&#39;) # DistilBertTokenizer和BertTokenizerFast一样。Fast版本比非Fast版本多了CPU多线程支持，所以快。 model = DistilBertForSequenceClassification.from_pretrained(&#39;./distilbert-base-uncased&#39;) # Tokenization ### 注1 train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512, return_tensors=&quot;pt&quot;) eval_encodings = tokenizer(eval_texts, truncation=True, padding=True, max_length=512, return_tensors=&quot;pt&quot;) # 定义 MyDataset类 class MyDataset(torch.utils.data.Dataset): def __init__(self, encodings, labels): self.encodings = encodings self.labels = labels ### 注2 def __getitem__(self, idx): item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} item[&#39;labels&#39;] = torch.tensor(self.labels[idx]) return item def __len__(self): return len(self.labels) # 创建数据集 train_dataset = MyDataset(train_encodings, train_labels) eval_dataset = MyDataset(eval_encodings, eval_labels) # MyTrainer参数设定 ### 注3 training_args = TrainingArguments( output_dir=&#39;./results&#39;, # 输出目录 num_train_epochs=epochs, # 训练轮数 per_device_train_batch_size=8, # 训练batch per_device_eval_batch_size=8, # 评估batch learning_rate=5e-5, # AdamW学习率 warmup_ratio=0.01, # 热身比率 weight_decay=0.01, # 衰减率 logging_steps=10, # log频率 metric_for_best_model=&#39;eval_accuracy&#39;, # 决定最好模型的metric eval_steps=500, # 评估频率 load_best_model_at_end=True, # 是否在训练结束后加载最好的模型 ) # 定义 计算评估指标函数 def compute_metrics(pred): labels = pred.label_ids preds = pred.predictions.argmax(-1) precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=&#39;macro&#39;) acc = accuracy_score(labels, preds) return { &#39;accuracy&#39;: acc, &#39;f1&#39;: f1, &#39;precision&#39;: precision, &#39;recall&#39;: recall } # 初始化MyTrainer MyTrainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset, compute_metrics=compute_metrics ) # 开始训练 MyTrainer.train() # 保存最后的模型 bert_model_save_dir = &#39;./bert_model.pkl&#39; torch.save(model, bert_model_save_dir) # 开始评估 output_dict = MyTrainer.evaluate() # 提取评估结果 eval_loss = output_dict[&#39;traineval_loss&#39;] eval_accuracy = output_dict[&#39;eval_accuracy&#39;] eval_macrof1 = output_dict[&#39;eval_f1&#39;] eval_precision = output_dict[&#39;eval_precision&#39;] eval_recall = output_dict[&#39;eval_recall&#39;] # 将评估结果写入tsv文件 with open(&#39;distilbert_train&#39; + str(train_num) + &#39;_eval&#39; + str(eval_num) + &#39;_result.csv&#39;, &#39;a&#39;, encoding=&#39;utf-8&#39;) as result: result.write(&#39;eval_loss,&#39; + str(eval_loss) + &#39;\\n&#39;) result.write(&#39;eval_accuracy,&#39; + str(eval_accuracy) + &#39;\\n&#39;) result.write(&#39;eval_macrof1,&#39; + str(eval_macrof1) + &#39;\\n&#39;) result.write(&#39;eval_precision,&#39; + str(eval_precision) + &#39;\\n&#39;) result.write(&#39;eval_recall,&#39; + str(eval_recall) + &#39;\\n&#39;) result.close() # 定义 主函数 def main(): train_file_name, eval_file_name, epochs = input(&#39;Please input your train, evaluation file directory and expected number of training epoch.\\nInput format: train eval 8\\n&#39;).split(&#39; &#39;) bert_classification_training(train_file_name, eval_file_name, float(epochs)) 2.2.2 DistilBERT模型预测# 库 import torch import torch.nn as nn from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification # 定义 bert预测函数 def bert_classification_prediction(input_text): # 设备选择 device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # 选择已经训练好的模型和tokenizer tokenizer = DistilBertTokenizerFast.from_pretrained(&#39;./distilbert-base-uncased&#39;) model = torch.load(&#39;./bert_model.pkl&#39;) model = model.to(device) # 将输入文本转化为对应label的归一化概率 input_encoding = tokenizer(input_text, truncation=True, padding=True, max_length=512, return_tensors=&quot;pt&quot;) # tokenization input_encoding = input_encoding.to(device) output = model(**input_encoding) # 模型输出 output_logit = output.logits # 对应label的logits softmax = nn.Softmax() # 选择归一化函数 output_prob = softmax(output_logit) # 得到归一化概率 # 将概率排序后输出 output_prob_dict = {} str2label = [&quot;cs.AI&quot;, &quot;cs.CE&quot;, &quot;cs.cv&quot;, &quot;cs.DS&quot;, &quot;cs.IT&quot;, &quot;cs.NE&quot;, &quot;cs.PL&quot;, &quot;cs.SY&quot;, &quot;math.AC&quot;, &quot;math.GR&quot;, &quot;math.ST&quot;] # 降序排列 for i in range(output_logit.shape[-1]): output_prob_dict[str2label[i]] = output_prob[0][i] out_prob_dict_sorted = sorted(output_prob_dict.items(), key=lambda x:x[1], reverse=True) # 打印 count = 1 for (key, val) in out_prob_dict_sorted: if count &gt; 5: # 打印概率前5的结果 break else: print([key, float(val)]) count = count + 1 # 定义 主函数 def main(): input_text = input(&quot;Please input your text: &quot;) bert_classification_prediction(input_text) 2.3 实验2.3.1 tsv文件范例 id label text 1 Label 1 Text 1 2 Label 2 Text 2 3 Label 3 Text 3 实际使用详情参见 read_tsv_file(file_dir): 函数的文件读取逻辑。 2.3.2 训练参数及结果 Key Value Task Document Classification Dataset arXiv Category 11 Train 2000 Eval 300 Batch Size 8(Max) Input Length 512 Tokens Epoch 8 Training Time 20min Accuracy 76.7% 2.4 注解2.4.1 tokenizertrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512, return_tensors=&quot;pt&quot;) truncation：多出裁剪 padding：不足填充 return_tensor=”pt”：‘tf’: Return TensorFlow tf.constant objects.‘pt’: Return PyTorch torch.Tensor objects.‘np’: Return Numpy np.ndarray objects. 文档链接：tokenizer方法 2.4.2 MyDatasetdef __getitem__(self, idx): item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} item[&#39;labels&#39;] = torch.tensor(self.labels[idx]) return item item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} 将被tokenizer编码后的encoding字典中的各项提取出来，包括input_ids、attention_mask。 item[&#39;labels&#39;] = torch.tensor(self.labels[idx]) 将labels也加入到item字典中，和input_ids、attention_mask并列。 2.4.3 MyTrainer见附录。 附录Trainer的Parameters &#39;&#39;&#39; Parameters: output_dir (:obj:`str`): The output directory where the model predictions and checkpoints will be written. overwrite_output_dir (:obj:`bool`, `optional`, defaults to :obj:`False`): If :obj:`True`, overwrite the content of the output directory. Use this to continue training if :obj:`output_dir` points to a checkpoint directory. do_train (:obj:`bool`, `optional`, defaults to :obj:`False`): Whether to run training or not. This argument is not directly used by :class:`~transformers.Trainer`, it&#39;s intended to be used by your training/evaluation scripts instead. See the `example scripts &lt;https://github.com/huggingface/transformers/tree/master/examples&gt;`__ for more details. do_eval (:obj:`bool`, `optional`): Whether to run evaluation on the validation set or not. Will be set to :obj:`True` if :obj:`evaluation_strategy` is different from :obj:`&quot;no&quot;`. This argument is not directly used by :class:`~transformers.Trainer`, it&#39;s intended to be used by your training/evaluation scripts instead. See the `example scripts &lt;https://github.com/huggingface/transformers/tree/master/examples&gt;`__ for more details. do_predict (:obj:`bool`, `optional`, defaults to :obj:`False`): Whether to run predictions on the test set or not. This argument is not directly used by :class:`~transformers.Trainer`, it&#39;s intended to be used by your training/evaluation scripts instead. See the `example scripts &lt;https://github.com/huggingface/transformers/tree/master/examples&gt;`__ for more details. evaluation_strategy (:obj:`str` or :class:`~transformers.trainer_utils.IntervalStrategy`, `optional`, defaults to :obj:`&quot;no&quot;`): The evaluation strategy to adopt during training. Possible values are: * :obj:`&quot;no&quot;`: No evaluation is done during training. * :obj:`&quot;steps&quot;`: Evaluation is done (and logged) every :obj:`eval_steps`. * :obj:`&quot;epoch&quot;`: Evaluation is done at the end of each epoch. prediction_loss_only (:obj:`bool`, `optional`, defaults to `False`): When performing evaluation and generating predictions, only returns the loss. per_device_train_batch_size (:obj:`int`, `optional`, defaults to 8): The batch size per GPU/TPU core/CPU for training. per_device_eval_batch_size (:obj:`int`, `optional`, defaults to 8): The batch size per GPU/TPU core/CPU for evaluation. gradient_accumulation_steps (:obj:`int`, `optional`, defaults to 1): Number of updates steps to accumulate the gradients for, before performing a backward/update pass. .. warning:: When using gradient accumulation, one step is counted as one step with backward pass. Therefore, logging, evaluation, save will be conducted every ``gradient_accumulation_steps * xxx_step`` training examples. eval_accumulation_steps (:obj:`int`, `optional`): Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If left unset, the whole predictions are accumulated on GPU/TPU before being moved to the CPU (faster but requires more memory). learning_rate (:obj:`float`, `optional`, defaults to 5e-5): The initial learning rate for :class:`~transformers.AdamW` optimizer. weight_decay (:obj:`float`, `optional`, defaults to 0): The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in :class:`~transformers.AdamW` optimizer. adam_beta1 (:obj:`float`, `optional`, defaults to 0.9): The beta1 hyperparameter for the :class:`~transformers.AdamW` optimizer. adam_beta2 (:obj:`float`, `optional`, defaults to 0.999): The beta2 hyperparameter for the :class:`~transformers.AdamW` optimizer. adam_epsilon (:obj:`float`, `optional`, defaults to 1e-8): The epsilon hyperparameter for the :class:`~transformers.AdamW` optimizer. max_grad_norm (:obj:`float`, `optional`, defaults to 1.0): Maximum gradient norm (for gradient clipping). num_train_epochs(:obj:`float`, `optional`, defaults to 3.0): Total number of training epochs to perform (if not an integer, will perform the decimal part percents of the last epoch before stopping training). max_steps (:obj:`int`, `optional`, defaults to -1): If set to a positive number, the total number of training steps to perform. Overrides :obj:`num_train_epochs`. lr_scheduler_type (:obj:`str` or :class:`~transformers.SchedulerType`, `optional`, defaults to :obj:`&quot;linear&quot;`): The scheduler type to use. See the documentation of :class:`~transformers.SchedulerType` for all possible values. warmup_ratio (:obj:`float`, `optional`, defaults to 0.0): Ratio of total training steps used for a linear warmup from 0 to :obj:`learning_rate`. warmup_steps (:obj:`int`, `optional`, defaults to 0): Number of steps used for a linear warmup from 0 to :obj:`learning_rate`. Overrides any effect of :obj:`warmup_ratio`. logging_dir (:obj:`str`, `optional`): `TensorBoard &lt;https://www.tensorflow.org/tensorboard&gt;`__ log directory. Will default to `runs/**CURRENT_DATETIME_HOSTNAME**`. logging_strategy (:obj:`str` or :class:`~transformers.trainer_utils.IntervalStrategy`, `optional`, defaults to :obj:`&quot;steps&quot;`): The logging strategy to adopt during training. Possible values are: * :obj:`&quot;no&quot;`: No logging is done during training. * :obj:`&quot;epoch&quot;`: Logging is done at the end of each epoch. * :obj:`&quot;steps&quot;`: Logging is done every :obj:`logging_steps`. logging_first_step (:obj:`bool`, `optional`, defaults to :obj:`False`): Whether to log and evaluate the first :obj:`global_step` or not. logging_steps (:obj:`int`, `optional`, defaults to 500): Number of update steps between two logs if :obj:`logging_strategy=&quot;steps&quot;`. save_strategy (:obj:`str` or :class:`~transformers.trainer_utils.IntervalStrategy`, `optional`, defaults to :obj:`&quot;steps&quot;`): The checkpoint save strategy to adopt during training. Possible values are: * :obj:`&quot;no&quot;`: No save is done during training. * :obj:`&quot;epoch&quot;`: Save is done at the end of each epoch. * :obj:`&quot;steps&quot;`: Save is done every :obj:`save_steps`. save_steps (:obj:`int`, `optional`, defaults to 500): Number of updates steps before two checkpoint saves if :obj:`save_strategy=&quot;steps&quot;`. save_total_limit (:obj:`int`, `optional`): If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in :obj:`output_dir`. no_cuda (:obj:`bool`, `optional`, defaults to :obj:`False`): Whether to not use CUDA even when it is available or not. seed (:obj:`int`, `optional`, defaults to 42): Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use the :func:`~transformers.Trainer.model_init` function to instantiate the model if it has some randomly initialized parameters. fp16 (:obj:`bool`, `optional`, defaults to :obj:`False`): Whether to use 16-bit (mixed) precision training instead of 32-bit training. fp16_opt_level (:obj:`str`, `optional`, defaults to &#39;O1&#39;): For :obj:`fp16` training, Apex AMP optimization level selected in [&#39;O0&#39;, &#39;O1&#39;, &#39;O2&#39;, and &#39;O3&#39;]. See details on the `Apex documentation &lt;https://nvidia.github.io/apex/amp.html&gt;`__. fp16_backend (:obj:`str`, `optional`, defaults to :obj:`&quot;auto&quot;`): The backend to use for mixed precision training. Must be one of :obj:`&quot;auto&quot;`, :obj:`&quot;amp&quot;` or :obj:`&quot;apex&quot;`. :obj:`&quot;auto&quot;` will use AMP or APEX depending on the PyTorch version detected, while the other choices will force the requested backend. fp16_full_eval (:obj:`bool`, `optional`, defaults to :obj:`False`): Whether to use full 16-bit precision evaluation instead of 32-bit. This will be faster and save memory but can harm metric values. local_rank (:obj:`int`, `optional`, defaults to -1): Rank of the process during distributed training. tpu_num_cores (:obj:`int`, `optional`): When training on TPU, the number of TPU cores (automatically passed by launcher script). debug (:obj:`bool`, `optional`, defaults to :obj:`False`): When training on TPU, whether to print debug metrics or not. dataloader_drop_last (:obj:`bool`, `optional`, defaults to :obj:`False`): Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size) or not. eval_steps (:obj:`int`, `optional`): Number of update steps between two evaluations if :obj:`evaluation_strategy=&quot;steps&quot;`. Will default to the same value as :obj:`logging_steps` if not set. dataloader_num_workers (:obj:`int`, `optional`, defaults to 0): Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the main process. past_index (:obj:`int`, `optional`, defaults to -1): Some models like :doc:`TransformerXL &lt;../model_doc/transformerxl&gt;` or :doc`XLNet &lt;../model_doc/xlnet&gt;` can make use of the past hidden states for their predictions. If this argument is set to a positive int, the ``Trainer`` will use the corresponding output (usually index 2) as the past state and feed it to the model at the next training step under the keyword argument ``mems``. run_name (:obj:`str`, `optional`): A descriptor for the run. Typically used for `wandb &lt;https://www.wandb.com/&gt;`_ logging. disable_tqdm (:obj:`bool`, `optional`): Whether or not to disable the tqdm progress bars and table of metrics produced by :class:`~transformers.notebook.NotebookTrainingTracker` in Jupyter Notebooks. Will default to :obj:`True` if the logging level is set to warn or lower (default), :obj:`False` otherwise. remove_unused_columns (:obj:`bool`, `optional`, defaults to :obj:`True`): If using :obj:`datasets.Dataset` datasets, whether or not to automatically remove the columns unused by the model forward method. (Note that this behavior is not implemented for :class:`~transformers.TFTrainer` yet.) label_names (:obj:`List[str]`, `optional`): The list of keys in your dictionary of inputs that correspond to the labels. Will eventually default to :obj:`[&quot;labels&quot;]` except if the model used is one of the :obj:`XxxForQuestionAnswering` in which case it will default to :obj:`[&quot;start_positions&quot;, &quot;end_positions&quot;]`. load_best_model_at_end (:obj:`bool`, `optional`, defaults to :obj:`False`): Whether or not to load the best model found during training at the end of training. .. note:: When set to :obj:`True`, the parameters :obj:`save_strategy` and :obj:`save_steps` will be ignored and the model will be saved after each evaluation. metric_for_best_model (:obj:`str`, `optional`): Use in conjunction with :obj:`load_best_model_at_end` to specify the metric to use to compare two different models. Must be the name of a metric returned by the evaluation with or without the prefix :obj:`&quot;eval_&quot;`. Will default to :obj:`&quot;loss&quot;` if unspecified and :obj:`load_best_model_at_end=True` (to use the evaluation loss). If you set this value, :obj:`greater_is_better` will default to :obj:`True`. Don&#39;t forget to set it to :obj:`False` if your metric is better when lower. greater_is_better (:obj:`bool`, `optional`): Use in conjunction with :obj:`load_best_model_at_end` and :obj:`metric_for_best_model` to specify if better models should have a greater metric or not. Will default to: - :obj:`True` if :obj:`metric_for_best_model` is set to a value that isn&#39;t :obj:`&quot;loss&quot;` or :obj:`&quot;eval_loss&quot;`. - :obj:`False` if :obj:`metric_for_best_model` is not set, or set to :obj:`&quot;loss&quot;` or :obj:`&quot;eval_loss&quot;`. ignore_skip_data (:obj:`bool`, `optional`, defaults to :obj:`False`): When resuming training, whether or not to skip the epochs and batches to get the data loading at the same stage as in the previous training. If set to :obj:`True`, the training will begin faster (as that skipping step can take a long time) but will not yield the same results as the interrupted training would have. sharded_ddp (:obj:`bool`, :obj:`str` or list of :class:`~transformers.trainer_utils.ShardedDDPOption`, `optional`, defaults to :obj:`False`): Use Sharded DDP training from `FairScale &lt;https://github.com/facebookresearch/fairscale&gt;`__ (in distributed training only). This is an experimental feature. A list of options along the following: - :obj:`&quot;simple&quot;`: to use first instance of sharded DDP released by fairscale (:obj:`ShardedDDP`) similar to ZeRO-2. - :obj:`&quot;zero_dp_2&quot;`: to use the second instance of sharded DPP released by fairscale (:obj:`FullyShardedDDP`) in Zero-2 mode (with :obj:`reshard_after_forward=False`). - :obj:`&quot;zero_dp_3&quot;`: to use the second instance of sharded DPP released by fairscale (:obj:`FullyShardedDDP`) in Zero-3 mode (with :obj:`reshard_after_forward=True`). - :obj:`&quot;offload&quot;`: to add ZeRO-offload (only compatible with :obj:`&quot;zero_dp_2&quot;` and :obj:`&quot;zero_dp_3&quot;`). If a string is passed, it will be split on space. If a bool is passed, it will be converted to an empty list for :obj:`False` and :obj:`[&quot;simple&quot;]` for :obj:`True`. deepspeed (:obj:`str`, `optional`): Use `Deepspeed &lt;https://github.com/microsoft/deepspeed&gt;`__. This is an experimental feature and its API may evolve in the future. The value is the location of its json config file (usually ``ds_config.json``). label_smoothing_factor (:obj:`float`, `optional`, defaults to 0.0): The label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded labels are changed from 0s and 1s to :obj:`label_smoothing_factor/num_labels` and :obj:`1 - label_smoothing_factor + label_smoothing_factor/num_labels` respectively. adafactor (:obj:`bool`, `optional`, defaults to :obj:`False`): Whether or not to use the :class:`~transformers.Adafactor` optimizer instead of :class:`~transformers.AdamW`. group_by_length (:obj:`bool`, `optional`, defaults to :obj:`False`): Whether or not to group together samples of roughly the same legnth in the training dataset (to minimize padding applied and be more efficient). Only useful if applying dynamic padding. report_to (:obj:`str` or :obj:`List[str]`, `optional`, defaults to :obj:`&quot;all&quot;`): The list of integrations to report the results and logs to. Supported platforms are :obj:`&quot;azure_ml&quot;`, :obj:`&quot;comet_ml&quot;`, :obj:`&quot;mlflow&quot;`, :obj:`&quot;tensorboard&quot;` and :obj:`&quot;wandb&quot;`. Use :obj:`&quot;all&quot;` to report to all integrations installed, :obj:`&quot;none&quot;` for no integrations. ddp_find_unused_parameters (:obj:`bool`, `optional`): When using distributed training, the value of the flag :obj:`find_unused_parameters` passed to :obj:`DistributedDataParallel`. Will default to :obj:`False` if gradient checkpointing is used, :obj:`True` otherwise. dataloader_pin_memory (:obj:`bool`, `optional`, defaults to :obj:`True`)): Whether you want to pin memory in data loaders or not. Will default to :obj:`True`. skip_memory_metrics (:obj:`bool`, `optional`, defaults to :obj:`False`)): Whether to skip adding of memory profiler reports to metrics. Defaults to :obj:`False`. &#39;&#39;&#39;","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"/tags/NLP/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"我的炉石卡组","slug":"我的炉石卡组","date":"2021-03-25T04:54:15.000Z","updated":"2021-11-26T07:18:17.358Z","comments":true,"path":"2021/03/25/我的炉石卡组/","link":"","permalink":"/2021/03/25/我的炉石卡组/","excerpt":"","text":"卡组api 调用自文章 炉石卡组代码解析 宇宙牧卡组一览 卡组代码AAEBAd35Ax7TCtcKuQ2BDsMWhRfgrAKDuwLYuwLRwQKWxALfxALmzAKJzQLo0AKQ0wLy7ALmiAPrmwPanQP8owP2sAOIsQORsQPIvgPOvgOm1QP21gO/4AOm7wMAAA== 选配包解析（5选2）选项1：唤醒造物者+死亡领主/克苏恩，破碎之劫（亡语包）选项2：寻求指引+死亡领主（发现包）选项3：黑暗主教本尼迪塔斯+克苏恩，破碎之劫（暗影包） 1 唤醒造物者 费用 属性 效果 价值 Combo 1费 任务 任务：己方召唤7个亡语，奖励：希望守护者阿玛拉（5/8/8，嘲讽，战吼：将己方英雄生命值置为40点） 亡语 体系，中后期的 回血 手段 搭配 雷诺+灵媒术，可实现理论 30+40+40+40=150点的中后期血量 1 寻求指引 费用 属性 效果 价值 Combo 1费 任务 任务1：使用2、3、4费牌各一张，奖励：从牌库中发现一张牌，任务2：使用5、6费牌各一张，奖励：从牌库中发现一张牌，任务3：使用7、8费牌各一张，奖励：圣徒泽瑞拉（5/8/8，嘲讽，战吼：将净化的碎片（10费法术：消灭敌方英雄）洗入牌库） 单卡 终结 对局 搭配 博学者普克尔特，可实现 做完任务后两回合终结对局 3 死亡领主（2/8） 费用 属性 效果 价值 Combo 3费 随从 嘲讽，亡语：将对手牌库中随机的一张随从牌置入敌方战场 亡语 体系，防快攻 非典型 5 黑暗主教本尼迪塔斯 费用 属性 效果 价值 Combo 3费 随从 对局开始时：若己方牌库中所有法术都为暗影法术，则进入暗影形态（英雄技能（2费）：造成两点伤害） 防快攻，中前期 控场 非典型 10 克苏恩，破碎之劫（6/6） 费用 属性 效果 价值 Combo 10费 随从 对局开始时：破碎为四张碎片，将超模的三张解牌及一张站场牌洗入牌库，使用四张碎片后：融合成 克苏恩，破碎之劫 并将其洗入牌库，战吼：造成30点伤害随机分配到所有敌人 单卡 终结 对局 非典型 卡组主体解析（28张）1 变色龙卡米洛斯（1/1） 费用 属性 效果 价值 Combo 1费 随从 如果这张牌在手牌中，每回合随机变为对方手牌中的一张牌（抽到的第一回合是本体） 窥屏 对手手牌 搭配 脏鼠/吞噬者穆坦努斯，可实现 精准打击对手key牌 1 灵魂之匣（1/3） 费用 属性 效果 价值 Combo 1费 随从 吸血，亡语：将 终极魂匣（7/6/8嘲讽，吸血，不可被敌方法术及英雄技能选中） 洗入牌库 亡语 体系，前期 回血站场，增加卡组中后期 厚度 非典型 2 暗言术：灭（暗影） 费用 属性 效果 价值 Combo 2费 法术 消灭选中 ATK&gt;=5 的随从 低费高攻 单体解 搭配 女王，可实现 8费精神控制 2 暗言术：痛（暗影） 费用 属性 效果 价值 Combo 2费 法术 消灭选中 ATK&lt;=3 的随从 低费低攻 单体解 搭配 脏鼠，可实现 低费解决低攻随从类Key牌（如法师的火妖3/2/4 &amp; 巫师学徒2/3/2） 2 灵媒术（暗影） 费用 属性 效果 价值 Combo 2费 法术 将选中随从的复制置入己方手牌 随从类Key牌 复制，根据对局提供 选择 非典型 2 暗影视界（暗影） 费用 属性 效果 价值 Combo 2费 法术 在己方牌库中 发现 一张法术的复制 发现 解牌，根据对局提供 选择 非典型 2 卑劣的脏鼠（2/6） 费用 属性 效果 价值 Combo 2费 随从 嘲讽，战吼：使你的对手从手牌中随机召唤一个随从 防快攻，销毁 对手随从类Key牌 搭配 铜须，可实现 高容错销毁对手的随从类Key牌 2 了不起的杰弗里斯（3/2） 费用 属性 效果 价值 Combo 2费 随从 战吼：若己方牌库中没有相同的牌，发现一张完美的牌 宇宙 体系，发现 解牌、斩杀、过牌、保命、大哥选项，根据对局提供 选择 非典型 3 布莱恩·铜须（2/4） 费用 属性 效果 价值 Combo 3费 随从 己方随从的 战吼 触发两次 实现更好的 战吼 效果 搭配 某些战吼类随从，可实现 原有战吼效果的两倍 3 拉祖尔女士（3/2） 费用 属性 效果 价值 Combo 2费 随从 战吼：发现 一张对方手牌的复制 窥屏 对手手牌 非典型 4 祖达克仪祭师（3/9） 费用 属性 效果 价值 Combo 4费 随从 嘲讽，战吼：为对手召唤3个随机的1费随从 防快攻，污染 对手坟场，单卡 针对 大哥牧 非典型 4 暗言术：毁（暗影） 费用 属性 效果 价值 Combo 4费 法术 消灭所有ATK&gt;=5的随从 低费高攻 群体解 搭配 女王，可实现 后期逆风返场 4 卡扎库斯（3/3） 费用 属性 效果 价值 Combo 4费 随从 战吼：若己方牌库没有相同的牌，创建一张自定义法术 宇宙 体系，发现 解牌、直伤、过牌、保命、返场选项，根据对局提供 选择，概率抽到变羊选项 针对 大哥牧 非典型 5 淤泥喷射者（3/5） 费用 属性 效果 价值 Combo 5费 随从 嘲讽，亡语：召唤一个1/1/2具有 嘲讽 的软泥怪 亡语 体系，防快攻 非典型 5 黏指狗头人（4/4） 费用 属性 效果 价值 Combo 5费 随从 战吼：偷取对手的武器 销毁 对手武器类Key牌，单卡 针对 弑君贼、剽窃贼 非典型 5 缚链者拉兹（5/5） 费用 属性 效果 价值 Combo 5费 随从 战吼：若己方牌库没有相同的牌，将英雄技能置为0费 宇宙 体系，中前期低成本 回血、中后期低成本 控场 搭配 暗影收割者安度因，可实现 中后期低成本的直伤 5 博学者普克尔特（4/5） 费用 属性 效果 价值 Combo 5费 随从 战吼：将己方牌库按费用消耗从高到低排序 中后期稳定的 定向 检索 搭配 寻求指引（净化的碎片）/克苏恩，破碎之劫，可实现 中后期稳定的斩杀 5 希尔瓦娜斯·风行者（5/5） 费用 属性 效果 价值 Combo 6费 随从 亡语：随机获得一个敌方随从的控制权 亡语 体系，单卡 站场 搭配 高攻解牌，可实现 解场返场 6 雷诺·杰克逊（4/6） 费用 属性 效果 价值 Combo 6费 随从 战吼：若己方牌库没有相同的牌，为己方英雄回复所有生命值 宇宙 体系，回血 非典型 7 心灵尖啸（暗影） 费用 属性 效果 价值 Combo 7费 法术 将所有随从洗入对手牌库 污染 对手牌库，无副作用的 群体解 非典型 7 大主教本尼迪塔斯（4/6） 费用 属性 效果 价值 Combo 7费 随从 战吼：复制对手的牌库并洗入己方牌库 防爆牌，单卡 针对 爆牌贼、宇宙术（提克特斯） 非典型 7 灵魂之镜（暗影） 费用 属性 效果 价值 Combo 7费 法术 召唤所有敌方随从的复制，并使敌方随从与其对应复制相互攻击 群体解，逆风返场，为 亡语 体系积累素材（唤醒造物者、恩佐斯） 非典型 7 吞噬者穆坦努斯（4/4） 费用 属性 效果 价值 Combo 7费 随从 战吼：吃掉对手手牌中的一张随机随从牌，并在7/4/4原有基础上叠加该随从的ATK、HP 销毁 对手随从类Key牌 非典型 8 暗影收割者安度因（护甲+5） 费用 属性 效果 价值 Combo 8费 死骑 战吼：消灭所有ATK&gt;=5的随从，英雄技能（2费）：造成2点伤害，使用卡牌后重置英雄技能 高攻 群体解，中后期 控场 搭配 缚链者拉兹，可实现 中后期低成本的直伤 8 耶比托·乔巴斯（6/6） 费用 属性 效果 价值 Combo 8费 随从 战吼：从牌库中抽取两张随从牌，将抽到的牌变为1/1/1 过牌，中后期较稳定的 定向 检索 非典型 9 黑曜石雕像（4/8） 费用 属性 效果 价值 Combo 9费 随从 嘲讽，吸血，亡语：随机消灭一个敌方随从 亡语 体系，中后期单卡 回血站场 非典型 9 红龙女王阿莱克斯塔萨（8/8） 费用 属性 效果 价值 Combo 9费 随从 战吼：若己方牌库没有相同的牌，将两张其他龙牌置入己方手牌并置为0费 宇宙 体系，后期单回合 返场 非典型 10 恩佐斯（5/7） 费用 属性 效果 价值 Combo 10费 随从 战吼：召唤所有本局对战中己方死亡的具有 亡语 的随从 亡语 体系，中后期单回合 返场 非典型 宇宙萨卡组一览 卡组代码AAEBAaoIHvUEsgaTCfoOwxb6qgKIrwL5vwKXwQLHwQLfxAKbywKr5wLz5wLg6gLv8QLv9wKtkQO9mQP8owPPpQPhpQPhqAOIsQPDtgPgzAOczgP21gOm7wOvnwQAAA== 宇宙术卡组一览 卡组代码AAEBAcn1Ah7OBtsGkgfOB8wI1hHDFoUX2LsC38QC58sCxcwCks0Cl9MC6OcCnPgC2p0D/KMD66wD7qwDkbEDv7kDxLkDkt4DzuED9uMD+OMDpu8D/voDh/sDAAA=","categories":[{"name":"生活","slug":"生活","permalink":"/categories/生活/"}],"tags":[{"name":"游戏","slug":"游戏","permalink":"/tags/游戏/"}],"keywords":[{"name":"生活","slug":"生活","permalink":"/categories/生活/"}]},{"title":"大阪隔离记录（在留卡再入国）","slug":"大阪隔离记录（在留卡再入国）","date":"2021-02-17T14:30:48.000Z","updated":"2021-11-25T09:15:46.000Z","comments":true,"path":"2021/02/17/大阪隔离记录（在留卡再入国）/","link":"","permalink":"/2021/02/17/大阪隔离记录（在留卡再入国）/","excerpt":"","text":"大阪隔离记录（在留卡再入国）1 入境前准备1.1 资料准备 在留卡、护照等必要证件 72小时内核酸证明 [1]（中英版本 必须鼻咽拭子 咽拭子不可以 标本类型也最好写着英文版的鼻咽拭子） 核酸证明的誓约书（日本国驻华大使馆官网） 1.2 预定准备 航班预定（最好定中国航司直航 转机有很高的被取消风险） 酒店预定 [2]（共15晚 官方说法是14天不包括入境单天） 接机预定 [3]（携程上关西机场到大阪市区接机要700人民币左右） 1.3 一些解释[1] 截止写文之日，日本多个县还处于紧急状态之中，入境日本的人仅包括日本人、在留卡再入国以及永驻，不包括新签证以及之前谈好的商务签相关通道。[2] 入境日本需要在不使用公共交通的前提下，在入境时报告的逗留地址或自己的住址自主隔离14天，期间避免乘坐公共交通，避免三密。因此若没有方法乘坐私车前往住址，则只能选择在酒店隔离。因为最近一直处于紧急状态，旅游业淡季，实际上大阪市内现在有很多便宜酒店。[3] 关西机场建立在一个人工岛之上，与内陆仅靠跨海大桥相连，行人无法步行通过。因此，想要离开机场前往酒店进行隔离不能依靠步行而只能依靠接机。否则只能选择步行关西机场内唯一的酒店进行隔离，价格比较昂贵。 2 入境当天流程2.1 环境介绍起飞：上海浦东国际机场到达：大阪关西国际机场航班：吉祥航空HO1333 2.2 入境流程 值机前，工作人员会要求乘客填写日本入境要求填写的问卷，并生成二维码。该二维码需要截屏保存。 值机前，工作人员还会要求乘客填写中国出境的健康申报书，并生成二维码。该二维码也需要截屏保存。 值机前，工作人员会检查乘客的72小时内PCR证明以及相关证件确保乘客拥有日本政府要求的入境基本条件。 值机办理托运后，海关离境前，工作人员会扫描2中要求的出境健康申报书，符合要求后才会进行出境相关手续的办理和盖章。 登机、起飞（飞行过程中照例会要求填写报税的项目）、到达。 到达机场后，会有很多会中文的中日工作人员帮助办理入境。 入境前，排队按顺序领试管进行唾液的新冠检查（试管和资料上会写着编号）。 入境前，扫描1中要求的二维码，提交核算证明原件及誓约书，填写新发的遵守隔离政策的誓约书，等待唾液检查的结果（大屏幕上会显示已经出结果的编号）。 核酸阴性结果显示后，拿着手上的誓约书和其他资料换一张红色的纸片以表示核酸检查没问题，之后找行李并办理出境手续。 联系接机并正常办理酒店入住（酒店并不会把你当成隔离人员特殊照顾）。 3 总结因为前期查了很多资料，做了各种准备，这次紧急状态期间的入境总耗时实际上也只有一个多小时，最后也在天黑之前顺利到达酒店办理了入住。虽然结果是很轻松也很顺利，但实际上在没顺利通过日本海关之前，谁都没办法掉以轻心。希望这个流程记录能或多或少帮到你，也希望还没入境的大家都能顺利入境。 附：隔离期间的三餐 便利店便当大赏Day 1Family Mart含税价格： 汉堡肉便当@598日元 热狗 * 2@128日元 * 2 四川麻婆豆腐盖饭@430日元 酱油拉面@598日元 Day 2Family Mart含税价格： 🍌@108日元 🍣卷@430日元 奶油巧克力豆夹心🍞 * 2@138日元 * 2 汉堡肉🍱@598日元 金枪鱼🥗@220日元 炸鸡块🍱@460日元 Day 37-11不含税价格： 脆皮🌭 * 2@150日元 * 2 🍣卷@298日元 巧克力🍩 * 2@178日元 * 2 金枪鱼🥗@198日元 酱油方便🍜@128日元 炸鸡块🍱@540日元 培根芝士🍝@398日元 Day 4Family Mart含税价格： 鸡油方便🍜@149日元 培根🍞 * 2@138日元 * 2 🍣卷@430日元 牛肉盖饭@430日元 广岛风什锦烧@550日元 Day X基本就这几种，重复不发了。。。","categories":[{"name":"生活","slug":"生活","permalink":"/categories/生活/"}],"tags":[{"name":"COVID-19","slug":"COVID-19","permalink":"/tags/COVID-19/"}],"keywords":[{"name":"生活","slug":"生活","permalink":"/categories/生活/"}]},{"title":"如何理解 CNN","slug":"如何理解 CNN","date":"2021-02-10T10:32:32.000Z","updated":"2021-02-10T10:32:32.000Z","comments":true,"path":"2021/02/10/如何理解 CNN/","link":"","permalink":"/2021/02/10/如何理解 CNN/","excerpt":"","text":"本文参考[1] Gradiant-Based Learning Applied to Document Recognition[2] CS231n Convolutional Neural Networks for Visual Recognition /Stanford 1 如何理解CNN1.1 什么是CNNCNN模型 通常被认为始于Yann LeCun在1998年发表的文章 Gradiant-Based Learning Applied to Document Recognition，该模型通常被称为 LeNet。 图1.1 LeNet的模型结构[1] 图1.1 为原文[1]中的配图，描述了一个两层卷积的CNN模型。 CNN（Convoolutional Neural Network）： CNN，卷积神经网络，是一种使用卷积层提取图像特征，再经过池化层保留重要特征的深度学习模型。 模型通常用于图像处理，能够很好的提取图像特征。 符号理解： INPUT：输入图像是一个32*32的图像。虽然图像显示的是个A，但原文写的是Digits Recognition。 Convolutions：将32*32的输入图像经过一个5*5的卷积核得到28*28的输出图像。这个卷积核即为传统神经网络中的参数矩阵，在该模型中第一层卷积共有6个经过初始化的卷积核。卷积核的概念可以参考数字图像处理算法中的滤波器，类似滤镜，例如 高斯滤波。 Feature Maps：经过卷积核得到的中间输出图像被称为特征图，模型希望通过多个特征图来获取不同的特征。个人感觉该机制与Multi-Head Mechanism中的Multi-Head很相似。 Subsampling：下采样，减少输出维度，在该领域主要体现为Pooling，即池化。个人理解池化的目的是用于保留并突出重要信息，下文将详细说明池化的实现。 Full connection：全连接层，多称为Fully Connected Layer。用于将高维张量映射为向量，最终经过高斯分布得到对应每个Digits的概率。 1.2 LeNet的模型架构1.2.1 图像输入 图1.2 LeNet的输入图像[1] 将32*32的原始图像输入到模型中。 1.2.2 第一层卷积 图1.3 LeNet的第一层卷积[1] 第一个卷积层将输入的32*32的原始图像经过6个5*5的初始化的卷积核分别得到6个28*28的特征图C1。卷积的本质我认为就是element-wise的加权求和。 1.2.3 第一层下采样 图1.4 LeNet的第一层下采样[1] 下采样在该模型上的体现即为池化。池化也可以理解为是卷积的一种，只不过池化有时候可能不存在有意义的卷积核（广义的卷积），比如最常用的池化方法max-pooling，如图1.5所示： 图1.5 max-pooling示意图[2] 在max-pooling中，上一层28*28的特征图C1将被分为14*14个2*2的block，模型将选出每个block中的最大值，并将最大值作为这个block的结果输出到下一层中，最终生成下一层14*14的特征图S2。 池化方法有很多种： max-pooling（取block最大值） mean-pooling（取block平均值，即经过一个平均卷积核） 高斯池化（经过一个高斯模糊卷积核） 可训练池化（经过一个可训练的初始化的卷积核） 但在Le-Net中，原文提到：The four inputs to a unit in S2 are added, then multiplied by a trainable coefficient, and added to a trainable bias …… Layer S2 has 12 trainable parameters …… 即LeNet在下采样部分并没有使用现在流行的所谓pooling方法，而是将block内的像素值相加后乘上参数w再加上偏置b。最终针对6个不同的特征图将会有6对(w, b)的组合，即12个可训练参数。 我想这也可以算是可训练池化的一种。 1.2.4 第二层卷积与下采样 图1.6 第二层卷积与下采样[2] 在第二轮的卷积中，模型通过一定的规则将经过第一轮卷积池化的6个特征图S2映射为16个特征图。 表1.1 第二层卷积的映射关系[2] 在LeNet中，原文提到这样映射的原因以及如何映射： Why not connect every S2 feature map to every C3 feature map?The reason is twofold. First, a non-complete connection sheme keeps the number of connection within reasonable bounds. More importantly, it forces a break of symmetry in the network.Different feature maps are forced to extract different(hopefully complementary) features because they get different sets of inputs. The rationale behind the connection sheme in the table Ⅰ is the followings.The first six C3 feature maps take inputs from every contiguous subsets of three feature maps in S2. (Column 0-5)The next six take input from every contiguous subset of four. (Column 6-11)The next three take input from some discontiiguous subsets of four. (Column 12-14)Finally the last one takes input from all S2 feature maps. (Column 15) Layer C3 has 1516 trainable parameters …… 即LeNet在第二层卷积中： 没有使用6张S2特征图到16张C3特征图的全映射，而是使用了 表1.1 的映射规律。 映射规律中，列0-5 使用了循环连续的三连特征图（理解为tri-gram），列6-11 使用了循环连续的四连特征图（理解为qua-gram），列12-14 使用了循环不连续的四连特征图（理解为两个bi-gram），列15 使用了S2全部特征图。 对于每个特征图到特征图的映射，需要一个5*5的初始化的卷积核，故共需要 25 * (3*6 + 4*6 + 4*3 + 6) + 16 = 1516 个参数。其中，25是每个卷积核的参数量，16是16个列每列对应偏置的参数量。 在第二层下采样中，采取了和第一层C1-S2相同的池化方式。通过选取无重叠的2*2block的最大值对C3进行max-pooling，将10*10的C3特征图池化为5*5的S4特征图。 1.2.5 第三层卷积以及全连接层 图1.7 第三层卷积以及全连接层[2] 第三层池化将第二层卷积池化后5*5的S4特征图再通过一定的规则经过5*5的卷积核卷积后得到120个1*1的特征图。 表1.2 第三层卷积的映射关系 0 1 … 120 0 X X … X 1 X X … X … … … … … 16 X X X X 在 图1.7 中，S4-C5下方的注释将这个过程称为全连接。而在LeNet原文中是这样描述的：C5 is labeled as a convolutional layer, instead of a fully-connected layer, because if LeNet-5 input were made bigger with everything else kept constant, the feature map dimension would be larger than 1*1.即C5在 图1.7 中表现为全连接只是一个巧合，实际情况当 1.2.1 的输入图像像素量变大时，可能会出现C5为X*X。 C5经过一个全连接层将120维的向量转换为84个（似乎是为了与ASCII匹配），在经过一个高斯连接层（还没来得及搞明白这层）映射为一个10维向量（与10个数字匹配）。 1.3 待续2 TextCNN待续。。。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"/tags/NLP/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"2020 全年更新日志","slug":"2020 全年更新日志","date":"2021-02-10T10:28:14.000Z","updated":"2021-02-10T10:28:14.000Z","comments":true,"path":"2021/02/10/2020 全年更新日志/","link":"","permalink":"/2021/02/10/2020 全年更新日志/","excerpt":"","text":"2020 全年更新日志2020.11.30更新日志 歌词api：http://music.163.com/api/song/media?id=1408319824 bilibili播放器： 2020.11.12更新日志 为上次更新的新歌单页重新设置toc对齐offset。 新日志中涉及了自定义歌词的html结构设置。 2020.09.28更新日志 将Valine评论更新并引入。 通过在sakara_app.js中使用$(document).ready(function(){})解决Valine刷新之后不执行的问题。 优化了歌单、归档、关于三个界面的标题显示，并将这三个界面的页面结构进行统一。 2020.08.09更新日志 将mathjax更新为国内cdn（https://cdn.baomitu.com/mathjax）。 将mathjax位置由common-article.ejs调整至footer.ejs（common-article.ejs部分似乎受异步加载限制，刷新后无法加载js，故将mathjax调整至不受限制的footer部分） 2020.07.17更新日志 调整toc与实际文章标题对应的offset，使对齐（sakara_app.js）。 调整搜索菜单的tag显示数量以及tag、分类、文章的显示顺序（InsightSearch.js）。 2020.07.16更新日志 增加表格边框 2020.07.05更新日志 将centerbg-background的参考系设为center center，实现对中心的缩放（style.css）。 优化手机端观感。将手机端沉浸背景去掉，因为适配时比例会出现问题。通过@media限制低宽度（&lt;400px）分辨率的页面将centerbg高度设为固定，并将page的背景设为白（style.css）。 优化手机端观感。为主页post的某些宽度情况添加阴影以及margin（style.css）。 优化归档、标签页观感。为归档和标签页增加半透明背景内衬（style.css）。 2020.07.03更新日志 去掉首页视频以及视频播放按钮（style.css） 2020.06.18更新日志 沉浸背景（sakara_app.js中赋予content与centerbg同样的style） 樱花特效（footer.ejs中添加特效插件） 为post页本地图片pattern-center以及toc-list添加border（style.css），设置pattern-center的max-width为1080px（max-width可以保证与主页面大小同时缩放，width的话溢出部分不会缩进） 主页设置content为透明，post-list为不透明；博文页设置entry-content（博文页文字部分）为半透明，toc为半透明。 2020上半年更新日志4月： 优化了音乐播放器的歌词宽度匹配。 5月： 调整歌单界面播放器，解决了之前列表与主题重叠的问题，将歌词居中，限制播放器最大宽度500px。 为歌单界面增加了与文章界面相同的标题分层链接toc。 调整二级标题的指示符号为蓝色的 | 。 调整了主页播放器歌词显示逻辑，禁止其在歌单页显示歌词与其他播放器冲突。（aplayer.ejs） 更新了关于本人页。（botui.js）","categories":[{"name":"生活","slug":"生活","permalink":"/categories/生活/"}],"tags":[{"name":"更新日志","slug":"更新日志","permalink":"/tags/更新日志/"}],"keywords":[{"name":"生活","slug":"生活","permalink":"/categories/生活/"}]},{"title":"2021 年前总结","slug":"2021 年前总结","date":"2021-02-10T10:23:30.000Z","updated":"2021-02-10T10:23:30.000Z","comments":true,"path":"2021/02/10/2021 年前总结/","link":"","permalink":"/2021/02/10/2021 年前总结/","excerpt":"","text":"2021 年前总结现在是2021.02.10下午，去年的这会儿我大概正在为回国机票突然被取消而感到慌张。过去的一年对我来说非常特别，毕竟是十多年求学生涯的第一次家里蹲学习。因为这样那样的事情，虽然总的来说谈不上全力以赴，至少也算是稍微学到了些东西吧。 2020.03学习： BERT论文阅读 Speech and LanguageProcessing基础理解 Stanford CS224N 日语语法Chapter 4 生活： CSGO入门 2020.04学习： BERTSUM论文阅读 开始组会汇报 日语语法Chapter 4 2020.05学习： Windows10 + Python3.7 + Tensorflow1.15.0 + GTX1060 + CUDA10.0环境配置 BERT预训练模型试玩 2020.06学习： SiameseBERT论文阅读 SentenceBERT论文阅读 RoBERTa论文阅读 RNN&amp;LSTM理解 Attention Mechanism理解 日语语法Chapter 5 2020.07学习： TinyBERT论文阅读 Transformer理解 BERT理解 日语语法Chapter 5 2020.08学习： opencv添加水印 BERT源码：run_classifier.py逐行注释 生活： 阳菜便服Ver. &amp; 和服Ver. 多图层尝试 2020.09学习： BERT源码：modeling.py逐行注释 日语语法Chapter 6 2020.10学习： 长文本分类 python的文件IO 2020.11生活： 日语歌词翻译尝试 2021.01学习： 日语单词N2 P01-P40 2021.02学习： CNN理解 附 2021 年后展望学习： 毕业设计 机器学习基础 数据结构与算法 Leetcode GTX1060 深度学习系列 生活： 摊煎饼进阶练习 如果可以的话让我去一次镰仓吧 如果可以的话让我去一次京都吧 如果可以的话让我去一次千叶吧 如果可以的话让我去一次秋叶原吧","categories":[{"name":"生活","slug":"生活","permalink":"/categories/生活/"}],"tags":[{"name":"总结","slug":"总结","permalink":"/tags/总结/"}],"keywords":[{"name":"生活","slug":"生活","permalink":"/categories/生活/"}]},{"title":"分类任务的F1-score","slug":"分类任务的F1-score","date":"2021-01-21T14:41:34.000Z","updated":"2021-01-21T14:41:34.000Z","comments":true,"path":"2021/01/21/分类任务的F1-score/","link":"","permalink":"/2021/01/21/分类任务的F1-score/","excerpt":"","text":"分类任务的F1-score背景：BERT的分类器源码run_classifier.py的评估指标部分只有accuracy和loss，没有F1-score。详情见 metric_fn 。 二分类模型的准确率、精确率、召回率以及F-score对于二分类模型： 预测 1’ 0’ Total 实值 1 TP（真正例） FN（假反例） P 0 FP（假正例） TN（真反例） N Total P’ N’ S 准确率（Accuracy）：\\begin{equation}Accuracy = \\frac{TP + TN}{TP + FN + FP + TN} = \\frac{TP + TN}{S}\\end{equation} 精确率/查准率（Precision）：\\begin{equation}Precision = \\frac{TP}{TP + FP} = \\frac{TP}{P’}\\end{equation} 召回率/查全率（Recall）：\\begin{equation}Recall = \\frac{TP}{TP + FN} = \\frac{TP}{P}\\end{equation} F-score：\\begin{equation}F-score = \\frac{(1 + \\beta^2) \\cdot precision \\cdot recall}{\\beta^2 \\cdot precision + recall}\\end{equation}F-score的本质是Precision和Recall的加权调和平均。（加权的积在和上飞） F1-score：当 $ \\beta^2 = 1 $ 时：\\begin{equation}F-score = \\frac{2 \\cdot precision \\cdot recall}{precision + recall}\\end{equation} 多分类任务的F1-score在多分类任务中，由于没有固定的正反例，没有统一的精确率、召回率等定义。通常有两种算法F1_micro以及F1_macro。 对于N分类模型中的第i类有： 预测 i’ 其他’ Total 实值 i TP1（真正例） FN1（假反例） P 其他 FP1（假正例） TN1（真反例） N Total P’ N’ S 这样的 针对第i类的实值-预测值 困惑矩阵 可通过总的 N类实值-N类预测值 困惑矩阵 查表得到，共可得到N个。 根据这N个困惑矩阵的数据给出如下定义： F1_micro：\\begin{equation}Recall_{micro} = \\frac{\\sum_{i=1}^{N}TP_i}{\\sum_{i=1}^{N}TP_i + \\sum_{i=1}^{N}FN_i}\\\\Precision_{micro} = \\frac{\\sum_{i=1}^{N}TP_i}{\\sum_{i=1}^{N}TP_i + \\sum_{i=1}^{N}FP_i}\\\\F1_{micro} = \\frac{2 \\cdot precision \\cdot recall}{precision + recall}\\end{equation}\\begin{equation}\\because\\sum_{i=1}^{N}TP_i + \\sum_{i=1}^{N}FN_i = \\sum_{i=1}^{N}TP_i + \\sum_{i=1}^{N}FP_i = S\\\\\\therefore Accuracy = \\frac{\\sum_{i=1}^{N}TP_i}{S} = Recall_{micro} = Precision_{micro} = F1_{micro}\\end{equation}意义：F1_micro将所有样本都视为具有同样权重的样本，并针对全部样本作为整体，计算整体的Recall和Precision，并以此进一步计算F1-score。缺点：对于类别样本不均衡的数据集，如A：B = 10：1。由于F1_micro将所有样本都视为同样权重的样本，A类样本的统计学特征将分配到10倍于B类样本的权重，造成不同类别样本的权重分配不均匀。 F1_macro：\\begin{equation}Recall_i = \\frac{TP_i}{TP_i + FN_i}\\\\Precision_i = \\frac{TP_i}{TP_i + FP_i}\\\\F1_i = \\frac{2 \\cdot precision_i \\cdot recall_i}{precision_i + recall_i}\\\\F1_{macro} = \\frac{1}{N}\\sum_{i=1}^{N}F1_i\\end{equation}意义：F1_macro将所有类别都视为具有同样权重的类别，并针对每个类别作为整体，分别计算每个类别的Recall和Precision，并以此进一步计算F1-score。优点：对于类别样本不均衡的数据集，如A：B = 10：1。由于F1_macro将所有类别都视为同样权重的类别，A类样本的统计学特征将分配到与B类样本相同的权重，不会造成不同类别样本的权重分配不均匀。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"python","slug":"python","permalink":"/tags/python/"},{"name":"数据处理","slug":"数据处理","permalink":"/tags/数据处理/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语单词 N2 P31-P40","slug":"日语单词 N2 P31-P40","date":"2021-01-20T13:59:28.000Z","updated":"2021-01-20T13:59:28.000Z","comments":true,"path":"2021/01/20/日语单词 N2 P31-P40/","link":"","permalink":"/2021/01/20/日语单词 N2 P31-P40/","excerpt":"","text":"课程来自youtube@日本語の森 N2 ことばP31 女性（じょせい）の心理（しんり）に応（おう）じて贈（おく）り物をする。 年齢（ねんれい）を聞いて全員（ぜんいん）が返事（へんじ）をする。 ５対２で負（ま）けてしまい、ファン（fan）に申（もう）し訳（わけ）ない。 单词/搭配 释义 合（あ）わせる 他动词 - 合并、配合、对照、引荐 答（こた）える 自动词 - 回答、解答 同 - 応え、報え 問（と）う 他动词 - 询问、调查 勝（か）つ 自动词 - 胜利 P32 曖昧（あいまい）な知識（ちしき）を述（の）べる。 出張（しゅっちょう）を減（へ）らすことを希望（きぼう）する。 正面（しょうめん）玄関（げんかん）に咲（さ）いていた花が枯（か）れた。 单词/搭配 释义 出入り口（でいりぐち） 名词 - 出入口 咲く 自动词 - （花）开 開（ひら）く 自动词 - 开 開（あ）く 自动词 - 开始（商店）、打开 他动词 - 张开（眼睛、嘴） 同 - 明く、空く 閉（し）まる 自动词 - 关闭、关门 閉（と）じる 自动词、他动词 - 关闭、合（书）、盖（容器） P33 どの店で扇風機（せんぷうき）を買っても価格（かかく）に違いはない。 兄は真面目（まじめ）な割に成績（せいせき）が悪い。 踊（おど）りを終えた人のみ帰っていい。 单词/搭配 释义 値段（ねだん） 名词 - 价格 真面目（まじめ） 形容动词 - 认真 割（わり）に 副词 - 比较（相对）、意外 踊（おど）る 他动词 - 跳舞 自动词 - 颠簸（汽车）、不平整（印刷） 終（お）える 他动词 - 完成、结束 のみ 副词 - 仅仅、只有 P34 道路（どうろ）が込んでいて通勤（つうきん）に余計（よけい）な時間がかかった。 ただの観光（かんこう）ではなくて科学（かがく）を勉強するために来た。 去年（きょねん）国際的（こくさいてき）に流行（りゅうこう）した柄（がら）の服を買う。 单词/搭配 释义 生物学（せいぶつがく） 名词 - 生物学 物理学（ぶつりがく） 名词 - 物理学 化学（かがく） 名词 - 化学 数学（すうがく） 名词 - 数学 文学（ぶんがく） 名词 - 文学 天文学（てんもんがく） 名词 - 天文学 学問（がくもん） 名词 - 学科 柄（がら） 名词 - 体格、品格、资格、（衣服的）花纹 P35 祭りに行くこと毎回（まいかい）写真（しゃしん）を沢山撮る。 日程（にってい）はそのままで、喫茶店（きっさてん）を変える。 のんびり絵本（えほん）を読んで休暇（きゅうか）を過ごした。 单词/搭配 释义 注文（ちゅうもん） 名词、他动词 - 订货、订单、要求 映す 他动词 - 映照、放映（影像） 映る 自动词 - 反射、相称 のんびり 副词 - 无忧无虑、悠闲 寛（くつろ）ぐ 自动词 - 轻松惬意休息、不拘礼节 P36 研究所（けんきゅうしょ）で一番偉い人を 尊敬そん けい する。そん：損 存 尊けい：敬 警 経 单词/搭配 释义 損失（そんしつ） 名词 - 损失 存在（そんざい） 名词、自动词 - 存在 尊敬（そんけい） 名词 - 尊敬 警察（けいさつ） 名词 - 警察 経済（けいざい） 名词 - 经济 身体（しんたい）の状態（じょうたい）を 丁寧てい ねい に調べる。てい：程 提 定 丁 停ねい：寧 单词/搭配 释义 程度（ていど） 名词 - 程度、水平 提供（ていきょう） 名词、他动词 - 提供、赞助 提出（ていしゅつ） 名词 - 提出 定期（ていき） 名词 - 定期 丁寧（ていねい） 形容动词 - 有礼貌、小心谨慎、细心 停止（ていし） 名词、自动词 - 中止、暂时停止 P37 ホテル（hotel）の会員（かいいん）に 率直そっ ちょく な意見を求（もと）める 单词/搭配 释义 区別（くべつ） 名词、他动词 - 区分、辨别 正直（しょうじき） 名词、形容动词、副词 - 诚实、坦率、正直 お茶碗（ちゃわん） 名词 - 茶碗 お茶碗蒸（む）し 名词 - 蒸鸡蛋羹 P28 お菓子が不足（ふそく）したので適切（てきせつ）な分だけ足した。 二人の身長（しんちょう）を比（くら）べると僅（わず）かに差（さ）がある。 長生きする人は思い出（で）が多い。 单词/搭配 释义 足（た）す 他动词 - 增加、填补 足（た）りる 自动词 - 足够、可以、值得 加（くわ）える 他动词 - 增加、附加 増（ま）す 自动词、他动词 - 增加、提升 増（ふ）やす 他动词 - 增加、增值 僅（わず）か 副词、形容动词 - 微小、少 微（かす）か 形容动词 - 微弱、模糊、可怜 思い出（で） 名词 - 回忆、纪念 思い出（だ）す 他动词 - 回忆、记起 P29 話を進（すす）める前にあなたに対（たい）してどうしても言いたい。 おかずの種類（しゅるい）を倍（ばい）に増（ふ）やした。 怪我（けが）が順調（じゅんちょう）に回復（かいふく）している。 单词/搭配 释义 話題（わだい） 名词 - 话题 どうしても 副词 - 无论如何也要 おかず 名词 - 菜肴 順序（じゅんじょ） 名词 - 顺序、步骤 純情（じゅんじょう） 名词、形容动词 - 纯真、天真 傷（きず）つける 他动词 - 弄伤（身体）、损坏（感情）、败坏（名誉） P30 息（いき）を吸（す）って吐くことを振（ふ）り返（かえ）す。 正確（せいかく）な日付（ひづけ）を聞く。 こんなに美しい光景（こうけい）を見たのは初めてだ。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语单词 N2 P21-P30","slug":"日语单词 N2 P21-P30","date":"2021-01-20T12:59:04.000Z","updated":"2021-01-20T12:59:04.000Z","comments":true,"path":"2021/01/20/日语单词 N2 P21-P30/","link":"","permalink":"/2021/01/20/日语单词 N2 P21-P30/","excerpt":"","text":"课程来自youtube@日本語の森 N2 ことばP21 温泉（おんせん）のお湯（ゆ）はかなり熱いから苦手（にがて）だ。 新しいスーツを買うかは値段（ねだん）次第（しだい）で決める。 この学年（がくねん）は芸術（げいじゅつ）に興味（きょうみ）を持っている学生が非常（ひじょう）に多い。 单词/搭配 释义 貯（た）まる 自动词 - 积存、积攒 脱（ぬ）ぐ 名他动词 - 脱掉、摘掉 脱（ぬ）げる 自动词 - （穿在身上的东西）脱落、掉下 拭（ぬぐ）う 他动词 - 擦拭、擦除 関心（かんしん） 名词 - 关心、感兴趣 趣味（しゅみ） 名词 - 趣味、爱好 P22 本社（ほんしゃ）に私の本が二冊（にさつ）あるのは確かだ。 時間はたっぷりあるのでコンビニに寄（よ）る。 交通（こうつう）の手段（しゅだん）が多いので行動（こうどう）範囲（はんい）が広かった。 单词/搭配 释义 たっぷり 副词 - 充分地、足够多 寄（よ）る 自动词 - 靠近、聚集、顺便去、凭靠 同 - 凭る||広がる|自动 - 拓宽、蔓延| P23 劇（げき）の役割（やくわり）を巡って喧嘩（けんか）する。 解答（かいとう）欄（らん）を完全（かんぜん）に埋（う）めることを諦（あきら）めた。 製品（せいひん）を早く届（とど）けるよう指示（しじ）する。 单词/搭配 释义 俳優（はいゆう） 名词 - 演员 演（えん）じる 他动词 - 表演、扮演 登場（とうじょう） 名词、自动词 - 登场、出现 出身（しゅっしん） 名词 - 出身、籍贯、来自 与（あた）える 他动词 - 给予、分配 役割（やくわり） 名词 - 职责、角色、作用 果たす 他动词 - 完成、实现、（用在动词词根后）尽 諦（あきら）めた 他动词 - 放弃、死心 P24 栄養（えいよう）というテーマ（theme）から議論（ぎろん）を展開（てんかい）する。 恋愛（れんあい）に消極的（しょうきょくてき）か積極的（せっきょくてき）か問（と）う。 受験（じゅけん）の時には自然（しぜん）と部屋が散（ち）らかる。 单词/搭配 释义 散（ち）らかる 自动词 - 零乱、（东西）散落 P25 組み合わせの良い料理（りょうり）を注文（ちゅうもん）する。 上にする面（めん）によって印象（いんしょう）が変わる。 火事（かじ）になった時の状況（じょうきょう）を想像（そうぞう）して動（うご）く。 单词/搭配 释义 組み合わせ 名词 - 配合、组合 表（おもて） 名词 - 正面、表面、外边 裏（うら） 名词 - 背面、后边 平（たい）ら 形容动词 - 平坦 兎（うさぎ） 名词 - 兔子 P26 日が沈（しず）むと眠い。 この程度（ていど）の肉では満腹感（まんぷくかん）が得られないと批判（ひはん）する。 読書（どくしょ）する人の割合は減少（げんしょう）する傾向（けいこう）にある。 单词/搭配 释义 方向（ほうこう） 名词 - 方向、方针 P27 人を外見（がいけん）だけで判断（はんだん）し、差別（さべつ）してはいけない。 先日（せんじつ）、通学（つうがく）途中（とちゅう）で不思議（ふしぎ）な体験をした。 正直（しょうじき）に話して謝（あやま）る。 单词/搭配 释义 区別（くべつ） 名词、他动词 - 区分、辨别 正直（しょうじき） 名词、形容动词、副词 - 诚实、坦率、正直 お茶碗（ちゃわん） 名词 - 茶碗 お茶碗蒸（む）し 名词 - 蒸鸡蛋羹 P28 お菓子が不足（ふそく）したので適切（てきせつ）な分だけ足した。 二人の身長（しんちょう）を比（くら）べると僅（わず）かに差（さ）がある。 長生きする人は思い出（で）が多い。 单词/搭配 释义 足（た）す 他动词 - 增加、填补 足（た）りる 自动词 - 足够、可以、值得 加（くわ）える 他动词 - 增加、附加 増（ま）す 自动词、他动词 - 增加、提升 増（ふ）やす 他动词 - 增加、增值 僅（わず）か 副词、形容动词 - 微小、少 微（かす）か 形容动词 - 微弱、模糊、可怜 思い出（で） 名词 - 回忆、纪念 思い出（だ）す 他动词 - 回忆、记起 P29 話を進（すす）める前にあなたに対（たい）してどうしても言いたい。 おかずの種類（しゅるい）を倍（ばい）に増（ふ）やした。 怪我（けが）が順調（じゅんちょう）に回復（かいふく）している。 单词/搭配 释义 話題（わだい） 名词 - 话题 どうしても 副词 - 无论如何也要 おかず 名词 - 菜肴 順序（じゅんじょ） 名词 - 顺序、步骤 純情（じゅんじょう） 名词、形容动词 - 纯真、天真 傷（きず）つける 他动词 - 弄伤（身体）、损坏（感情）、败坏（名誉） P30 息（いき）を吸（す）って吐くことを振（ふ）り返（かえ）す。 正確（せいかく）な日付（ひづけ）を聞く。 こんなに美しい光景（こうけい）を見たのは初めてだ。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语单词 N2 P11-P20","slug":"日语单词 N2 P11-P20","date":"2021-01-09T09:02:18.000Z","updated":"2021-01-09T09:02:18.000Z","comments":true,"path":"2021/01/09/日语单词 N2 P11-P20/","link":"","permalink":"/2021/01/09/日语单词 N2 P11-P20/","excerpt":"","text":"课程来自youtube@日本語の森 N2 ことばP11 この商品（しょうひん）から満足（まんぞく）を得（え）た。 大きな事故（じこ）があった。その後（ご）、みんな気を付ける。 礼儀（れいぎ）を守らないと面接（めんせつ）で落（お）ちやすい。 单词/搭配 释义 縫（ぬ）いぐるみ 名词 - 布制玩偶 人形（にんぎょう） 名词 - 人偶、傀儡 硬（かた）い 形容词 - 硬的、坚固的 同 - 固い、堅い 交通（こうつ） 名词 - 交通 动词词根 + やすい 搭配 - 动词（动作） 很容易发生 質問（しつもん） 名词 - 询问、问题 P12 世界的に生活水準（すいじゅん）が高くなった。 哲学（てつがく）の本を読んで涙が止まらない。 更（さら）に怒（いか）りを感じた。 多摩川（たまがわ）は深（ふか）い。 单词/搭配 释义 浅（あさ）い 形容词 - 浅 P13 社長（しゃちょう）が余（あま）り頑張らなかったので全体（ぜんたい）の利益（りえき）が下（さ）がりつつある。 難（むずか）しすぎるものは直接（ちょくせつ）示（しめ）す。 单词/搭配 释义 余（あま）り 名词 - 剩下、过度…的结果 副词 - 过分、太（常后接否定状态） 沢山（たくさん） 名词 - 很多、充足 动词词根 + つつある 搭配 - 动词（动作） 正在进行中 触（さわ）る 自动词 - 触碰、触犯 捕（つか）まえる 他动词 - 抓住 嵌（は）める 他动词 - 镶嵌、戴上、陷害 示（しめ）す 他动词 - 出示、表示、指示 P14 この地域（ちいき）の人々（ひとびと）は豊（ゆた）かに暮らしている。 住民（じゅうみん）の疑問（ぎもん）を政府（せいふ）に聞く。 幼稚園（ようちえん）では物事（ものごと）の経験（けいけん）が大事だ。 今日の主役（しゅやく）は試合（しあい）で一位になった健人（けんと）です 单词/搭配 释义 豊か 形容动词 - 富裕、充实 P15 車の生産（せいさん）量（りょう）が増加（ぞうか）する。 アメリカの大学院（だいがくいん）に留学する。 沖縄（おきなわ）から離（はな）れることは不安（ふあん）だ。 結婚式（けっこんしき）に参加（さんか）する。 单词/搭配 释义 増（ふ）える 自动词 - 增加 卒業（そつぎょう） 名词 - 毕业 お祝（いわ）い 名词 - 祝贺、庆祝 P16 必要（ひつよう）な分（ぶん）だけ供給（きょうきゅう）する。 文化（ぶんか）は言葉で表（あらわ）すものではなく体験（たいけん）するものだ。 交際（こうさい）が十年間続（つづ）いて疲れた。 ジョギング（jogging）或（ある）いは筋トレ（きんとれ）で体を鍛（きた）える。 单词/搭配 释义 授業（じゅぎょう） 名词 - 授课 住所（じゅうしょ） 名词 - 住址 重要（じゅうよう） 名词 - 重要 形容动词 - 重要 情報（じょうほう） 名词 - 信息 状況（じょうきょう） 名词 - 状况 P17 電池（でんち）が切れたので携帯を充電（じゅうでん）する。 今朝早稲田大学に行く途中（とちゅう）で恋人（こいびと）に会った。 限（かぎ）られた給料（きゅうりょう）で暮らす。 技術（ぎじゅつ）の進歩（しんぽ）によって多くの情報（じょうほう）を纏（まと）めることが可能（かのう）になった。 单词/搭配 释义 纏（もと）める 他动词 - 汇总、集中、解决 整理（せいり） 名词、他动词 - 整理、清理 他动词 - 整理、清理 調（しら）べる 他动词 - 调查 P18 スキー（ski）が上手な小学生（しょうがくせい）は目立（めだ）つ。 広告（こうこく）の変更（へんこう）にがっかりする。 旅館（りょかん）にお風呂（ふろ）がないので宿泊（しゅくはく）を断（ことわ）った。 单词/搭配 释义 目立（めだ）つ 自动 - 显眼 集（あつ）まる 自动词 - 聚集 がっかり 副词 - 失望、沮丧 反対（はんたい） 名词、形动词、自动词 - 反对 P19 具体的（ぐたいてき）な資料（しりょう）をつくることが重要（じゅうよう）だ。 後輩（こうはい）を四人ずつのグループに分ける。 一週間（いっしゅうかん）以内（いない）に地球（ちきゅう）に大きな穴（あな）が空（あ）くのは噓だ。 单词/搭配 释义 捨（す）てる 他动词 - 扔掉、抛弃 同 - 棄てる お喋（しゃべ）り 名词 - 聊天、闲聊 形容动词 - 健谈的、爱说话的 掛（か）ける 他动词 - 悬挂、戴上、打（电话） 同 - 懸ける はっきり 副词 - 清楚地 ずつ 接尾词 - 表示同样数量、比例、程度 分ける 他动词 - 分开、分发 同 - 別ける 空（あ）く 空（す）く 自动词 - 空缺、变空、空闲 自动词 - 空间内数量变少、肚子空、空闲 間（あいだ） 名词 - 间隔、距离、中间 P20 農業（のうぎょう）には日光（にっこう）のエネルギー（energy）が必要だ。 財産（ざいさん）を巡（めぐ）って争（あらそ）う。 世界の平均（へいきん）寿命（じゅみょう）が伸（の）びる。 单词/搭配 释义 お米 こめ 名词 - 大米 小麦粉（こむぎこ） 名词 - 面粉 野菜（やさい） 名词 - 蔬菜 蔬菜（そさい） 名词 - 蔬菜 青菜（あおな） 名词 - 绿叶菜 伸びる 自动词 - 伸长、舒展、扩大、（面）失去弹性 伸ばす 他动词 - 伸长、延长 年齢（ねんれい） 名词 - 年龄","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语单词 N2 P01-P10","slug":"日语单词 N2 P01-P10","date":"2021-01-09T09:00:28.000Z","updated":"2021-01-09T09:00:28.000Z","comments":true,"path":"2021/01/09/日语单词 N2 P01-P10/","link":"","permalink":"/2021/01/09/日语单词 N2 P01-P10/","excerpt":"","text":"课程来自youtube@日本語の森 N2 ことばP1 じゃんけんの相手（あいて）を決める。 最も（もっとも）合う睡眠（すいみん）の方法（ほうほう）を探す。 こんな時間に昼寝する訳ない。 場によって適当（てきとう）な行動（こうどう）を取る。 効果的（こうかてき）な調査（ちょうさ）方法を考える。 P2 その筆者（ひっしゃ）があんなひどい事を書くなんて信じられない。 その商品（しょうひん）の購買（こうばい）理由（りゆう）を発表（はっぴょう）する。 𠮟（しか）られて怒（おこ）った。 单词/搭配 释义 筆（ふで） 名词 - 毛笔 P3 活動（かつどう）の内容（ないよう）がおもしろい。 この企業（きぎょう）は三つのグループ（group）に分かれている。 回答（かいとう）の結果（けっか）を理解（りかい）する この二つの書類（しょるい）がセット（set）です 单词/搭配 释义 動（うご）く 自动词 - （位置）移动、（机器）开动、（情况、心情）变动 中身（なかみ） 名词 - 内容 結（むす）ぶ 他动词 - 系、连结 果物（くだもの） 名词 - 水果 段階（だんかい） 名词 - 阶段 P4 社員（しゃいん）の努力（どりょく）によって成功（せいこう）した。 「半沢直樹」をみて面白（おもしろ）いと感じてた。 人間（にんげん）と植物（しょくぶつ）の関係（かんけい）が変化（へんか）する。 職業（しょくぎょう）を無くした人の割合（わりあい）を失業率（しつぎょうりつ）と言う。 单词/搭配 释义 繋（つな）がり 名词 - 羁绊、关联 数字 + 割（わり） 搭配 - 数字（几） 成 仕事（しごと） 名词 - 工作 P5 日本での生活が長くなるに連れて表現（ひょうげん）の間違（まちが）いの数（かず）が少（すく）なくなった。 他人（たにん）に相談（そうだん）した方がいい。 評価（ひょうか）の基準（きじゅん）を変えた。評価（ひょうか）の基準（きじゅん）が変わった。 单词/搭配 释义 他（ほか）の人 搭配 - 其他的人 相談（そうだん）に乗（の）る 搭配 - 接受别人的咨询 P6 中学生（ちゅうがくせい）の初めの頃（ころ）は友人（ゆうじん）の名前を覚えるのが大変。 日の出（で）を見るために太陽（たいよう）が昇（のぼ）る時間を確認（かくにん）する。 ビジネス会話（かいわ）にも技術（ぎじゅつ）が必要（ひつよう）だ。 单词/搭配 释义 沈（しず）む 自动词 - 沉没 动词词根 + 方（かた） 搭配 - 动词（动作） 的方法 P7 オリンピックが経済（けいざい）に影響（えいきょう）を与（あた）える。 巨額（きょがく）の税金（ぜいきん）で悩（なや）む。 妻（つま）と約（やく）一時間 釣（つ）りをした。 実際、ゆとり教育（きょういく）は社会（しゃかい）における効果（こうか）があった 单词/搭配 释义 ゆとり 名词 - 宽松 名词 + における 搭配 - 对于 名词 而言 P8 国民（こくみん）の関心（かんしん）を調（しら）べてメモ（memo）する。 十代（じゅうだい）の頃から将来（しょうらい）のことを考える。 人生（じんせい）の第一の目的（もくてき）は健康（けんこう）であること。 学（まな）ぶためにはまねをすることも大事（だいじ）だ。 单词/搭配 释义 真似（まね） 名词 - 模仿 重要（じゅうよう） 名词、形容动词 - 重要 P9 勉強は能力（のうりょく）より日常（にちじょう）の努力（どりょく）が重要（じゅうよう）だ。 初（はじ）めて相手の考えに合わせた。 就職（しゅうしょく）活動（かつどう）を一生懸命（いっしょうけんめい）した。 人は礼儀（れいぎ）を身（み）につけて成長（せいちょう）する 单词/搭配 释义 生（う）まれる 自动词 - 出生 力（ちから） 名词 - 力量 運動（うんどう） 名词 - 运动 考（かんが）え 名词 - 想法、意见 合（あ）わせる 他动词 - 调和、达成一致 得（え）る 他动词 - 得到 働（はたら）く 自动词 - 工作、起作用 他动词 - 做坏事 逆（ぎゃく） 名词 - 反、倒 お菓子（かし） 名词 - 和果子 身（み）につける 搭配 - 掌握、携带、穿在身上 P10 十年ぶりに帰ると故郷（ふるさと）の様子（ようす）が違（ちが）った。 日本のばすで大声（おおごえ）を出すことは、詰（つ）まり迷惑（めいわく）だ。 書類（しょるい）を袋（ふくろ）に入れて郵便（ゆうびん）で送（おく）る。 環境（かんきょう）に関するスピーチ（speech）コンテスト（contest）で優勝（ゆうしょう）した。 单词/搭配 释义 詰（つ）まり 副词 - 总之、也就是说（加强语气） 名词 - 结尾、最后、尽头 競（きそ）う合う 搭配 - 互相比赛","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"「ロマンスの約束」歌词翻译","slug":"「ロマンスの約束」歌词翻译","date":"2020-11-30T07:51:19.000Z","updated":"2021-11-25T09:15:00.000Z","comments":true,"path":"2020/11/30/「ロマンスの約束」歌词翻译/","link":"","permalink":"/2020/11/30/「ロマンスの約束」歌词翻译/","excerpt":"","text":"「ロマンスの約束」「ロマンスの約束」作品介绍 该作品由 幾田りら 词曲并献唱。 [00:00.00][歌词制作 by Yuk1n0][00:00.200]ロマンスの約束 - ikura (幾田りら)[00:00.400]词：幾田りら[00:00.600]曲：幾田りら[00:00.800]これから二人過ごしてゆくために *为了今后能两个人一同度过*[00:05.969]約束して欲しいことがあるの *有没有想要约定的事情呢*[00:11.116]声が枯（か）れて名前が呼べなくなる *声嘶力竭不能呼唤你的名字*[00:16.183]その日まで忘れないて *直到那天为止（请）不要忘记*[00:21.460]光を探すような眠れない夜は *像是要寻找光明而睡不着的夜晚*[00:27.181]朝まで手を握（にぎ）っていて欲しい *想要到早上为止一直（紧）握着手*[00:34.887]沢山（たくさん）の愛で溢れたなら *如果爱意满溢而出*[00:40.007]明（あ）けない夜の夢を見せてほしい *想要让你看到我梦想（幻想）这夜晚不会结束*[00:45.205]天秤（てんびん）はいつも傾（かたむ）くけど *尽管天秤永远倾向一方*[00:49.777]今夜だけは同じでいたい *只有今晚想要（和你）一样*[01:03.256]二人で進み始めたこの列車（れっしゃ）の *两人一起开始前进的这座列车*[01:07.827]切符（きっぷ）は最後まで失（な）くさないでね *请直到最后也不要将车票弄丢哦*[01:13.000]もしも行き先を見失（みうしな）うったなら *如果迷失了终点的方向*[01:18.146]その場所でまた始めよう *就在这个地方再次开始吧*[01:23.370]頬を濡（ぬ）らすような眠れない夜は *脸颊浸润着（泪水）辗转反侧的夜晚*[01:29.117]心地（ここち）良（い）い左肩（ひだりかた）を貸して欲しい *想要借你舒服的左肩（靠一靠）*[01:36.849]沢山（たくさん）の愛を知れたのなら *如果能察觉到这多（到满溢）的爱*[01:41.943]口紅（くちべに）を溶（と）かすようなキスをして *（给我一个）融化口红一般的吻*[01:47.142]その後（あと）は鼻先（はなさき）でくすっと笑って *在那之后鼻尖偷笑*[01:51.713]終わりはないと言って抱（だ）きしめて *说着不会结束 紧紧相拥*[02:17.992]君の短所（たんしょ）や私の長所（ちょうしょ）が変わってしまっても *就算你的短处和我的长处在不经意间（随着时间）不断变化*[02:22.694]代わりは居（い）ないよ きっと *一定没有（其他人可以）替代哟*[02:28.572]思い出（おもいで）が示（しめ）すよ また手を取ろう *回忆（在脑海中）浮现 再次握住手吧*[02:36.226]星屑（ほしくず）のようなこの世界で *如星尘一般的这个世界*[02:41.267]照らされた光の先にいたんだ *被光线的尽头照耀着存在着*[02:46.309]君のままそのままが美しいから *因为你那样不变就已经很美了*[02:51.116]それでいい それだけでいい *那样就足够了 只要那样就足够了*[02:56.862]沢山（たくさん）の愛で溢れたなら *如果爱意满溢而出*[03:01.930]明けない夜の夢を見せて欲しい *想要让你看到我梦想（幻想）这夜晚不会结束*[03:07.129]天秤（てんびん）はきっとまた傾（かたむ）くけど *尽管天秤一定会再次倾向一方*[03:11.674]ずっと ずっと *一直 一直*[03:13.241]君と一緒（いっしょ）にいたい *想要和你在一起* 「ロマンスの約束」歌词翻译これから二人過ごしてゆくために为了今后能两个人一同度过約束して欲しいことがあるの有没有想要约定的事情呢声が枯（か）れて名前が呼べなくなる声嘶力竭不能呼唤你的名字その日まで忘れないて直到那天为止（请）不要忘记光を探すような眠れない夜は像是要寻找光明而睡不着的夜晚朝まで手を握（にぎ）っていて欲しい想要到早上为止一直（紧）握着手沢山（たくさん）の愛で溢れたなら如果爱意满溢而出明（あ）けない夜の夢を見せてほしい想要让你看到我梦想（幻想）这夜晚不会结束天秤（てんびん）はいつも傾（かたむ）くけど尽管天秤永远倾向一方今夜だけは同じでいたい只有今晚想要（和你）一样二人で進み始めたこの列車（れっしゃ）の两人一起开始前进的这座列车切符（きっぷ）は最後まで失（な）くさないでね请直到最后也不要将车票弄丢哦もしも行き先を見失（みうしな）うったなら如果迷失了终点的方向その場所でまた始めよう就在这个地方再次开始吧頬を濡（ぬ）らすような眠れない夜は脸颊浸润着（泪水）辗转反侧的夜晚心地（ここち）良（い）い左肩（ひだりかた）を貸して欲しい想要借你舒服的左肩（靠一靠）沢山（たくさん）の愛を知れたのなら如果能察觉到这多（到满溢）的爱口紅（くちべに）を溶（と）かすようなキスをして（给我一个）融化口红一般的吻その後（あと）は鼻先（はなさき）でくすっと笑って在那之后鼻尖偷笑終わりはないと言って抱（だ）きしめて说着不会结束 紧紧相拥君の短所（たんしょ）や私の長所（ちょうしょ）が変わってしまっても就算你的短处和我的长处在不经意间（随着时间）不断变化代わりは居（い）ないよ きっと一定没有（其他人可以）替代哟思い出（おもいで）が示（しめ）すよ また手を取ろう回忆（在脑海中）浮现 再次握住手吧星屑（ほしくず）のようなこの世界で如星尘一般的这个世界照らされた光の先にいたんだ被光线的尽头照耀着存在着君のままそのままが美しいから因为你那样不变就已经很美了それでいい それだけでいい那样就足够了 只要那样就足够了沢山（たくさん）の愛で溢れたなら如果爱意满溢而出明けない夜の夢を見せて欲しい想要让你看到我梦想（幻想）这夜晚不会结束天秤（てんびん）はきっとまた傾（かたむ）くけど尽管天秤一定会再次倾向一方ずっと ずっと一直 一直君と一緒（いっしょ）にいたい想要和你在一起 「ロマンスの約束」Live「ロマンスの約束」 - 23:16 本视频转载自 https://www.bilibili.com/video/BV1by4y1z7Pb 。","categories":[{"name":"生活","slug":"生活","permalink":"/categories/生活/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"},{"name":"音乐","slug":"音乐","permalink":"/tags/音乐/"},{"name":"翻译","slug":"翻译","permalink":"/tags/翻译/"}],"keywords":[{"name":"生活","slug":"生活","permalink":"/categories/生活/"}]},{"title":"run_classifier.py逐行注释","slug":"run_classifier.py逐行注释","date":"2020-11-28T08:07:48.000Z","updated":"2020-11-28T08:07:48.000Z","comments":true,"path":"2020/11/28/run_classifier.py逐行注释/","link":"","permalink":"/2020/11/28/run_classifier.py逐行注释/","excerpt":"","text":"本文为BERT的run_classifier.py模块，即分类模块，进行逐行注释。 run_classifier.py头文件&quot;&quot;&quot;BERT finetuning runner.&quot;&quot;&quot; from __future__ import absolute_import from __future__ import division from __future__ import print_function import collections import csv import os import modeling import optimization import tokenization import tensorflow as tf 参数定义flags = tf.flags FLAGS = flags.FLAGS ## Required parameters flags.DEFINE_string( &quot;data_dir&quot;, None, &quot;The input data dir. Should contain the .tsv files (or other data files) &quot; &quot;for the task.&quot;) flags.DEFINE_string( &quot;bert_config_file&quot;, None, &quot;The config json file corresponding to the pre-trained BERT model. &quot; &quot;This specifies the model architecture.&quot;) flags.DEFINE_string(&quot;task_name&quot;, None, &quot;The name of the task to train.&quot;) flags.DEFINE_string(&quot;vocab_file&quot;, None, &quot;The vocabulary file that the BERT model was trained on.&quot;) flags.DEFINE_string( &quot;output_dir&quot;, None, &quot;The output directory where the model checkpoints will be written.&quot;) ## Other parameters flags.DEFINE_string( &quot;init_checkpoint&quot;, None, &quot;Initial checkpoint (usually from a pre-trained BERT model).&quot;) flags.DEFINE_bool( &quot;do_lower_case&quot;, True, &quot;Whether to lower case the input text. Should be True for uncased &quot; &quot;models and False for cased models.&quot;) flags.DEFINE_integer( &quot;max_seq_length&quot;, 128, &quot;The maximum total input sequence length after WordPiece tokenization. &quot; &quot;Sequences longer than this will be truncated, and sequences shorter &quot; &quot;than this will be padded.&quot;) flags.DEFINE_bool(&quot;do_train&quot;, False, &quot;Whether to run training.&quot;) flags.DEFINE_bool(&quot;do_eval&quot;, False, &quot;Whether to run eval on the dev set.&quot;) flags.DEFINE_bool( &quot;do_predict&quot;, False, &quot;Whether to run the model in inference mode on the test set.&quot;) flags.DEFINE_integer(&quot;train_batch_size&quot;, 32, &quot;Total batch size for training.&quot;) flags.DEFINE_integer(&quot;eval_batch_size&quot;, 8, &quot;Total batch size for eval.&quot;) flags.DEFINE_integer(&quot;predict_batch_size&quot;, 8, &quot;Total batch size for predict.&quot;) flags.DEFINE_float(&quot;learning_rate&quot;, 5e-5, &quot;The initial learning rate for Adam.&quot;) flags.DEFINE_float(&quot;num_train_epochs&quot;, 3.0, &quot;Total number of training epochs to perform.&quot;) flags.DEFINE_float( &quot;warmup_proportion&quot;, 0.1, &quot;Proportion of training to perform linear learning rate warmup for. &quot; &quot;E.g., 0.1 = 10% of training.&quot;) flags.DEFINE_integer(&quot;save_checkpoints_steps&quot;, 1000, &quot;How often to save the model checkpoint.&quot;) flags.DEFINE_integer(&quot;iterations_per_loop&quot;, 1000, &quot;How many steps to make in each estimator call.&quot;) flags.DEFINE_bool(&quot;use_tpu&quot;, False, &quot;Whether to use TPU or GPU/CPU.&quot;) tf.flags.DEFINE_string( &quot;tpu_name&quot;, None, &quot;The Cloud TPU to use for training. This should be either the name &quot; &quot;used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 &quot; &quot;url.&quot;) tf.flags.DEFINE_string( &quot;tpu_zone&quot;, None, &quot;[Optional] GCE zone where the Cloud TPU is located in. If not &quot; &quot;specified, we will attempt to automatically detect the GCE project from &quot; &quot;metadata.&quot;) tf.flags.DEFINE_string( &quot;gcp_project&quot;, None, &quot;[Optional] Project name for the Cloud TPU-enabled project. If not &quot; &quot;specified, we will attempt to automatically detect the GCE project from &quot; &quot;metadata.&quot;) tf.flags.DEFINE_string(&quot;master&quot;, None, &quot;[Optional] TensorFlow master URL.&quot;) flags.DEFINE_integer( &quot;num_tpu_cores&quot;, 8, &quot;Only used if `use_tpu` is True. Total number of TPU cores to use.&quot;) tf.flags是tf.app.flags的新版，使用方法见 https://abhisheksaurabh1985.github.io/2017-12-30-flags-in-python-tf 。flags的引入可以让使用者在cmd中直接操作自定义参数，比如：python run_classifier.py --do_train Trueflags的引入还可以在程序全局使用FLAGS.do_train返回该参数的值。 可自定义的参数如下： 输入文件目录（训练，验证，测试用） BERT配置文件位置（json格式） 任务名称（对应不同的测试数据集） 词汇文件位置（txt格式） 输出结果目录（将会包括一些checkpoint） 初始checkpoint（可继承自预训练BERT模型） 单样本序列长度最大值定义（默认128，超出截断，不足补足） 训练轮赋能 验证轮赋能 预测轮赋能 训练轮并行样本数（默认32） 验证轮并行样本数（默认8） 预测轮并行样本数（默认8） 学习率（默认5e-5） 训练轮数（默认3） 热身比例（默认0.1） checkpoints保存频率（默认1000步保存一次） iterations_per_loop指使用TPU时每轮循环的迭代步数 6个TPU相关的配置 InputExampleclass InputExample(object): &quot;&quot;&quot;A single training/test example for simple sequence classification.&quot;&quot;&quot; def __init__(self, guid, text_a, text_b=None, label=None): &quot;&quot;&quot;Constructs a InputExample. Args: guid: Unique id for the example. text_a: string. The untokenized text of the first sequence. For single sequence tasks, only this sequence must be specified. text_b: (Optional) string. The untokenized text of the second sequence. Only must be specified for sequence pair tasks. label: (Optional) string. The label of the example. This should be specified for train and dev examples, but not for test examples. &quot;&quot;&quot; self.guid = guid self.text_a = text_a self.text_b = text_b self.label = label 该类用于初始化输入样本。其中，init的四个参数： guid：样本的唯一识别id text_a：sentence-pair的首句 text_b：sentence-pair的次句（可以没有） label：该样本的标签/真值。 PaddingInputExampleclass PaddingInputExample(object): &quot;&quot;&quot;Fake example so the num input examples is a multiple of the batch size. When running eval/predict on the TPU, we need to pad the number of examples to be a multiple of the batch size, because the TPU requires a fixed batch size. The alternative is to drop the last batch, which is bad because it means the entire output data won&#39;t be generated. We use this class instead of `None` because treating `None` as padding battches could cause silent errors. &quot;&quot;&quot; 当样本数不够一个batch时，该类用来充数（padding）。 InputFeaturesclass InputFeatures(object): &quot;&quot;&quot;A single set of features of data.&quot;&quot;&quot; def __init__(self, input_ids, input_mask, segment_ids, label_id, is_real_example=True): self.input_ids = input_ids self.input_mask = input_mask self.segment_ids = segment_ids self.label_id = label_id self.is_real_example = is_real_example 该类用于初始化输入特征，将输入的raw数据转换成BERT可用的数据。其中init的5个参数： input_ids：输入id，模型会在之后将token通过vocab.txt转化成整数id。 input_mask：输入掩码，用一个包含1，0的列表指示/真token/伪token（padding的token）/。 segment_ids：段落id，用一个包含0，1的列表指示/首句/次句/。（BERT中的E_a、E_b） label_id：标签id，用[0, n-1]编号n种标签。 is_real_example：指示是否为真样本的布尔值（存在充数的样本）。 DataProcessorclass DataProcessor(object): &quot;&quot;&quot;Base class for data converters for sequence classification data sets.&quot;&quot;&quot; def get_train_examples(self, data_dir): &quot;&quot;&quot;Gets a collection of `InputExample`s for the train set.&quot;&quot;&quot; raise NotImplementedError() def get_dev_examples(self, data_dir): &quot;&quot;&quot;Gets a collection of `InputExample`s for the dev set.&quot;&quot;&quot; raise NotImplementedError() def get_test_examples(self, data_dir): &quot;&quot;&quot;Gets a collection of `InputExample`s for prediction.&quot;&quot;&quot; raise NotImplementedError() def get_labels(self): &quot;&quot;&quot;Gets the list of labels for this data set.&quot;&quot;&quot; raise NotImplementedError() @classmethod def _read_tsv(cls, input_file, quotechar=None): &quot;&quot;&quot;Reads a tab separated value file.&quot;&quot;&quot; with tf.gfile.Open(input_file, &quot;r&quot;) as f: reader = csv.reader(f, delimiter=&quot;\\t&quot;, quotechar=quotechar) lines = [] for line in reader: lines.append(line) return lines 该类是之后出现的多个特化数据处理器的父类。定义的几个方法： get_train_examples：用于特化数据处理器获取训练集样本。 get_dev_examples：用于特化数据处理器获取验证集样本。 get_test_examples：用于特化数据处理器获取测试集样本。 get_labels：用于特化数据处理器获取标签。 _read_tsv：用于特化数据处理器将tsv/csv类型文件转换为列表。 MrpcProcessorclass MrpcProcessor(DataProcessor): &quot;&quot;&quot;Processor for the MRPC data set (GLUE version).&quot;&quot;&quot; def get_train_examples(self, data_dir): &quot;&quot;&quot;See base class.&quot;&quot;&quot; return self._create_examples( self._read_tsv(os.path.join(data_dir, &quot;train.tsv&quot;)), &quot;train&quot;) def get_dev_examples(self, data_dir): &quot;&quot;&quot;See base class.&quot;&quot;&quot; return self._create_examples( self._read_tsv(os.path.join(data_dir, &quot;dev.tsv&quot;)), &quot;dev&quot;) def get_test_examples(self, data_dir): &quot;&quot;&quot;See base class.&quot;&quot;&quot; return self._create_examples( self._read_tsv(os.path.join(data_dir, &quot;test.tsv&quot;)), &quot;test&quot;) def get_labels(self): &quot;&quot;&quot;See base class.&quot;&quot;&quot; return [&quot;0&quot;, &quot;1&quot;] def _create_examples(self, lines, set_type): &quot;&quot;&quot;Creates examples for the training and dev sets.&quot;&quot;&quot; examples = [] for (i, line) in enumerate(lines): if i == 0: continue #跳过第一行，第一行包含表头信息，不包含样本。 guid = &quot;%s-%s&quot; % (set_type, i) text_a = tokenization.convert_to_unicode(line[3]) text_b = tokenization.convert_to_unicode(line[4]) if set_type == &quot;test&quot;: label = &quot;0&quot; else: label = tokenization.convert_to_unicode(line[0]) examples.append( InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label)) return examples 该类用于文件输入并将数据转化为class InputExample(object)，并写入列表examples。该类是DataProcessor的子类，继承DataProcessor的相关方法。 类似的，还有：class XnliProcessor(DataProcessor)class MnliProcessor(DataProcessor)class ColaProcessor(DataProcessor) 在我们需要处理自己的数据集时，需要自行创建一个自己的数据处理器，并根据数据集的格式对数据进行处理得到InputExample类需要的四个参数guid=guid, text_a=text_a, text_b=text_b, label=label。 convert_single_exampledef convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer): &quot;&quot;&quot;Converts a single `InputExample` into a single `InputFeatures`.&quot;&quot;&quot; # 若输入的example为充数的样本， # 则将InputFeatures的前4个参数设为相应长度的0的列表，将is_real_example设为假。 if isinstance(example, PaddingInputExample): return InputFeatures( input_ids=[0] * max_seq_length, input_mask=[0] * max_seq_length, segment_ids=[0] * max_seq_length, label_id=0, is_real_example=False) # 建立一个{[标签]: 标签序号}的字典 label_map = {} for (i, label) in enumerate(label_list): label_map[label] = i # 将首句和次句Tokenize并截短 tokens_a = tokenizer.tokenize(example.text_a) tokens_b = None if example.text_b: tokens_b = tokenizer.tokenize(example.text_b) if tokens_b: # Modifies `tokens_a` and `tokens_b` in place so that the total # length is less than the specified length. # Account for [CLS], [SEP], [SEP] with &quot;- 3&quot; # 超过阈值则首句次句轮流pop一个token _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3) else: # Account for [CLS] and [SEP] with &quot;- 2&quot; # 超过阈值则直接截短取前端 if len(tokens_a) &gt; max_seq_length - 2: tokens_a = tokens_a[0:(max_seq_length - 2)] # The convention in BERT is: # (a) For sequence pairs: # tokens: [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP] # type_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 # (b) For single sequences: # tokens: [CLS] the dog is hairy . [SEP] # type_ids: 0 0 0 0 0 0 0 # # Where &quot;type_ids&quot; are used to indicate whether this is the first # sequence or the second sequence. The embedding vectors for `type=0` and # `type=1` were learned during pre-training and are added to the wordpiece # embedding vector (and position vector). This is not *strictly* necessary # since the [SEP] token unambiguously separates the sequences, but it makes # it easier for the model to learn the concept of sequences. # # For classification tasks, the first vector (corresponding to [CLS]) is # used as the &quot;sentence vector&quot;. Note that this only makes sense because # the entire model is fine-tuned. tokens = [] segment_ids = [] # 为token添加[CLS]以及[SEP] # 若存在句子对，则将两个句子级联为tokens。 # 为segment_ids根据句子的前后添加指示，0表示首句，1表示次句。 tokens.append(&quot;[CLS]&quot;) segment_ids.append(0) for token in tokens_a: tokens.append(token) segment_ids.append(0) tokens.append(&quot;[SEP]&quot;) segment_ids.append(0) if tokens_b: for token in tokens_b: tokens.append(token) segment_ids.append(1) tokens.append(&quot;[SEP]&quot;) segment_ids.append(1) # 将tokens通过vocab.txt转换为整数input_ids input_ids = tokenizer.convert_tokens_to_ids(tokens) # The mask has 1 for real tokens and 0 for padding tokens. Only real # tokens are attended to. # 用掩码1表示真token，0表示充数的token。 input_mask = [1] * len(input_ids) # Zero-pad up to the sequence length. # 若input_ids长度小于max_seq_length，则用0充数。 while len(input_ids) &lt; max_seq_length: input_ids.append(0) input_mask.append(0) segment_ids.append(0) # 断言函数判断是否相等，否则raise错误 assert len(input_ids) == max_seq_length assert len(input_mask) == max_seq_length assert len(segment_ids) == max_seq_length # 从字典label_map中找到example.label的序号id label_id = label_map[example.label] # 打印所有样本中前5个样本的信息 if ex_index &lt; 5: tf.logging.info(&quot;*** Example ***&quot;) tf.logging.info(&quot;guid: %s&quot; % (example.guid)) tf.logging.info(&quot;tokens: %s&quot; % &quot; &quot;.join( [tokenization.printable_text(x) for x in tokens])) tf.logging.info(&quot;input_ids: %s&quot; % &quot; &quot;.join([str(x) for x in input_ids])) tf.logging.info(&quot;input_mask: %s&quot; % &quot; &quot;.join([str(x) for x in input_mask])) tf.logging.info(&quot;segment_ids: %s&quot; % &quot; &quot;.join([str(x) for x in segment_ids])) tf.logging.info(&quot;label: %s (id = %d)&quot; % (example.label, label_id)) # 返回包含5个变量的InputFeatures类 feature = InputFeatures( input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_id=label_id, is_real_example=True) return feature 该类用于将数据处理器返回的多个InputExample类中的单个InputExample转换为InputFeatures类。 file_based_convert_examples_to_featuresdef file_based_convert_examples_to_features( examples, label_list, max_seq_length, tokenizer, output_file): &quot;&quot;&quot;Convert a set of `InputExample`s to a TFRecord file.&quot;&quot;&quot; # TFRecordWriter初始化 writer = tf.python_io.TFRecordWriter(output_file) # 循环写入每个样本的features for (ex_index, example) in enumerate(examples): # 进度条（每10000提示一次） if ex_index % 10000 == 0: tf.logging.info(&quot;Writing example %d of %d&quot; % (ex_index, len(examples))) # 将InputExample转换为InputFeatures feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer) # 建立整型的特征集，特征集以字典形式存在： # int64_list { # value: 0 # value: 1 # value: ... # } def create_int_feature(values): f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values))) return f # 建立一个有序字典 features = collections.OrderedDict() # 建立5个整型特征集 features[&quot;input_ids&quot;] = create_int_feature(feature.input_ids) features[&quot;input_mask&quot;] = create_int_feature(feature.input_mask) features[&quot;segment_ids&quot;] = create_int_feature(feature.segment_ids) features[&quot;label_ids&quot;] = create_int_feature([feature.label_id]) features[&quot;is_real_example&quot;] = create_int_feature( [int(feature.is_real_example)]) # 建立嵌套字典存储features # features { # feature { # key: &quot;input_ids&quot; # value { # int64_list { # value: 0 # value: 1 # } # } # } # feature { # key: &quot;input_mask&quot; # value { # int64_list { # value: 0 # value: 0 # } # } # } # } tf_example = tf.train.Example(features=tf.train.Features(feature=features)) #序列化之后（转换为二进制流）写入TFRecord writer.write(tf_example.SerializeToString()) writer.close() 该函数用于将每个样本的特征写入TFRecord留底。关于TFRecord，我找到了一篇比较简洁明了的博文 https://www.cnblogs.com/yanshw/p/12419616.html 。 file_based_input_fn_builderdef file_based_input_fn_builder(input_file, seq_length, is_training, drop_remainder): &quot;&quot;&quot;Creates an `input_fn` closure to be passed to TPUEstimator.&quot;&quot;&quot; name_to_features = { &quot;input_ids&quot;: tf.FixedLenFeature([seq_length], tf.int64), &quot;input_mask&quot;: tf.FixedLenFeature([seq_length], tf.int64), &quot;segment_ids&quot;: tf.FixedLenFeature([seq_length], tf.int64), &quot;label_ids&quot;: tf.FixedLenFeature([], tf.int64), &quot;is_real_example&quot;: tf.FixedLenFeature([], tf.int64), } def _decode_record(record, name_to_features): &quot;&quot;&quot;Decodes a record to a TensorFlow example.&quot;&quot;&quot; example = tf.parse_single_example(record, name_to_features) # tf.Example only supports tf.int64, but the TPU only supports tf.int32. # So cast all int64 to int32. for name in list(example.keys()): t = example[name] if t.dtype == tf.int64: t = tf.to_int32(t) example[name] = t return example def input_fn(params): &quot;&quot;&quot;The actual input function.&quot;&quot;&quot; batch_size = params[&quot;batch_size&quot;] # For training, we want a lot of parallel reading and shuffling. # For eval, we want no shuffling and parallel reading doesn&#39;t matter. d = tf.data.TFRecordDataset(input_file) if is_training: d = d.repeat() d = d.shuffle(buffer_size=100) d = d.apply( tf.contrib.data.map_and_batch( lambda record: _decode_record(record, name_to_features), batch_size=batch_size, drop_remainder=drop_remainder)) return d return input_fn * 该函数用于基于文件将输入函数传入TPU，有关TPU的部分先搁置。 _truncate_seq_pairdef _truncate_seq_pair(tokens_a, tokens_b, max_length): &quot;&quot;&quot;Truncates a sequence pair in place to the maximum length.&quot;&quot;&quot; # This is a simple heuristic which will always truncate the longer sequence # one token at a time. This makes more sense than truncating an equal percent # of tokens from each, since if one sequence is very short then each token # that&#39;s truncated likely contains more information than a longer sequence. while True: total_length = len(tokens_a) + len(tokens_b) # 若总长小于等于最大长度则跳出循环 if total_length &lt;= max_length: break # 若总长大于最大长度， # 则根据句子对的长短依次pop超出的部分直到符合要求。 if len(tokens_a) &gt; len(tokens_b): tokens_a.pop() else: tokens_b.pop() 该函数用于处理句子对输入时，总长超出最大长度的情况。 create_modeldef create_model(bert_config, is_training, input_ids, input_mask, segment_ids, labels, num_labels, use_one_hot_embeddings): &quot;&quot;&quot;Creates a classification model.&quot;&quot;&quot; # 初始化一个BERT模型 model = modeling.BertModel( config=bert_config, is_training=is_training, input_ids=input_ids, input_mask=input_mask, token_type_ids=segment_ids, use_one_hot_embeddings=use_one_hot_embeddings) # In the demo, we are doing a simple classification task on the entire # segment. # # If you want to use the token-level output, use model.get_sequence_output() # instead. # 输出定义为pooling后的输出 output_layer = model.get_pooled_output() hidden_size = output_layer.shape[-1].value # 生成一个对应输出层的截断正态分布的参数矩阵（标准差为0.02） output_weights = tf.get_variable( &quot;output_weights&quot;, [num_labels, hidden_size], initializer=tf.truncated_normal_initializer(stddev=0.02)) # 偏置设为0 output_bias = tf.get_variable( &quot;output_bias&quot;, [num_labels], initializer=tf.zeros_initializer()) with tf.variable_scope(&quot;loss&quot;): if is_training: # I.e., 0.1 dropout # 为了减弱过拟合，将dropout的保持概率设为0.9 output_layer = tf.nn.dropout(output_layer, keep_prob=0.9) # W_t * x + b 输出未归一化的“概率”logits，对应num_labels个label logits = tf.matmul(output_layer, output_weights, transpose_b=True) logits = tf.nn.bias_add(logits, output_bias) # 将digits经过softmax后得到对应num_labels个label的归一化输出 probabilities = tf.nn.softmax(logits, axis=-1) log_probs = tf.nn.log_softmax(logits, axis=-1) # 为了计算样本误差，将label进行one-hot编码 one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32) # 计算每个样本对应标签的误差 per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1) # 得到平均误差（应该是交叉熵） loss = tf.reduce_mean(per_example_loss) # 返回平均误差loss，包含每个对应label误差的样本误差per_example_loss，未归一化的概率logits，归一化后的概率probabilities。 return (loss, per_example_loss, logits, probabilities) 该函数借助model.py的一些参数和方法建立了一个基于BERT模型的encoder + 单层神经网络分类器，是run_classifier.py算法实现的核心框架部分。关于dropout的源码解释，参考 https://blog.csdn.net/qq_20412595/article/details/82824830 。关于logits和softmax的理解，参考 https://www.zhihu.com/question/60751553 。 model_fn_builderdef model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate, num_train_steps, num_warmup_steps, use_tpu, use_one_hot_embeddings): &quot;&quot;&quot;Returns `model_fn` closure for TPUEstimator.&quot;&quot;&quot; def model_fn(features, labels, mode, params): # pylint: disable=unused-argument &quot;&quot;&quot;The `model_fn` for TPUEstimator.&quot;&quot;&quot; tf.logging.info(&quot;*** Features ***&quot;) for name in sorted(features.keys()): tf.logging.info(&quot; name = %s, shape = %s&quot; % (name, features[name].shape)) input_ids = features[&quot;input_ids&quot;] input_mask = features[&quot;input_mask&quot;] segment_ids = features[&quot;segment_ids&quot;] label_ids = features[&quot;label_ids&quot;] is_real_example = None if &quot;is_real_example&quot; in features: is_real_example = tf.cast(features[&quot;is_real_example&quot;], dtype=tf.float32) else: is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32) is_training = (mode == tf.estimator.ModeKeys.TRAIN) (total_loss, per_example_loss, logits, probabilities) = create_model( bert_config, is_training, input_ids, input_mask, segment_ids, label_ids, num_labels, use_one_hot_embeddings) tvars = tf.trainable_variables() initialized_variable_names = {} scaffold_fn = None if init_checkpoint: (assignment_map, initialized_variable_names ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint) if use_tpu: def tpu_scaffold(): tf.train.init_from_checkpoint(init_checkpoint, assignment_map) return tf.train.Scaffold() scaffold_fn = tpu_scaffold else: tf.train.init_from_checkpoint(init_checkpoint, assignment_map) tf.logging.info(&quot;**** Trainable Variables ****&quot;) for var in tvars: init_string = &quot;&quot; if var.name in initialized_variable_names: init_string = &quot;, *INIT_FROM_CKPT*&quot; tf.logging.info(&quot; name = %s, shape = %s%s&quot;, var.name, var.shape, init_string) output_spec = None if mode == tf.estimator.ModeKeys.TRAIN: train_op = optimization.create_optimizer( total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu) output_spec = tf.contrib.tpu.TPUEstimatorSpec( mode=mode, loss=total_loss, train_op=train_op, scaffold_fn=scaffold_fn) elif mode == tf.estimator.ModeKeys.EVAL: def metric_fn(per_example_loss, label_ids, logits, is_real_example): predictions = tf.argmax(logits, axis=-1, output_type=tf.int32) accuracy = tf.metrics.accuracy( labels=label_ids, predictions=predictions, weights=is_real_example) loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example) return { &quot;eval_accuracy&quot;: accuracy, &quot;eval_loss&quot;: loss, } eval_metrics = (metric_fn, [per_example_loss, label_ids, logits, is_real_example]) output_spec = tf.contrib.tpu.TPUEstimatorSpec( mode=mode, loss=total_loss, eval_metrics=eval_metrics, scaffold_fn=scaffold_fn) else: output_spec = tf.contrib.tpu.TPUEstimatorSpec( mode=mode, predictions={&quot;probabilities&quot;: probabilities}, scaffold_fn=scaffold_fn) return output_spec return model_fn * 该函数用于建立BERT分类模型并将参数传入TPU，有关TPU的部分先搁置。 input_fn_builderdef input_fn_builder(features, seq_length, is_training, drop_remainder): &quot;&quot;&quot;Creates an `input_fn` closure to be passed to TPUEstimator.&quot;&quot;&quot; all_input_ids = [] all_input_mask = [] all_segment_ids = [] all_label_ids = [] for feature in features: all_input_ids.append(feature.input_ids) all_input_mask.append(feature.input_mask) all_segment_ids.append(feature.segment_ids) all_label_ids.append(feature.label_id) def input_fn(params): &quot;&quot;&quot;The actual input function.&quot;&quot;&quot; batch_size = params[&quot;batch_size&quot;] num_examples = len(features) # This is for demo purposes and does NOT scale to large data sets. We do # not use Dataset.from_generator() because that uses tf.py_func which is # not TPU compatible. The right way to load data is with TFRecordReader. d = tf.data.Dataset.from_tensor_slices({ &quot;input_ids&quot;: tf.constant( all_input_ids, shape=[num_examples, seq_length], dtype=tf.int32), &quot;input_mask&quot;: tf.constant( all_input_mask, shape=[num_examples, seq_length], dtype=tf.int32), &quot;segment_ids&quot;: tf.constant( all_segment_ids, shape=[num_examples, seq_length], dtype=tf.int32), &quot;label_ids&quot;: tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32), }) if is_training: d = d.repeat() d = d.shuffle(buffer_size=100) d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder) return d return input_fn * 该函数用于不基于文件将输入函数传入TPU，有关TPU的部分先搁置。 convert_examples_to_featuresdef convert_examples_to_features(examples, label_list, max_seq_length, tokenizer): &quot;&quot;&quot;Convert a set of `InputExample`s to a list of `InputFeatures`.&quot;&quot;&quot; features = [] for (ex_index, example) in enumerate(examples): if ex_index % 10000 == 0: tf.logging.info(&quot;Writing example %d of %d&quot; % (ex_index, len(examples))) feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer) features.append(feature) return features 该函数用于批量将InputExample类转换为InputFeatures类。 \\main:def main(_): # 将日志信息打印 tf.logging.set_verbosity(tf.logging.INFO) # 建立关于数据处理器的字典 processors = { &quot;cola&quot;: ColaProcessor, &quot;mnli&quot;: MnliProcessor, &quot;mrpc&quot;: MrpcProcessor, &quot;xnli&quot;: XnliProcessor, } # 验证do_lower_case的选择是否与checkpoint匹配 tokenization.validate_case_matches_checkpoint(FLAGS.do_lower_case, FLAGS.init_checkpoint) # 保证至少在参数部分选择了train/eval/predict三个操作中的一种，否则报错 if not FLAGS.do_train and not FLAGS.do_eval and not FLAGS.do_predict: raise ValueError( &quot;At least one of `do_train`, `do_eval` or `do_predict&#39; must be True.&quot;) # 输入BERT的配置文件 bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file) # 如果参数部分设定的最大序列长度大于BERT的最大position embedding长度，则报错 if FLAGS.max_seq_length &gt; bert_config.max_position_embeddings: raise ValueError( &quot;Cannot use sequence length %d because the BERT model &quot; &quot;was only trained up to sequence length %d&quot; % (FLAGS.max_seq_length, bert_config.max_position_embeddings)) # 创建参数设定的输出目录 tf.gfile.MakeDirs(FLAGS.output_dir) # 传入任务名称 task_name = FLAGS.task_name.lower() # 若任务不在数据处理器的字典中，则报错 if task_name not in processors: raise ValueError(&quot;Task not found: %s&quot; % (task_name)) # 选择对应任务的数据处理器 processor = processors[task_name]() # 传入数据处理器处理后的标签列表 label_list = processor.get_labels() # 选择使用FullTokenizer分词，选择词典和是否小写 tokenizer = tokenization.FullTokenizer( vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case) # TPU参数初始化 tpu_cluster_resolver = None if FLAGS.use_tpu and FLAGS.tpu_name: tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver( FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project) is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2 # TPU配置设定 run_config = tf.contrib.tpu.RunConfig( cluster=tpu_cluster_resolver, master=FLAGS.master, model_dir=FLAGS.output_dir, save_checkpoints_steps=FLAGS.save_checkpoints_steps, tpu_config=tf.contrib.tpu.TPUConfig( iterations_per_loop=FLAGS.iterations_per_loop, num_shards=FLAGS.num_tpu_cores, per_host_input_for_training=is_per_host)) train_examples = None num_train_steps = None num_warmup_steps = None # 训练部分初始化参数 if FLAGS.do_train: # 通过数据处理器的函数获得训练样本 train_examples = processor.get_train_examples(FLAGS.data_dir) # 实际训练步数 = 训练样本总数 / 批大小 * 训练轮数 num_train_steps = int( len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs) # warmup步数 = 实际训练步数 * 热身比例 num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion) # BERT建模 model_fn = model_fn_builder( bert_config=bert_config, num_labels=len(label_list), init_checkpoint=FLAGS.init_checkpoint, learning_rate=FLAGS.learning_rate, num_train_steps=num_train_steps, num_warmup_steps=num_warmup_steps, use_tpu=FLAGS.use_tpu, use_one_hot_embeddings=FLAGS.use_tpu) # If TPU is not available, this will fall back to normal Estimator on CPU # or GPU. # 配置TPUEstimator（若没有TPU，则会使用CPU/GPU） estimator = tf.contrib.tpu.TPUEstimator( use_tpu=FLAGS.use_tpu, model_fn=model_fn, config=run_config, train_batch_size=FLAGS.train_batch_size, eval_batch_size=FLAGS.eval_batch_size, predict_batch_size=FLAGS.predict_batch_size) # 训练部分主题 if FLAGS.do_train: # 指定输出训练数据的TFRecord文件的位置 train_file = os.path.join(FLAGS.output_dir, &quot;train.tf_record&quot;) # 批量将样本转化为包含features的TFRecord文件 file_based_convert_examples_to_features( train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file) # 打印样本总数、批大小、实际训练步数 tf.logging.info(&quot;***** Running training *****&quot;) tf.logging.info(&quot; Num examples = %d&quot;, len(train_examples)) tf.logging.info(&quot; Batch size = %d&quot;, FLAGS.train_batch_size) tf.logging.info(&quot; Num steps = %d&quot;, num_train_steps) # 基于文件解包并生成传入TPUEstimator的包含features的输入函数 train_input_fn = file_based_input_fn_builder( input_file=train_file, seq_length=FLAGS.max_seq_length, is_training=True, drop_remainder=True) # 将features和实际训练步数传入TPUEstimator并开始训练 estimator.train(input_fn=train_input_fn, max_steps=num_train_steps) # 评估阶段 if FLAGS.do_eval: # 数据处理器获取评估集样本 eval_examples = processor.get_dev_examples(FLAGS.data_dir) # 评估集样本个数 num_actual_eval_examples = len(eval_examples) # 如果使用TPU if FLAGS.use_tpu: # TPU requires a fixed batch size for all batches, therefore the number # of examples must be a multiple of the batch size, or else examples # will get dropped. So we pad with fake examples which are ignored # later on. These do NOT count towards the metric (all tf.metrics # support a per-instance weight, and these get a weight of 0.0). # 如果（样本个数 / 批大小）不整除，则做padding充数使其可被整除 while len(eval_examples) % FLAGS.eval_batch_size != 0: eval_examples.append(PaddingInputExample()) # 建立评估TFRecord文件的目录 eval_file = os.path.join(FLAGS.output_dir, &quot;eval.tf_record&quot;) # 批量将样本转化为包含features的TFRecord文件 file_based_convert_examples_to_features( eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file) # 打印实际样本个数、padding的样本个数、批大小 tf.logging.info(&quot;***** Running evaluation *****&quot;) tf.logging.info(&quot; Num examples = %d (%d actual, %d padding)&quot;, len(eval_examples), num_actual_eval_examples, len(eval_examples) - num_actual_eval_examples) tf.logging.info(&quot; Batch size = %d&quot;, FLAGS.eval_batch_size) # This tells the estimator to run through the entire set. eval_steps = None # However, if running eval on the TPU, you will need to specify the # number of steps. # 如果使用TPU，需要确定评估阶段的步数。 if FLAGS.use_tpu: # 断言函数确保（样本个数 / 批大小）整除，否则报错。 assert len(eval_examples) % FLAGS.eval_batch_size == 0 # 实际评估步数即为整除后的得到的整数 # 可以看到这里写程序的同学十分谨慎，既用了整除“//”，又强制转换了类型为整型 eval_steps = int(len(eval_examples) // FLAGS.eval_batch_size) # 如果使用TPU则用drop_remainder，否则不用。 eval_drop_remainder = True if FLAGS.use_tpu else False # 基于文件解包并生成传入TPUEstimator的包含features的输入函数 eval_input_fn = file_based_input_fn_builder( input_file=eval_file, seq_length=FLAGS.max_seq_length, is_training=False, drop_remainder=eval_drop_remainder) # 将features和实际评估步数传入TPUEstimator并开始评估 result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps) # 创建评估结果的txt文档 output_eval_file = os.path.join(FLAGS.output_dir, &quot;eval_results.txt&quot;) # 向评估结果的txt文档写入result返回的四个参数 # eval_accuracy、eval_loss、global_step、loss # *目前不知道这几个参数的具体含义，需要进一步查看estimator.evaluate返回的参数是如何定义的。 with tf.gfile.GFile(output_eval_file, &quot;w&quot;) as writer: tf.logging.info(&quot;***** Eval results *****&quot;) for key in sorted(result.keys()): # 屏幕打印 tf.logging.info(&quot; %s = %s&quot;, key, str(result[key])) # 写入txt文件 writer.write(&quot;%s = %s\\n&quot; % (key, str(result[key]))) # 预测阶段 if FLAGS.do_predict: # 传入预测样本 predict_examples = processor.get_test_examples(FLAGS.data_dir) # 预测样本总数 num_actual_predict_examples = len(predict_examples) # 如果使用TPU if FLAGS.use_tpu: # TPU requires a fixed batch size for all batches, therefore the number # of examples must be a multiple of the batch size, or else examples # will get dropped. So we pad with fake examples which are ignored # later on. # 若样本总数不能整除batch，则padding样本充数。 while len(predict_examples) % FLAGS.predict_batch_size != 0: predict_examples.append(PaddingInputExample()) # 创建TFRecord文件目录 predict_file = os.path.join(FLAGS.output_dir, &quot;predict.tf_record&quot;) # 批量将样本转化为包含features的TFRecord文件 file_based_convert_examples_to_features(predict_examples, label_list, FLAGS.max_seq_length, tokenizer, predict_file) # 打印实际样本数、padding样本数、批大小 tf.logging.info(&quot;***** Running prediction*****&quot;) tf.logging.info(&quot; Num examples = %d (%d actual, %d padding)&quot;, len(predict_examples), num_actual_predict_examples, len(predict_examples) - num_actual_predict_examples) tf.logging.info(&quot; Batch size = %d&quot;, FLAGS.predict_batch_size) # 如果使用TPU则用drop_remainder，否则不用。 predict_drop_remainder = True if FLAGS.use_tpu else False # 基于文件解包并生成传入TPUEstimator的包含features的输入函数 predict_input_fn = file_based_input_fn_builder( input_file=predict_file, seq_length=FLAGS.max_seq_length, is_training=False, drop_remainder=predict_drop_remainder) # 开始预测 result = estimator.predict(input_fn=predict_input_fn) # 创建预测结果的tsv文件 output_predict_file = os.path.join(FLAGS.output_dir, &quot;test_results.tsv&quot;) # 写入tsv文件 with tf.gfile.GFile(output_predict_file, &quot;w&quot;) as writer: num_written_lines = 0 tf.logging.info(&quot;***** Predict results *****&quot;) # 按照标签顺序写入每个预测样本对应标签的概率 # *需要进一步查看estimator.predict返回的数据是如何定义的 for (i, prediction) in enumerate(result): probabilities = prediction[&quot;probabilities&quot;] if i &gt;= num_actual_predict_examples: break output_line = &quot;\\t&quot;.join( str(class_probability) for class_probability in probabilities) + &quot;\\n&quot; writer.write(output_line) num_written_lines += 1 # 断言函数保证写入tsv的行数与预测样本总数相同，否则报错。 assert num_written_lines == num_actual_predict_examples 主函数部分。 if name == “main“:# 主函数调用 if __name__ == &quot;__main__&quot;: # 将data_dir、task_name、vocab_file、bert_config_file、output_dir设置为必要参数 flags.mark_flag_as_required(&quot;data_dir&quot;) flags.mark_flag_as_required(&quot;task_name&quot;) flags.mark_flag_as_required(&quot;vocab_file&quot;) flags.mark_flag_as_required(&quot;bert_config_file&quot;) flags.mark_flag_as_required(&quot;output_dir&quot;) # 该函数会先处理flag之后默认运行main函数 tf.app.run() flag处理以及主函数调用。tf.app.run()的源码解释详见 https://blog.csdn.net/helei001/article/details/51859423 def run(main=None): f = flags.FLAGS f._parse_flags() main = main or sys.modules[&#39;__main__&#39;].main sys.exit(main(sys.argv)) 后记run_classifier.py的代码官方给出了982行，我写这篇文章写了两天，前期准备不知道用了多久，因为一直在看这个分类器的代码。说实话准备充分的话看起来还是很轻松的，但几个月之前刚下BERT看这个分类器的代码的时候真的是一头雾水，即便我已经大致了解了BERT的实际工作原理。 还剩下一些关于TPUEstimator参数配置的部分没加注释，还有一些关于评估阶段测试阶段的result返回数据没确认。之后会继续补齐。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"python","slug":"python","permalink":"/tags/python/"},{"name":"NLP","slug":"NLP","permalink":"/tags/NLP/"},{"name":"BERT","slug":"BERT","permalink":"/tags/BERT/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"「深愛」 歌词翻译","slug":"「深愛」歌词翻译","date":"2020-11-16T07:11:48.000Z","updated":"2021-11-25T09:14:16.000Z","comments":true,"path":"2020/11/16/「深愛」歌词翻译/","link":"","permalink":"/2020/11/16/「深愛」歌词翻译/","excerpt":"","text":"「深愛」「深愛」作品介绍 该作品为TV动画「白色相簿」OP，由 歌手兼声优的 水树奈奈 献唱。 [00:00.00][歌词制作 by Yuk1n0][00:02.00]深愛 - 水树奈奈 (みずき なな)[00:03.67]词：水樹奈々[00:05.18]曲：上松範康（Elements Garden）[00:08.43]编曲：藤間仁（Elements Garden）[00:11.67]Drum：村石雅行[00:13.61]Bass：美久月千晴[00:15.34]Guitar：渡辺格（cherry boys）[00:17.72]Arpa：上松美香[00:19.24]Strings：篠崎正嗣[00:21.18]Oboe：石橋雅一[00:23.13]Other Sound/Programming：藤間仁（Elements Garden）[00:25.18]雪が舞（ま）い散る夜空（よぞら） *雪花四散飞舞的夜空*[00:29.00]二人寄（よ）り添（そ）い見上げた *两个人相互依偎仰望（天空）*[00:32.38]繋がる手と手の温（ぬく）もりは *紧握着的双手的温暖*[00:36.40]とても優しかった *十分柔和*[00:39.74]淡いオールド（old）ブルー（blue）の *在淡淡的蓝灰色的*[00:42.54]雲間（くもま）に消えて行（ゆ）くでしょう *云间（的天空）就要消失了吧*[00:47.41]永遠（えいえん）へと続（つづ）くはずの あの約束（やくそく） *本应持续到永远的那个约定*[00:56.00]あなたの傍にいるだけで ただそれだけで良かった *只是在你的身边 仅仅只有这一件（事情满足）就好了*[01:03.00]いつの間にか膨（ふく）らむ 現実（いま）以上（いじょう）の夢に気付かずに *不知在何时膨胀到现实以上的梦境 我没有注意到*[01:14.00]どんな時も どこにいる時でも *不论何时 不论何地*[01:17.47]強く強く抱（だ）き締（し）めていて *一直紧紧相拥*[01:21.59]情熱（じょうねつ）が日常（にちじょう）に染（そ）まるとしても *就算激情被日常所染色*[01:28.00]あなたへのこの想いは全（すべ）て *对你的思念也是我的全部*[01:35.27]終わりなどないと信じている *我一直相信没有结束什么的*[01:42.40]あなただけずっと 見つめているの *正在注视的 一直只有你（一个人）*[01:53.71]交（か）わす言葉（ことば）と時間 姿を変えていくでしょう *交错的言语和时间 姿态也有了改变对吧*[02:01.33]白い頬（ほほ）に解（と）けたそれは月の涙 *从洁白的脸颊落下（挣脱）的是月亮的眼泪*[02:10.00]「行かないで、もう少しだけ」何度も言いかけては *「别走、就再呆一会」不论多少次刚一开口*[02:17.00]「まだあえるよね？ きっと」何度も自分（じぶん）に問（と）いかける *「还能再见到对吧？一定」不论多少次（就会这样反过来）开始自问*[02:28.00]突然（とつぜん）走り出した *突然间开始行动*[02:30.14]行（ゆ）く先の違（ちが）う二人もう止まらない *去向不同（相背而行）的两人已经无法停下脚步*[02:35.58]沈黙（ちんもく）が想像（そうぞう）を超え引き裂（さ）いて *沉默将想象穿破撕裂*[02:42.00]一つだけ許（ゆる）される願（ねが）いがあるなら *如果仅仅有一个能被允许的愿望（这样的事）存在的话*[02:50.76]「ごめんね」と伝えたいよ *「对不起」想要这样告诉你*[03:00.46]いくら想っていても届（とど）かない *不论多少想念却也传递不到*[03:04.53]声にしなきゃ 動（うご）き出さなきゃ *如果不发声 不开始行动的话*[03:08.08]隠したままの二人の秘密 *一直这样隐藏起来的两人的秘密*[03:12.00]このまま忘れられてしまうの *这样下去就会不注意间被忘掉*[03:18.39]だから ねぇ 早く今（いま）ココにきて *所以啊 呐 快点现在就来这里（我的身边）*[03:54.60]あなたの傍にいるだけで ただそれだけで良かった *只是在你的身边 仅仅只有这一件（事情满足）就好了*[04:02.00]今度巡（めぐ）り 会えたら もっともっと笑い合えるかな *下次如果兜兜转转（再次）遇到的话 就可以再多一起笑了对吧*[04:12.63]どんな時もどこにいる時でも *不论何时 不论何地*[04:16.13]強く強く抱（だ）き締（し）めていて *一直紧紧相拥*[04:20.19]情熱よりアツイ体温（ねつ）で溶（と）かして *比起激情更火热的体温相溶*[04:26.55]あなたへのこの想いは全（すべ）て *对你的思念就是我的全部*[04:34.00]終わりなどないと信じている *我一直相信没有结束什么的*[04:41.10]あなただけずっと見つめているの *正在注视的 一直只有你（一个人）* 「深愛」歌词翻译雪が舞（ま）い散る夜空（よぞら）雪花四散飞舞的夜空二人寄（よ）り添（そ）い見上げた两个人相互依偎仰望（天空）繋がる手と手の温（ぬく）もりは紧握着的双手的温暖とても優しかった十分柔和淡いオールド（old）ブルー（blue）の在淡淡的蓝灰色的雲間（くもま）に消えて行（ゆ）くでしょう云间（的天空）就要消失了吧永遠（えいえん）へと続（つづ）くはずの あの約束（やくそく）本应持续到永远的那个约定あなたの傍にいるだけで ただそれだけで良かった只是在你的身边 仅仅只有这一件（事情满足）就好了いつの間にか膨（ふく）らむ 現実（いま）以上（いじょう）の夢に気付かずに不知在何时膨胀到现实以上的梦境 我没有注意到どんな時も どこにいる時でも不论何时 不论何地強く強く抱（だ）き締（し）めていて一直紧紧相拥情熱（じょうねつ）が日常（にちじょう）に染（そ）まるとしても就算激情被日常所染色あなたへのこの想いは全（すべ）て对你的思念也是我的全部終わりなどないと信じている我一直相信没有结束什么的あなただけずっと 見つめているの正在注视的 一直只有你（一个人）交（か）わす言葉（ことば）と時間 姿を変えていくでしょう交错的言语和时间 姿态也有了改变对吧白い頬（ほほ）に解（と）けたそれは月の涙从洁白的脸颊落下（挣脱）的是月亮的眼泪「行かないで、もう少しだけ」何度も言いかけては「别走、就再呆一会」不论多少次刚一开口「まだあえるよね？ きっと」何度も自分（じぶん）に問（と）いかける「还能再见到对吧？一定」不论多少次（就会这样反过来）开始自问突然（とつぜん）走り出した突然间开始行动行（ゆ）く先の違（ちが）う二人もう止まらない去向不同（相背而行）的两人已经无法停下脚步沈黙（ちんもく）が想像（そうぞう）を超え引き裂（さ）いて沉默将想象穿破撕裂一つだけ許（ゆる）される願（ねが）いがあるなら如果仅仅有一个能被允许的愿望（这样的事）存在的话「ごめんね」と伝えたいよ「对不起」想要这样告诉你いくら想っていても届（とど）かない不论多少想念却也传递不到声にしなきゃ 動（うご）き出さなきゃ如果不发声 不开始行动的话隠したままの二人の秘密一直这样隐藏起来的两人的秘密このまま忘れられてしまうの这样下去就会不注意间被忘掉だから ねぇ 早く今（いま）ココにきて所以啊 呐 快点现在就来这里（我的身边）あなたの傍にいるだけで ただそれだけで良かった只是在你的身边 仅仅只有这一件（事情满足）就好了今度巡（めぐ）り 会えたら もっともっと笑い合えるかな下次如果兜兜转转（再次）遇到的话 就可以再多一起笑了对吧どんな時もどこにいる時でも不论何时 不论何地強く強く抱（だ）き締（し）めていて一直紧紧相拥情熱よりアツイ体温（ねつ）で溶（と）かして比起激情更火热的体温相溶あなたへのこの想いは全（すべ）て对你的思念就是我的全部終わりなどないと信じている我一直相信没有结束什么的あなただけずっと見つめているの正在注视的 一直只有你（一个人） 米澤円版本 「深愛」","categories":[{"name":"生活","slug":"生活","permalink":"/categories/生活/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"},{"name":"音乐","slug":"音乐","permalink":"/tags/音乐/"},{"name":"翻译","slug":"翻译","permalink":"/tags/翻译/"}],"keywords":[{"name":"生活","slug":"生活","permalink":"/categories/生活/"}]},{"title":"「再会」 歌词翻译","slug":"「再会」歌词翻译","date":"2020-11-10T15:30:27.000Z","updated":"2021-11-25T09:13:32.000Z","comments":true,"path":"2020/11/10/「再会」歌词翻译/","link":"","permalink":"/2020/11/10/「再会」歌词翻译/","excerpt":"","text":"「再会」「再会」作品介绍 该作品由 给鬼灭唱主题曲破圈了的 LiSA 和 影视剧主题曲专业户 Uru 合作出演，音乐制作请到了YOASOBI组合的制作人Ayase。 该作品主要讲述了 LiSA和Uru之间届不到的恋 ，整首歌充斥着相思之苦。值得一提的是，该作的转场部分编曲沿用了与YOASOBI创作的「たぶん」中类似的鼓点，让我感到有点出戏。详情可参考网易云版「たぶん」1:04 以及 网易云版「再会」1:06。 LiSA和Uru的不同唱法导致了两人音色和特点的不同：* LiSA在中低音部分主要用真声，中高音假声切换十分丝滑（嗓子太好，某些中高音也可以实现真声起爆），能够全频段给听众强大的声场震撼，适合动漫的燃曲。* Uru基本全程鼻腔发声，声音非常空灵，这导致她很难表现出强大的震撼力，适合一些偏冷色调的曲目。 [00:00.00][歌词制作 by Yuk1n0][00:00.30]再会(produced by Ayase) - LiSA (织部里沙)/Uru (うる)[00:00.50]词：Ayase[00:01.10]曲：Ayase[00:01.30]编曲：Ayase[00:01.50]L：またねと 笑って見せて くれた *你洋溢着笑容对我告别*[00:07.00]L：同じように 笑い返していた のに *明明我也回赠了一样的笑容*[00:13.00]L：気付けば 少し滲（にじ）んでいた *不经意间*[00:17.50]L：あなたの姿 *你的身影已经微微渗入（我的心中）*[00:23.00]U：あれから いくつ夜を 越えた *在那之后 不知度过了多少个夜晚*[00:28.00]U：窓越し（まどごし）の白い画面（がめん）に 映（うつ）った *越过窗户 映在纯白的画面上*[00:34.00]U：あなたと 見たい景色を 今も *想要和亲爱的你 一同欣赏的景色*[00:38.50]U：ずっとずっど 見つめたまま *如今 我也一直注视着*[00:46.50]L主：降りしきる雪が 積もるように *如漫天飞雪堆积般*[00:52.00]L主：この町で ただ あなたを 想う *在这座城市 我只想念着你*[00:57.50]L主：離れていても 同じ空が *纵使分离*[01:03.00]L：どうか 見えていますように *也一定要能够看到同一片天空*[01:16.00]U：またねと 優しい声が 響く *温柔的道别声（在空中）回响着*[01:21.00]U：耳元に あなたが 残した 静寂（しじま） *耳边 你只留下了一片沉寂*[01:27.50]L：世界が切り離された 夜 *在（你与我）世界割裂的夜晚*[01:32.00]L：また 目を 瞑（つぶ）る *我再一次闭上双眼*[01:38.00]U：くだらないことに ずっと[01:40.50]L：幸せを 感じてた きっと *相信一定能在这些不断的琐事中感受到幸福*[01:43.50]U：特別じゃない日々を もっと[01:46.00]L：二人で ただ過ごしていたくて *只想要两个人一同度过更多平凡的日常*[01:49.00]U：季節が 何度変わろと *无论季节如何变幻*[01:51.50]L：隣に いたいよねえ *都好想让你在我身边*[01:53.00]L：それ以上 何も いらないから *除此之外 我已经别无所求*[01:59.00]U主：降りしきる雪が 積もるように *如漫天飞雪堆积般*[02:04.00]U主：遠い町で ただ あなたを 想う *在这座遥远的城市 我只想念着你*[02:09.50]U主：触れ合うことが できなくても *就算不能（相互）触碰*[02:15.00]U：変わること なく *（你我）也没有改变*[02:18.00]L：何度だってそう *不论多少次都是如此*[02:20.00]L：振り返れば *若能回顾（过去）*[02:21.00]L：あの日のあなたの言葉（ことば）が 声が *你那天的话语和声音*[02:27.00]L：会いたくなるんだよ *都会让我变得想要见（你）*[02:29.00]U：何度だってそう *不论多少次都是如此[02:30.50]U：信じ合えれば \\若能彼此信任*[02:32.00]U：いつまでも 二人 繋がっていられる *不论何时 两人都能够紧紧相连*[02:41.50]U：雪明かり 照らす この町にも *就算是雪光照耀的这座城市*[02:47.00]U：いつかは 優しい春が 芽吹（めぶく）く *总有一天 温柔之春也将苏醒*[02:52.00]L：ここで また会えた その時は *在这里再次相遇的那个时候*[02:57.50]L：涙溢（こぼ）さない ように *希望不掉眼泪*[03:02.50]L主：冬の終わりを 告（つ）げる 淡（あわ）雪 *宣告冬天结束的薄雪*[03:08.00]L主：そのひと時に 願いを 乗せる *在那个时候承载着我的心愿*[03:13.50]L主：どん季節も 景色も *不论什么季节 怎样的风景*[03:17.00]L主：あなたと 共（とも）に 同じ（L） 場所で（U） 感じていたい *我都想和你在同一个地方一起感受*[03:24.00]U主：町に 柔（やわ）らかな風が 吹いて *城市中吹起柔和的（春）风*[03:29.50]U主：鮮（あざ）やかな花が 咲く その日を *鲜花盛开的那一天*[03:35.00]U主：待ち続（つづ）ける 二人にも *两个人也继续等待着（这一天的到来）*[03:40.50]U：春が 訪（おとず）れます ように *希望春天到来*[03:45.50]合：笑顔で また会えます ように *希望再次看到你的微笑（首尾呼应）* 「再会」歌词翻译L：またねと 笑って見せて くれた你洋溢着笑容对我告别L：同じように 笑い返していた のに明明我也回赠了一样的笑容L：気付けば 少し滲（にじ）んでいた あなたの姿不经意间 你的身影已经微微渗入（我的心中）U：あれから いくつ夜を 越えた在那之后 不知度过了多少个夜晚U：窓越し（まどごし）の白い画面（がめん）に 映（うつ）った越过窗户 映在纯白的画面上U：あなたと 見たい景色を 今も想要和亲爱的你 一同欣赏的景色U：ずっとずっど 見つめたまま如今 我也一直注视着L主：降りしきる雪が 積もるように如漫天飞雪堆积般L主：この町で ただ あなたを 想う在这座城市 我只想念着你L主：離れていても 同じ空が纵使分离L：どうか 見えていますように也一定要能够看到同一片天空U：またねと 優しい声が 響く温柔的道别声（在空中）回响着U：耳元に あなたが 残した 静寂（しじま）耳边 你只留下了一片沉寂L：世界が切り離された 夜在（你与我）世界割裂的夜晚L：また 目を 瞑（つぶ）る我再一次闭上双眼U：くだらないことに ずっとL：幸せを 感じてた きっと相信一定能在这些不断的琐事中感受到幸福U：特別じゃない日々を もっとL：二人で ただ過ごしていたくて只想要两个人一同度过更多平凡的日常U：季節が 何度変わろと无论季节如何变幻L：隣に いたいよねえ都好想让你在我身边L：それ以上 何も いらないから除此之外 我已经别无所求U主：降りしきる雪が 積もるように如漫天飞雪堆积般U主：遠い町で ただ あなたを 想う在这座遥远的城市 我只想念着你U主：触れ合うことが できなくても就算不能（相互）触碰U：変わること なく（你我）也没有改变L：何度だってそう不论多少次都是如此L：振り返れば 若能回顾（过去）L：あの日のあなたの言葉（ことば）が 声が你那天的话语和声音L：会いたくなるんだよ都会让我变得想要见（你）U：何度だってそう不论多少次都是如此U：信じ合えれば若能彼此信任U：いつまでも 二人 繋がっていられる不论何时 两人都能够紧紧相连U：雪明かり 照らす この町にも就算是雪光照耀的这座城市U：いつかは 優しい春が 芽吹（めぶく）く总有一天 温柔之春也将苏醒L：ここで また会えた その時は在这里再次相遇的那个时候L：涙溢（こぼ）さない ように希望不掉眼泪L主：冬の終わりを 告（つ）げる 淡（あわ）雪宣告冬天结束的薄雪L主：そのひと時に 願いを 乗せる在那个时候承载着我的心愿L主：どん季節も 景色も不论什么季节 怎样的风景L主：あなたと 共（とも）に 同じ（L） 場所で（U） 感じていたい我都想和你在同一个地方一起感受U主：町に 柔（やわ）らかな風が 吹いて城市中吹起柔和的（春）风U主：鮮（あざ）やかな花が 咲く その日を鲜花盛开的那一天U主：待ち続（つづ）ける 二人にも两个人也继续等待着（这一天的到来）U：春が 訪（おとず）れます ように希望春天到来合：笑顔で また会えます ように希望再次看到你的微笑（首尾呼应） 「たぶん」对比用 「たぶん」1:04鼓点转场与「再会」1:06相似，但氛围完全不一样。","categories":[{"name":"生活","slug":"生活","permalink":"/categories/生活/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"},{"name":"音乐","slug":"音乐","permalink":"/tags/音乐/"},{"name":"翻译","slug":"翻译","permalink":"/tags/翻译/"}],"keywords":[{"name":"生活","slug":"生活","permalink":"/categories/生活/"}]},{"title":"文献阅读 TINYBERT DISTILLING BERT FOR NATURAL LANGUAGE UNDERSTANDING","slug":"文献阅读 TINYBERT DISTILLING BERT FOR NATURAL LANGUAGE UNDERSTANDING","date":"2020-10-23T15:45:42.000Z","updated":"2020-10-23T15:45:42.000Z","comments":true,"path":"2020/10/23/文献阅读 TINYBERT DISTILLING BERT FOR NATURAL LANGUAGE UNDERSTANDING/","link":"","permalink":"/2020/10/23/文献阅读 TINYBERT DISTILLING BERT FOR NATURAL LANGUAGE UNDERSTANDING/","excerpt":"","text":"1 Introduction[1] TINYBERT: DISTILLING BERT FOR NATURAL LANGUAGE UNDERSTANDINGLink : http://arxiv.org/abs/1909.10351Institute：Huazhong University of Science and Technology, Huawei Noah’s Ark Lab, Huawei Technologies Co., Ltd.Code : https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT 1.1 Achievement Propose a novel Transformer distillation method Introduce a new two-stage learning framework. Achieve 96% the performance of BERT_BASE while being 7.5x smaller and 9.4x faster on inference. 2 Method2.1 Transformer Distillation Figure 2.1: An overview of Transformer distillation[1] Problem : Original BERT is too big and need too much time for training. Requirement : Reduce the scale of BERT while maintaining almost the same performance. Solution : (Teacher-Student Framework) Choose M layers (Layer Num. of Student Model) from N layers of the Teacher Model for Transformer-layer distillation Use n = g(m) to denote the mapping function For embedding-layer distillation and prediction layer distillation, the corresponding layer mappings are denoted as 0 = g(0) and N + 1 = g(M + 1). Loss Function:\\begin{equation}L_{model} = \\sum_{m=1}^{M+1}\\lambda_mL_{layer}(S_m, T_{g(m)})\\end{equation}* Where lambda_m is the hyper-parameter represents the importance of the m-th layer distillation, L_layer( ) is the loss function of a given layer pair.* This formula calculate the total loss of every layer ranged from 0 to M+1. 2.1.1 Transformer-layer Distillation Use attention-based distillation to learn the matrices of multi-head attention in the teacher network Attention-based Loss Function:\\begin{equation}L_{attn} = \\frac{1}{h}\\sum_{i=1}^{h}MSE(A_i^S, A_i^T)\\end{equation}* Where h is the number of attention heads, A_i(l*l) is the attention matrix of the i-th head, l is the length of the input text, MSE( ) is the mean square error function.* It shows that A_i without softmax has a faster convergence rate and better performance. Use hidden states based distillation to learn the feature of hidden states in the teacher network* I think this part actually enable the student model to learn the feature of FNN in teacher model. Hidden States based Loss Function:\\begin{equation}L_{attn} = MSE(H^SW_h, H^T)\\end{equation}* Where H^S (l*d’) and H^T (l*d) refer to the hidden states of student and teacher, W_h(d’*d) is a learnable linear transformation mapping student network into the vector space of teacher network, d’ and d refer to the hidden sizes of student and teacher network (Usually d’ &lt; d) . 2.1.2 Embedding-layer Distillation Use learnable matrix W_e to map student model into the vector space of teacher model Embedding-based Loss Function:\\begin{equation}L_{embd} = MSE(E^SW_e, E^T)\\end{equation}* Where E_S and E_T refer to the embedding matrix of student and teacher model, W_e is the learnable linear transformation. 2.1.3 Prediction-Layer Distillation Use soft cross-entropy loss:\\begin{equation}L_{pred} = -softmax(z^T)log\\_softmax(z^S / t)\\end{equation}* Where z^T and z^S refer to the logits vector output by teacher and student model, t is the temperature value.* In the experiment, they find t = 1 performs well. 2.1.4 Final Distillation Objective Function\\begin{equation}L_{layer}(S_m, T_{g(m)}) = \\begin{cases} L_{embd}(S_0, T_0), &amp; {m = 0}\\newline L_{hidn}(S_m, T_{g(m)}) + L_{attn}(S_m, T_{g(m)}), &amp; M \\geq m &gt; 0\\newline L_{pred}(S_{M+1}, T_{N+1}), &amp; m = M+1 \\end{cases}\\end{equation} 2.2 TinyBERT LearningThey apply a two-stage learning framework: General Distillation:Help TinyBERT learn rich knowledge embedding and improve the generaliztion capability of TinyBERT Task-specific Distillation:Use specified task to enable TinyBERT to learn task-specific knowledge 2.2.1 General Distillation Use the original BERT without fine-tuning as the teacher and a large-scale text corpus of general domain as the training data Perform the above mentioned transformer distillation to obtain a general TinyBERT which may have worse performance than BERT 2.2.2 Task-specific Distillation Use fine-tuned BERT as the teacher and a domain-specific dataset as the training data Perform transformer distillation on the TinyBERT after general distillation 2.2.3 Discussion about Two-stage Learning FrameworkThe above two learning stages are complementary to each other:*1. The general distillation provides a good initialization for the task-specific distillation. The task-specific distillation further improves TinyBERT by focusing on learning the task-specific knowledge. Finally, although there is a big gap between BERT and TinyBERT in model size, by performing the proposed two-stage distillation, the TinyBERT can achieve competitive performances in various NLP tasks.\\ From the original paper 3 Experiment ResultTable 3.1: Results of GLUE official benchmark[1] Table 3.2 The model sizes and inference time for baselines and TinyBERT[1] The experiment results demonstrate that:*1. There is a large performance gap betweenBERT_SMALL and BERT_BASE due to the big reduction in model size. TinyBERT is consistently better than BERT_SMALL in all the GLUE tasks and achieves a large improvement of 6.3% on average. This indicates that the proposed KD learning framework can effectively improve the performances of small models regardless of downstream tasks. TinyBERT significantly outperforms the state-of-the-art KD baselines (i.e., BERT-PKD and DistillBERT) by a margin of at least 3.9%, even with only 28% parameters and 31% inference time of baselines. Compared with the teacher BERT_BASE, TinyBERT is 7.5x smaller and 9.4x faster in the model efficiency, while maintaining competitive performances.* * From the original paper 4 Conclusion Introduce a new KD(Knowledge Distillation) method for transformer-based distillation Propose a two-stage framework for TinyBERT learning Achieves competitive performance while reducing the model size and the inference time","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"/tags/NLP/"},{"name":"文献阅读","slug":"文献阅读","permalink":"/tags/文献阅读/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"文献阅读 Text Summarization with Pretrained Encoder","slug":"文献阅读 Text Summarization with Pretrained Encoder","date":"2020-10-23T15:45:36.000Z","updated":"2020-10-23T15:45:36.000Z","comments":true,"path":"2020/10/23/文献阅读 Text Summarization with Pretrained Encoder/","link":"","permalink":"/2020/10/23/文献阅读 Text Summarization with Pretrained Encoder/","excerpt":"","text":"1 IntroductionTitle : Text Summarization with Pretrained EncoderLink : http://arxiv.org/abs/1908.08345 Author : Yang Liu, Mirella LapataConference : Accepted by EMNLP2019Code : https://github.com/nlpyang/PreSumm 1.1 Achievement Test the feasibility of BERT in text summarization Build a general framework with extractive model and abstractive model Propose a two-stage fine-tuning approach 2 Method2.1 BERTSUM Problem : Original BERT[1] can only apply to single sentence/sentence-pair input Requirement : Input with multiple sentences and output the contextual embeddings Solution : (Modified BERT) Insert [CLS] tokens at the start of every separate sentence to collect the feature of the following[2] sentence Use crossing segment embeddings[3] EA and EB to distinguish multiple sentences in a document [1] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2019[2] In the original BERT, [CLS] symbol only appear at the start of the document for text classification. In this paper, the author said [CLS] is used for collecting the feature of the preceding sentence. However, it seems that the [CLS] symbol is actually used for collecting the feature of the following sentence.[3] In the original BERT, EA and EB are also used but limited by one sentence-pair. In this paper, EA and EB are expanded to the whole word embedding. 2.2 BERTSUMEXT[1]Requirement : Input with sentence representation and output the chosen extractive summary subset Solution : (BERTSUM -&gt; Transformer -&gt; Sigmoid Function) Use the contextual embedding of the [CLS] symbol as the sentence representation Use a L-th layer Transformer to collect document-level features for extracting summaries (L = 2 performs best) Output the binary result {0, 1} (1 for included) with a sigmoid function at the end [1] Attention is all you need, 2017 2.3 BERTSUMABS[1]Requirement : Input with sentence representation and output with abstractive summary Solution : (BERTSUM -&gt; Transformer) Use the contextual embedding of the [CLS] symbol as the sentence representation Use a 6-layered Transformer as the decoder to generate the abstractive summary [1] Get to the point: Summarization with pointer- generator networks, 2017 2.4 BERTSUMEXTABS[1][2]Requirement : Input with sentence representation and output with abstractive summary Solution : (BERTSUM -&gt; Extractor Transformer -&gt; Abstractor Transformer) Use the contextual embedding of the [CLS] symbol as the sentence representation Use a L-th layer Transformer to collect document-level features for extracting summaries (L = 2 performs best) Output the binary result {0, 1} (1 for included) with a sigmoid function at the end Use a 6-layered Transformer to paraphrase the extracted sentence into the abstractive summary Advantage : Extractive summary can boost the performance of abstractive summarization This two-stage approach with extractor and abstractor can share the information between them [1] Bottom-up abstractive summarization, 2018[2] Improving neural abstractive document summarization with explicit information selection modeling, 2018 3 Experiment Result3.1 CNN/DailyMail[1] [1] Using the split of Teaching machines to read and understand, 2015 3.2 NYT[1] [1] Following Learning-based single-document summarizationwith compression and anaphoricity constraints, 2016 4 Conclusion Apply pre-trained BERT in text summarization Introduce document-level encoder (BERTSUM) Propose a framework for extractive and abstractive summarization (BERTSUMEXTABS) Achieve state-of-the-art on dataset","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"/tags/NLP/"},{"name":"文献阅读","slug":"文献阅读","permalink":"/tags/文献阅读/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"文献阅读 Summary Level Training of Sentence Rewriting for Abstractive Summarization","slug":"文献阅读 Summary Level Training of Sentence Rewriting for Abstractive Summarization","date":"2020-10-23T15:45:32.000Z","updated":"2020-10-23T15:45:32.000Z","comments":true,"path":"2020/10/23/文献阅读 Summary Level Training of Sentence Rewriting for Abstractive Summarization/","link":"","permalink":"/2020/10/23/文献阅读 Summary Level Training of Sentence Rewriting for Abstractive Summarization/","excerpt":"","text":"1 IntroductionTitle : Summary Level Training of Sentence Rewriting for Abstractive SummarizationLink : http://arxiv.org/abs/1909.08752 Author : Sanghwan Bae, Taeuk Kim, Jihoon Kim and Sang-goo LeeConference : Proceeding of the 2nd Workshop on New Frontiers in Summarization (held by EMNLP) 1.1 Achievement Present a novel training signal that directly maximizes summary-level ROUGE scores through reinforcement learning Obtains new state-of-the-art performance on both CNN/Daily Mail and New York Times datasets[1] [1] Actually, the result of (Liu, 2019) is now state-of-the-art. 2 Method2.1 Extractor Requirement : Input with document sequence D and output with the chosen extractive sentence subset Solution : (Sequence D -&gt; BERTSUM (Encoder) -&gt; LSTM Pointer Network (Decoder)) Input the sentence vector into BERTSUM[1] in which they use BERT BASE, a small version of BERT LARGE Feed the sentence representation vector H into the LSTM Pointer Network[2] with 256 hidden size Advantage : Enable the decoder to consider the previous selected sentence Avoid information overlapping between the selected sentences [1] Text Summarization with Pretrained Encoder, 2019[2] Pointer networks, 2015 2.2 Abstractor[1]Requirement : Input with selected extractive sentence subset and output with the corresponding paraphased sentence Solution : (extractive sentence subset -&gt; seq2seq model) Use word2vec[2] model of 128 dimensions Feed extractive sentence subset modified by word2vec into seq2seq[3] model which use copying mechanism[4] for handling out-of-vocabulary words [1] Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting, 2018[2] Distributed representations of words and phrases and their compositionality, 2013[3] Neural machine translation by jointly learning to align and translate, 2015; Effective approaches to attention-based neural machine translation, 2015[4] Get to the point: Summarization with pointer-generator networks, 2017 3 Training3.1 Extractor Pre-training[1]Requirement : Selected sentences should be the ones that maximize the Rouge score with respect to gold summaries Problem : Most summarization corpora only contain human written abstractive summaries as ground truth (No extractive summary for training) It is computationally expensive to find a globally optimal subset of sentences that maximizes the Rouge score Solution : Add the document sentence to the extractive oracles until the remaining candidate can’t improve the Rouge score with respect to entire gold summary Train the extractive model with cross-entropy [1] Summarunner: A recurrent neural network based sequence model for extractive summarization of documents, 2017 3.2 Abstractor TrainingRequirement : Find the proper paraphrased function between original sentence and the gold summary Solution : For each ground truth summary sentence, find the most similar sentence in the original document with Rouge score Train the model as a usual seq2seq model with cross-entropy 3.3 Reinforcement Learning OptimizationRequirement : Optimize the Rouge metric Solution : Assume the extractor as an agent in reinforcement learning paradigm Not quite understand yet. 4 ResultTheir Rouge score is worse than the score of (Liu, 2019) . 5 Comparison and ConclusionIn this paper : Extractor : BERTSUM (Encoder) + LSTM (Decoder) Abstractor : seq2seq In (Liu, 2019) : Extractor : BERTSUM (Encoder) + Transformer + sigmoid classifier (Decoder) Abstractor : 6 layered Transformer Conclusion : Seems that Transformer in (Liu, 2019) performs better than seq2seq in this paper","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"/tags/NLP/"},{"name":"文献阅读","slug":"文献阅读","permalink":"/tags/文献阅读/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"文献阅读 RoBERTa A Robustly Optimized BERT Pretraining Approach","slug":"文献阅读 RoBERTa A Robustly Optimized BERT Pretraining Approach","date":"2020-10-23T15:45:26.000Z","updated":"2020-10-23T15:45:26.000Z","comments":true,"path":"2020/10/23/文献阅读 RoBERTa A Robustly Optimized BERT Pretraining Approach/","link":"","permalink":"/2020/10/23/文献阅读 RoBERTa A Robustly Optimized BERT Pretraining Approach/","excerpt":"","text":"1 Introduction[1] RoBERTa: A Robustly Optimized BERT Pretraining ApproachLink : http://arxiv.org/abs/1907.11692Institute：University of Washington, Facebook AICode : https://github.com/pytorch/fairseq 1.1 Achievement Present a replication study of BERT, carefully measuring the impact of many key parameters and training data size Find that BERT can be further improved Achieve state-of-the-art on GLUE, RACE and SQuAD 2 MethodFor the detail of the original BERT model, visit here.In the coming sections, the modification of their proposed model based on experiment will be introduced. 2.1 Static vs. Dynamic MaskingTable 2.1 Static vs. Dynamic Masking BERT Method 1 Static Mask Dynamic Mask BERT: (Static Mask) Only mask once while data-preprocessing Result in information loss Method 1: (Dynamic Mask) Mask in 10 different ways for 4 loops Reduce the effect of information loss Table 2.2 Comparison between static and dynamic masking Result: Static masking performs similar to the original BERT model, and dynamic masking is comparable or slightly better than static masking. Finally choose dynamic masking for RoBERTa 2.2 Model Input Format and Next Sentence PredictionTable 2.3 Several Training Format BERT Method 1 Method 2 Method 3 SEGMENT-PAIR+NSP SENTENCE-PAIR+NSP FULL-SENTENCES DOC-SENTENCES BERT: (SEGMENT-PAIR+NSP) Input with a pair of segments* Each segment can contain multiple sentences but the maximum token length of the input is 512. Train the model with NSP loss Method 1: (SENTENCE-PAIR+NSP) Input with a pair of sentences* They increase batch size to obtain a similar total number of tokens with BERT(SEGMENT-PAIR+NSP). Train the model with NSP loss Method 2: (FULL-SENTENCES) Input with full sentences sampled contiguously from different documents* The total length is at most 512. Add an extra separator token to indicate the end of one document and directly begin sampling the sentences of the next document* Deal with the boundary Remove NSP loss Method 3: (DOC-SENTENCES) Input with full sentences sampled contiguously from different documents but boundary-crossing is not allowed Input tokens may much shorter than 512 because of the cut-off near the boundary. Dynamically increase the batch size near the boundary to achieve a similar token number as Method 2(FULLSENTENCES) Remove NSP loss Table 2.4 Comparison between Different Input Formating Methods Result: Using individual sentences hurts performance on downstream tasks* They hypothesize that the model may not able to learn long-term dependency if using individual sentences. Removing the NSP loss matches or slightly improve the downstream task performance Method 3(DOC-SENTENCES) performs slightly better than Method 2(FULL-SENTENCES). To avoid variable batch size, they finally choose Method 2(FULL-SENTENCES) for RoBERTa. 2.3 Training with Large BatchesIn this section, they try to get a better result by increasing the batch size. Table 2.5 Comparison between Different Batch Size Result: It shows that training with large batches improves perplexity and end-task accuracy. Large batches can be easier to do parallelization. 2.4 Text EncodingIn this section, they try to get a better result by using a larger byte-level BPE(Byte-Pair Encoding). 3 RoBERTaTable 3.1 Comparison between RoBERTa, BERT_LARGE and XLNet_LARGE RoBERTa: (Robustly optimized BERT approach) RoBERTa, a modification version of BERT model, is trained with dynamic masking(2.1), FULL-SENTENCES without NSP loss(2.2), large mini-batches(2.3), and a larger byte-level BPE(2.4). Experiment settings:2.1. Based on BERT_LARGE(L = 24, H = 1024, A = 16, 355M parameters)2.2. Trained with 1024 Tesla V100 GPUs for about one day Experiment Result:3.1. Improve a lot over the original BERT3.2. The ultimate version achieves state-of-the-art, outperforming BERT and XLNet. 4 ConclusionWhat may improve the performance of BERT: Train the model for longer time, with bigger batches over more data Remove NSP Train on longer sequences Dynamic masking","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"/tags/NLP/"},{"name":"文献阅读","slug":"文献阅读","permalink":"/tags/文献阅读/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"python的文件IO","slug":"python的文件IO","date":"2020-10-16T10:04:22.000Z","updated":"2020-10-16T10:04:22.000Z","comments":true,"path":"2020/10/16/python的文件IO/","link":"","permalink":"/2020/10/16/python的文件IO/","excerpt":"","text":"python的文件IO背景：最近要实现一个批量读取txt，并提取信息写入csv的小功能，用作数据预处理。记录一下python的文件I/O，免得下次又得现baidu。 txt的写txt写入： with open(&#39;test.txt&#39;, &#39;w&#39;, encoding = &#39;utf-8&#39;, newline=&#39;&#39;) as txtfile: txtfile.write(&#39;Hello World\\n&#39;) txtfile.writelines([&#39;你好&#39;, &#39;你好\\n&#39;, &#39;世界&#39;]) txtfile.close() txt输出： Hello World 你好你好 世界 txt的读txt读取： with open(&#39;test.txt&#39;, &#39;r&#39;, encoding = &#39;utf-8&#39;, newline=&#39;&#39;) as txtfile: print(txtfile.readlines()) txtfile.close() 输出回显： [&#39;Hello World\\n&#39;, &#39;你好你好\\n&#39;, &#39;世界&#39;] txt读取： with open(&#39;test.txt&#39;, &#39;r&#39;, encoding = &#39;utf-8&#39;, newline=&#39;&#39;) as txtfile: for line in txtfile.readlines(): print(line) txtfile.close() 输出回显： Hello World 你好你好 世界 csv的写csv写入： import csv with open(&#39;test.csv&#39;, &#39;a&#39;, encoding = &#39;utf-8&#39;, newline=&#39;&#39;) as csvfile: csvpen = csv.writer(csvfile) csvpen.writerow([&#39;id&#39;, &#39;category&#39;, &#39;content&#39;]) with open(&#39;test.txt&#39;, &#39;r&#39;, encoding = &#39;utf-8&#39;, newline=&#39;&#39;) as txtfile: for line in txtfile.readlines(): csvpen.writerow([&#39;id&#39;, &#39;category&#39;, line.replace(&#39;\\n&#39;, &#39;&#39;)]) txtfile.close() csvfile.close() csv输出： id,category,content id,category,Hello World id,category,你好你好 id,category,世界 csv的读csv读取： import csv with open(&#39;test.csv&#39;, &#39;r&#39;, encoding = &#39;utf-8&#39;, newline=&#39;&#39;) as csvfile: csvreader = csv.reader(csvfile) for row in csvreader: print(row[-1]) 输出回显： content Hello World 你好你好 世界 附录1 open()函数open(&#39;文件名&#39;, &#39;读写方式&#39;, encoding = &#39;文本编码方式&#39;, newline=&#39;&#39;) 读写方式主要为：w（write）、r（read）、a（append）。 文本编码方式最好用utf-8，避免乱码，csv最好都用代码编译器打开。 newline是用于处理文本行尾的换行转义符的，若果不设置为空，写入csv会出现写一行空一行的bug。 open的具体用法可参考 Python3 open() 函数newline的具体原理可参考 python open函数newline用法 结尾部分 附录2 文本处理方法replace以及strip方法str.replace(old, new[, max])替换str中所有old字符串为new字符串，最多max次。 str.strip([chars])剔除str首尾中包含[chars]中任意字符，直到首尾都出现[chars]中不包含的字符为止。 strip的具体用法可参考 Python replace()方法strip的具体用法可参考 Python strip()方法 附录3 文件地址的基本用法 os.path()常用方法 用途 返回示例 os.path.abspath(‘.’) 返回当前位置的绝对路径 /root/runoob.txt os.path.abspath(‘..’) 返回当前位置的父目录的绝对路径 /root os.path.basename(‘/root/runoob.txt’) 返回当前目录的最后一个元素（不区分文件、文件夹） runoob.txt os.path.split(‘/root/runoob.txt’) 返回该目录的分割的文件夹与文件名 (‘/root’, ‘runoob.txt’)","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"python","slug":"python","permalink":"/tags/python/"},{"name":"数据处理","slug":"数据处理","permalink":"/tags/数据处理/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 6.4 表现出的迹象（〜がる、ばかり、〜めく）","slug":"日语语法 6.4 表现出的迹象（〜がる、ばかり、〜めく）","date":"2020-09-14T14:14:47.000Z","updated":"2021-11-26T18:03:51.321Z","comments":true,"path":"2020/09/14/日语语法 6.4 表现出的迹象（〜がる、ばかり、〜めく）/","link":"","permalink":"/2020/09/14/日语语法 6.4 表现出的迹象（〜がる、ばかり、〜めく）/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 6 高级话题6.4 表现出的迹象（〜がる、ばかり、〜めく）6.4.1 用「〜がる」描述表现出的感情用法公式： （い形容词）い形容词去「い」 + がる （な形容词）な形容词 + がる感情类形容词与「がる」连用表示某人表现出来的感情，显う动词性。 「がる」的活用 肯定 否定 非过去 怖がる 像是吓到 怖がらない 像是没吓到 过去 怖がった 像是吓到了 怖がらなかった 像是没吓到 示例： 早くきてよ！何を恥ずかしがっているの？快来这里哦，你看起来在害羞什么？ 彼女は朝早く起こされるのを嫌がるタイプです。我女友是那种看起来很烦被很早叫醒的类型。 うちの子供はプールに入るのを理由もなく怖がる。我们家小孩无理由的害怕进泳池。 示例： 家に帰ったら、すぐパソコンを使いたがる。看起来像是一回家就要玩电脑的样子。 みんなイタリアに行きたがっているんだけど、私の予算で行けるかどうかはとても怪しい。大家都一副想去意大利的样子，我要是提出视预算再决定去不去的话就显得奇怪了。* 「行きたがっているん」原文例句中没有「い」应该是笔误。 妻はルイヴィトンのバッグを欲しがっているんだけど、そんなもん、買えるわけないでしょう！老婆像是想要LV包的样子，但我哪里买得起！ 「〜がる」和「屋」一起用的时候表示什么样的人，比如「恥ずかしがり屋」（容易害羞的人）、「寒がり屋」（怕冷的人）或「暑がり屋」（怕热的人）。 示例： 私は寒がり屋だから、ミネソタで暮らすのは辛かった。我是怕冷的人，所以住在明尼苏达州很痛苦。 6.4.2 用「ばかり」表示像是要做某事用法公式： （动词非过去、肯定）动词否定式去掉ない + ん + ばかり （动词其他形式）动词其他形式 + ばかり动词与「ばかり」连用表示某个动作像是要发生。 肯定 否定 非过去 言わんばかり 像是要说 言わないばかり 像是没说 过去 言ったばかり 像是说过 言わなかったばかり 像是没说过 示例： ボールは爆発せんばかりに、膨らんでいた。球一直在膨胀，像是要爆炸了。 「あんたとは関係ない」と言わんばかりに彼女は彼を無視していた。她无视了他，像是在说「这是跟你没关系」。 昨日の喧嘩で何も言わなかったばかりに、平気な顔をしている。他正脸色平静，仿佛在昨天的斗殴中什么也没说。 6.4.3 用「めく」形容某种状态的气氛用法公式： 名词/な形容词 + めく表示出现了/名词/な形容词/一样的气氛，显う动词性。 肯定 否定 非过去 謎めく 谜一样的气氛 謎めかない 非谜一样的气氛* 过去 謎めいた 谜一样的气氛 謎めかなかった 非谜一样的气氛* * 过去式用法不常见，仅为语法正确。 示例： 紅葉が始まり、すっかり秋めいた空気になってきた。当树叶开始变色，开始变得像秋天一样的空气了。 そんな謎めいた顔をされても、うまく説明できないよ。就算摆出那种谜一样的脸色，也无法好好说明。 いつも皮肉めいた言い方をしたら、みんなを嫌がらせるよ。要是继续用这种讽刺的口气说话，会让大家讨厌的。* 「嫌がる」为う动词，表示讨厌。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 6.8 被某物覆盖（だらけ、まみれ、ずくめ）","slug":"日语语法 6.8 被某物覆盖（だらけ、まみれ、ずくめ）","date":"2020-09-13T15:35:46.000Z","updated":"2021-11-26T18:05:33.878Z","comments":true,"path":"2020/09/13/日语语法 6.8 被某物覆盖（だらけ、まみれ、ずくめ）/","link":"","permalink":"/2020/09/13/日语语法 6.8 被某物覆盖（だらけ、まみれ、ずくめ）/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 6 高级话题6.8 被某物覆盖（だらけ、まみれ、ずくめ）6.8.1 用「だらけ」表达充斥着什么东西用法公式： 名词 + だらけ名词与「だらけ」表示充斥着该名词。合用后显名词性。 示例： このドキュメントは間違いだらけで、全然役に立たない。这份文档错误百出，完全无用。 携帯を２年間使ってたら、傷だらけになった。手机使用了两年之后，到处是伤痕。 6.8.2 用「まみれ」描述覆盖用法公式： 名词 + まみれ名词与「まみれ」合用表示物理意义上的覆盖，如灰尘、液体。 示例： 彼は油まみれになりながら、車の修理に頑張りました。他一边变得全身都是油，一边努力修着车。 たった１キロを走っただけで、汗まみれになるのは情けない。只跑了一公里就全身是汗真是让人同情。 6.8.3 用「ずくめ」表示总体用法公式： 名词 + ずくめ名词与「ずくめ」合用表示该名词全体。 示例： 白ずくめ団体は去年ニュースになっていた。一身白的团体去年有上过新闻。 このシェークは、おいしいし、栄養たっぷりで体にいいですから、いいことずくめですよ。这奶昔好吃又有营养，对你身体好，所以完全是个好东西。 让我们和作者一起水完最后几个小节/狗头","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 6.7 高级的意志（まい、であろう、かろう）","slug":"日语语法 6.7 高级的意志（まい、であろう、かろう）","date":"2020-09-13T15:35:14.000Z","updated":"2021-11-26T18:05:15.936Z","comments":true,"path":"2020/09/13/日语语法 6.7 高级的意志（まい、であろう、かろう）/","link":"","permalink":"/2020/09/13/日语语法 6.7 高级的意志（まい、であろう、かろう）/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 6 高级话题6.7 高级的意向6.7.1 负面的意向用法公式： る动词词根/う动词辞书原形 + まい （例外）する → するまい/しまい （例外）くる → くるまい动词与「まい」连用表示意志上对该动作持消极/负面态度。 示例： 相手は剣の達人だ。そう簡単には勝てまい。对手是用剑的好手，不会这么简单赢的。 そんな無茶な手段は認めますまい！不会承认这样无理的手段！ 用法公式： 名词性 + を + やめる表达劝退。 从句 + ように + する表示为了从句而努力。 示例： 明日に行くのをやめよう。明天不去了吧。 肉を食べないようにしている。正试着不吃肉。 6.7.2 用意向形表示缺少关联用法公式： 动词意向形 + が + 同动词的否定意向形 + が、句子。动词的意向形和否定意向形与「が」合用表示该动作与后面的句子没有关系。 示例： あいつが大学に入ろうが入るまいが、俺とは関係ないよ。那家伙上不上大学都跟我没关系哦。 時間があろうがあるまいが、間に合わせるしかない。不管有没有时间都只能准时。 最近のウィルスは強力で、プログラムを実行しようがしまいが、ページを見るだけで感染するらしい。最近病毒很强力，不管是否运行了程序，似乎光是看网页都能被感染。 6.7.3 用「であろう」表示可能性用法公式： 名词/形容词/动词 + であろう/名词/形容词/动词/与「であろう」连用表示可能性，用于正式场合。 示例： 今後50年、人間が直面するであろう問題に正面から向き合って、自ら解決をはかりつつ、そのノウハウが次の産業となるシナリオを考えたい。（来源：www.jkokuryo.com）今后50年，正视人类即将可能直面的问题，一边为自己计划解决办法，一边考虑那个诀窍成为下一个产业的场景。* 这个诀窍指的应该是前面说的解决方案，即将直面的问题解决的方法发展成为了一个新的产业。 もちろん、生徒数減少の現在、学科の新設は困難であろうが、職業科の統廃合や科内コースの改編などで時代に合わせた変革が求められているはずである。（来源：www1.normanet.ne.jp）当然，在目前学生数量减少的情况下，虽然新设学科可能是比较困难的，但本应该会被期望学科重组以及学科内课程调整这样和形势匹配的改革的。 6.7.4 用「かろう」作为「い」结尾单词的意向形结尾用法公式： 否定式去掉「い」/い形容词去掉「い」 + かろう「い」结尾单词的意向形。 示例： どんな商品でもネットで販売するだけで売上が伸びるというものではなかろう。只是把什么商品都在网上出售，不一定是能够增加销售量这样的事情。 運動を始めるのが早かろうが遅かろうが、健康にいいというのは変わりません。不管你开始运动早或晚，对健康有益的事实是不会变的。 休日であろうが、なかろうが、この仕事では関係ないみたい。是不是休息日好像跟这个工作没有关系。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 6.10 其他（思いきや、がてら、あげく）","slug":"日语语法 6.10 其他（思いきや、がてら、あげく）","date":"2020-09-13T14:36:56.000Z","updated":"2021-11-26T18:03:07.421Z","comments":true,"path":"2020/09/13/日语语法 6.10 其他（思いきや、がてら、あげく）/","link":"","permalink":"/2020/09/13/日语语法 6.10 其他（思いきや、がてら、あげく）/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 6 高级话题6.10 其他（思いきや、がてら、あげく）6.10.1 用「思いきや」描述意料之外的事用法公式： 想法 + と + 思いきや、结果。「思いきや」用于描述前面的预期想法与真实结果不同。 示例： 昼間だから絶対込んでいると思いきや、一人もいなかった。因为是白天，本以为肯定会变得拥挤，但竟然一个人也没有。 このレストランは安いと思いきや、会計は5千円以上だった！本以为这家餐厅会便宜，但结账却要五千多円！ 6.10.2 用「〜がてら」描述一次做两件事 名词/动词1词根 + がてら、动词2。用「がてら」描述在名词/动作1的过程中，还进行了动作2。 示例： 散歩がてら、タバコを買いに行きました。散步的时候，还去买了烟。 博物館を見がてらに、お土産を買うつもりです。参观博物馆的时候，我还准备买点特产。 6.10.3 用「〜あげく（挙句）」描述不好的结果用法公式： （名词）名词 + の + あげく、结果。 （动词）动词过去形 + あげく、结果。用「あげく（挙句）」描述经过了 名词/动作 大量的 时间/努力 之后得到了结果。这个结果通常是负面的。「あげく（挙句）」显名词性。「あげくの果【は】て」是语气更强的版本。 示例： 事情を2時間かけて説明したあげく、納得してもらえなかった。解释了情况两小时，还没有得到理解。 先生と相談のあげく、退学をしないことにした。与老师相谈之后，才决定不退学。 2020.02.17-2020.07.21-2020.09.13 日语语法中译本p23-p251-p296 好了现在过了一遍这本魔法书语法书 之后会做一些校正然后做个目录类的东西","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 6.9 描述时间空间接近的高级方法（が早いか、や否や、そばから）","slug":"日语语法 6.9 描述时间空间接近的高级方法（が早いか、や否や、そばから）","date":"2020-09-13T14:36:37.000Z","updated":"2021-11-26T18:02:36.290Z","comments":true,"path":"2020/09/13/日语语法 6.9 描述时间空间接近的高级方法（が早いか、や否や、そばから）/","link":"","permalink":"/2020/09/13/日语语法 6.9 描述时间空间接近的高级方法（が早いか、や否や、そばから）/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 6 高级话题6.9 描述时间空间接近的高级方法6.9.1 用「が早いか」描述事情发生的那一刻用法公式： 动词1辞书形/（动词过去形） ＋ が早いか、动词2过去形。「が早いか」用于描述动作1发生后动作2发生。该用法只能描述直接相关、确实发生了的事件。 示例： 彼女は、教授の姿を見るが早いか、教室から逃げ出した。一看到教授的身影，她就从教室逃出去了。 「食べてみよう」と言うが早いか、口の中に放り込んだ。刚说完「吃了吧」，他就扔进了嘴里。 「食べてみよう」と言ったが早いか、口の中に放り込んだ。刚说完「吃了吧」，他就扔进了嘴里。 6.9.2 用「や/や否や」描述紧接着发生了什么用法公式： 动词1辞书形 + や/や否や、动词2过去式/辞书形。「や/や否や」用于描述动作1发生后动作2发生。动作2为过去形时，该动作需要确实发生过；动作2为辞书形时，该动作是定期发生的。 示例： 私の顔を見るや、何か言おうとした。一看到我的脸就想说些什么。* 「何か言おうとした」的用法详见 4.13.2 试图做成某事 。 搭乗のアナウンスが聞こえるや否や、みんながゲートの方へ走り出した。一听到登机通知的声音，大家就开始往登机口跑。 6.9.3 用「そばから」描述随后重复发生的事用法公式： 动词1辞书形 + そばから + 动词2辞书形「そばから」用于描述动作1发生后动作2会重复发生。我觉得因为会重复发生因此动作2需要是现在时（辞书形，可能存在时态不变的活用）。 示例： 子供が掃除するそばから散らかすから、もうあきらめたくなった。刚打扫完房间小孩子就又总是把它弄乱了，所以已经快要变得放弃了。 教科書を読んだそばから忘れてしまうので勉強ができない。读完课本后总是不小心忘记，所以我没法学习。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"modeling.py逐行注释","slug":"modeling.py逐行注释","date":"2020-09-04T14:35:16.000Z","updated":"2020-09-04T14:35:16.000Z","comments":true,"path":"2020/09/04/modeling.py逐行注释/","link":"","permalink":"/2020/09/04/modeling.py逐行注释/","excerpt":"","text":"本文为BERT的modeling.py模块，即建模模块，进行逐行注释。 modeling.py头文件&quot;&quot;&quot;BERT finetuning runner.&quot;&quot;&quot; from __future__ import absolute_import from __future__ import division from __future__ import print_function import collections import csv import os import modeling import optimization import tokenization import tensorflow as tf BertConfigclass BertConfig(object): &quot;&quot;&quot;Configuration for `BertModel`.&quot;&quot;&quot; def __init__(self, vocab_size, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, hidden_act=&quot;gelu&quot;, hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, max_position_embeddings=512, type_vocab_size=16, initializer_range=0.02): &quot;&quot;&quot;Constructs BertConfig. Args: vocab_size: Vocabulary size of `inputs_ids` in `BertModel`. hidden_size: Size of the encoder layers and the pooler layer. num_hidden_layers: Number of hidden layers in the Transformer encoder. num_attention_heads: Number of attention heads for each attention layer in the Transformer encoder. intermediate_size: The size of the &quot;intermediate&quot; (i.e., feed-forward) layer in the Transformer encoder. hidden_act: The non-linear activation function (function or string) in the encoder and pooler. hidden_dropout_prob: The dropout probability for all fully connected layers in the embeddings, encoder, and pooler. attention_probs_dropout_prob: The dropout ratio for the attention probabilities. max_position_embeddings: The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048). type_vocab_size: The vocabulary size of the `token_type_ids` passed into `BertModel`. initializer_range: The stdev of the truncated_normal_initializer for initializing all weight matrices. &quot;&quot;&quot; # 将参数init为该类的属性 self.vocab_size = vocab_size self.hidden_size = hidden_size self.num_hidden_layers = num_hidden_layers self.num_attention_heads = num_attention_heads self.hidden_act = hidden_act self.intermediate_size = intermediate_size self.hidden_dropout_prob = hidden_dropout_prob self.attention_probs_dropout_prob = attention_probs_dropout_prob self.max_position_embeddings = max_position_embeddings self.type_vocab_size = type_vocab_size self.initializer_range = initializer_range # 从python字典中读取BERT的配置参数 @classmethod def from_dict(cls, json_object): &quot;&quot;&quot;Constructs a `BertConfig` from a Python dictionary of parameters.&quot;&quot;&quot; config = BertConfig(vocab_size=None) for (key, value) in six.iteritems(json_object): config.__dict__[key] = value return config # 从json文件中读取BERT的配置参数 @classmethod def from_json_file(cls, json_file): &quot;&quot;&quot;Constructs a `BertConfig` from a json file of parameters.&quot;&quot;&quot; with tf.gfile.GFile(json_file, &quot;r&quot;) as reader: text = reader.read() return cls.from_dict(json.loads(text)) # 将实体序列化为python字典 def to_dict(self): &quot;&quot;&quot;Serializes this instance to a Python dictionary.&quot;&quot;&quot; output = copy.deepcopy(self.__dict__) return output # 将实体序列化为json字符串 def to_json_string(self): &quot;&quot;&quot;Serializes this instance to a JSON string.&quot;&quot;&quot; return json.dumps(self.to_dict(), indent=2, sort_keys=True) + &quot;\\n&quot; 该类用于存储BERT参数。各参数意义： vocab_size：inputs_ids的Vocabulary的大小 hidden_size：隐藏层（编码层/pooler层）的大小 num_hidden_layers：Transformer编码器的hidden层个数 num_attention_heads：Transformer编码器的注意力头个数 intermediate_size：中间层（FFN）大小 hidden_act：隐藏层（编码层/pooler层）激活函数 hidden_dropout_prob：隐藏层（embedding层/编码层/pooler层）dropout概率 attention_probs_dropout_prob：注意力概率的dropout概率 max_position_embeddings：最大序列长度（512/1024/2048） type_vocab_size：token_type_ids的Vocabulary的大小（*有篇文章说这个是segement_id???） initializer_range：设置个参数矩阵初始值时truncated_normal_initializer的标准差 默认值：vocab_size,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act=”gelu”,hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=16,initializer_range=0.02 BertModelclass BertModel(object): &quot;&quot;&quot;BERT model (&quot;Bidirectional Encoder Representations from Transformers&quot;). Example usage: \\\\\\python # Already been converted into WordPiece token ids input_ids = tf.constant([[31, 51, 99], [15, 5, 0]]) input_mask = tf.constant([[1, 1, 1], [1, 1, 0]]) token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]]) config = modeling.BertConfig(vocab_size=32000, hidden_size=512, num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024) model = modeling.BertModel(config=config, is_training=True, input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids) label_embeddings = tf.get_variable(...) pooled_output = model.get_pooled_output() logits = tf.matmul(pooled_output, label_embeddings) ... \\\\\\ &quot;&quot;&quot; def __init__(self, config, is_training, input_ids, input_mask=None, token_type_ids=None, use_one_hot_embeddings=False, scope=None): &quot;&quot;&quot;Constructor for BertModel. Args: config: `BertConfig` instance. is_training: bool. true for training model, false for eval model. Controls whether dropout will be applied. input_ids: int32 Tensor of shape [batch_size, seq_length]. input_mask: (optional) int32 Tensor of shape [batch_size, seq_length]. token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length]. use_one_hot_embeddings: (optional) bool. Whether to use one-hot word embeddings or tf.embedding_lookup() for the word embeddings. scope: (optional) variable scope. Defaults to &quot;bert&quot;. Raises: ValueError: The config is invalid or one of the input tensor shapes is invalid. &quot;&quot;&quot; config = copy.deepcopy(config) # 若不是训练，则将隐藏层以及注意力概率的dropout设置为0 if not is_training: config.hidden_dropout_prob = 0.0 config.attention_probs_dropout_prob = 0.0 # 获取input_id的形状列表 # 期待的rank为2，若实际值不是2，则报错 # 猜测input_ids的格式为 # [[10, 200, 3000, 40000], # [20, 300, 4000, 50000],] # 例子中的batch_size为2，seq_length为4。 input_shape = get_shape_list(input_ids, expected_rank=2) batch_size = input_shape[0] seq_length = input_shape[1] # 若input_mask为None， # 则为其创立一个与input_ids同shape的矩阵。 if input_mask is None: input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32) # 若token_type_ids为None， # 则为其创立一个与input_ids同shape的矩阵。 if token_type_ids is None: token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32) # BERT模型主体 with tf.variable_scope(scope, default_name=&quot;bert&quot;): # embedding部分 with tf.variable_scope(&quot;embeddings&quot;): # Perform embedding lookup on the word ids. # 得到word_embedding输出 以及 # input_ids到word_embedding输出的变换矩阵embedding_table # 旨在将vocab_size的one-hot编码的输入转化为hidden_size的tensor (self.embedding_output, self.embedding_table) = embedding_lookup( # 输入id input_ids=input_ids, # vocabulary大小 vocab_size=config.vocab_size, # embedding大小（隐藏层大小） embedding_size=config.hidden_size, # 初始化参数矩阵的标准差 initializer_range=config.initializer_range, # word_embedding命名 word_embedding_name=&quot;word_embeddings&quot;, # 是否使用one-hot编码 use_one_hot_embeddings=use_one_hot_embeddings) # Add positional embeddings and token type embeddings, then layer # normalize and perform dropout. # 得到positional_embedding以及 # token_type_embedding（segment_embedding） self.embedding_output = embedding_postprocessor( # 输入张量为上一轮输出的包含word_embedding的embedding_output input_tensor=self.embedding_output, # 是否使用segment_embedding use_token_type=True, # segment_embedding赋值 token_type_ids=token_type_ids, # token_type_ids的vocabulary大小 token_type_vocab_size=config.type_vocab_size, # token_type_embedding命名 token_type_embedding_name=&quot;token_type_embeddings&quot;, # 是否使用positional_embedding use_position_embeddings=True, # position_embedding命名 position_embedding_name=&quot;position_embeddings&quot;, # 参数矩阵初始化的正态分布标准差 initializer_range=config.initializer_range, # 最长序列长度 max_position_embeddings=config.max_position_embeddings, # 隐藏层dropout概率 dropout_prob=config.hidden_dropout_prob) # encoder部分 with tf.variable_scope(&quot;encoder&quot;): # This converts a 2D mask of shape [batch_size, seq_length] to a 3D # mask of shape [batch_size, seq_length, seq_length] which is used # for the attention scores. # 将[batch_size, seq_length]转化为[batch_size, seq_length, seq_length] # 用于计算attention scores attention_mask = create_attention_mask_from_input_mask( input_ids, input_mask) # Run the stacked transformer. # `sequence_output` shape = [batch_size, seq_length, hidden_size]. # 多层Transformer模型 self.all_encoder_layers = transformer_model( # 输入相加后的word_embedding、segment_embedding、positioanl embedding input_tensor=self.embedding_output, # attention掩码[batch_size, seq_length, seq_length] attention_mask=attention_mask, # 隐藏层大小 hidden_size=config.hidden_size, # 隐藏层层数 num_hidden_layers=config.num_hidden_layers, # attention头个数 num_attention_heads=config.num_attention_heads, # 中间层大小 intermediate_size=config.intermediate_size, # 中间层激活函数 intermediate_act_fn=get_activation(config.hidden_act), # 隐藏层dropout概率 hidden_dropout_prob=config.hidden_dropout_prob, # attention概率的dropout概率 attention_probs_dropout_prob=config.attention_probs_dropout_prob, # 参数初始化正态分布的标准差 initializer_range=config.initializer_range, # 是否返回所有层输出 do_return_all_layers=True) # 选择最终的Transformer输出为最后一层 self.sequence_output = self.all_encoder_layers[-1] # pooler部分 # The &quot;pooler&quot; converts the encoded sequence tensor of shape # [batch_size, seq_length, hidden_size] to a tensor of shape # [batch_size, hidden_size]. This is necessary for segment-level # (or segment-pair-level) classification tasks where we need a fixed # dimensional representation of the segment. with tf.variable_scope(&quot;pooler&quot;): # We &quot;pool&quot; the model by simply taking the hidden state corresponding # to the first token. We assume that this has been pre-trained # 取序列中的第一个token（[CLS]）jingguo Transformer的output first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1) # 建立一个输出为hidden_size的全连接网络 # 将Transformer的输出维度pooling为固定长度 self.pooled_output = tf.layers.dense( # 输入张量 first_token_tensor, # 输出维度 config.hidden_size, # 激活函数 activation=tf.tanh, # 参数初始化 kernel_initializer=create_initializer(config.initializer_range)) # 获取pooled输出（返回pooling的[CLS]输出） def get_pooled_output(self): return self.pooled_output # 获取序列输出（返回Transfomer最后一层的全部输出） def get_sequence_output(self): &quot;&quot;&quot;Gets final hidden layer of encoder. Returns: float Tensor of shape [batch_size, seq_length, hidden_size] corresponding to the final hidden of the transformer encoder. &quot;&quot;&quot; return self.sequence_output # 获取Transformer中所有encoder层的输出 def get_all_encoder_layers(self): return self.all_encoder_layers # 获取normalization之后的 # word_embedding、segment_embedding、positioinal_embedding的和 def get_embedding_output(self): &quot;&quot;&quot;Gets output of the embedding lookup (i.e., input to the transformer). Returns: float Tensor of shape [batch_size, seq_length, hidden_size] corresponding to the output of the embedding layer, after summing the word embeddings with the positional embeddings and the token type embeddings, then performing layer normalization. This is the input to the transformer. &quot;&quot;&quot; return self.embedding_output # 获取input_ids到word_embedding输出的变换矩阵 def get_embedding_table(self): return self.embedding_table 该类是BERT模型的主体部分，包含了从 input_ids -&gt; embedding -&gt; Transfomer输出张量 的所需方法。 geludef gelu(x): &quot;&quot;&quot;Gaussian Error Linear Unit. This is a smoother version of the RELU. Original paper: https://arxiv.org/abs/1606.08415 Args: x: float Tensor to perform activation. Returns: `x` with the GELU activation applied. &quot;&quot;&quot; cdf = 0.5 * (1.0 + tf.tanh( (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))) return x * cdf 定义gelu函数。 get_activationdef get_activation(activation_string): &quot;&quot;&quot;Maps a string to a Python function, e.g., &quot;relu&quot; =&gt; `tf.nn.relu`. Args: activation_string: String name of the activation function. Returns: A Python function corresponding to the activation function. If `activation_string` is None, empty, or &quot;linear&quot;, this will return None. If `activation_string` is not a string, it will return `activation_string`. Raises: ValueError: The `activation_string` does not correspond to a known activation. &quot;&quot;&quot; # We assume that anything that&quot;s not a string is already an activation # function, so we just return it. # 判断activation_string是否为six.string_types字符串 # 若非字符串，则视为已经是定义好的激活函数，直接返回activation_string。 if not isinstance(activation_string, six.string_types): return activation_string # 若是字符串，但字符串为空，则返回None。 if not activation_string: return None # 若是字符串，且不为空，则将该字符串强制小写。 act = activation_string.lower() # 判断激活函数类型 if act == &quot;linear&quot;: return None elif act == &quot;relu&quot;: return tf.nn.relu elif act == &quot;gelu&quot;: return gelu elif act == &quot;tanh&quot;: return tf.tanh else: # 若非linear、relu、gelu、tanh，则报错不支持的激活函数。 raise ValueError(&quot;Unsupported activation: %s&quot; % act) 该函数用于通过输入字符串调用不同的激活函数。 get_assignment_map_from_checkpointdef get_assignment_map_from_checkpoint(tvars, init_checkpoint): &quot;&quot;&quot;Compute the union of the current variables and checkpoint variables.&quot;&quot;&quot; assignment_map = {} initialized_variable_names = {} name_to_variable = collections.OrderedDict() for var in tvars: name = var.name m = re.match(&quot;^(.*):\\\\d+$&quot;, name) if m is not None: name = m.group(1) name_to_variable[name] = var init_vars = tf.train.list_variables(init_checkpoint) assignment_map = collections.OrderedDict() for x in init_vars: (name, var) = (x[0], x[1]) if name not in name_to_variable: continue assignment_map[name] = name initialized_variable_names[name] = 1 initialized_variable_names[name + &quot;:0&quot;] = 1 return (assignment_map, initialized_variable_names) * 没看明白，需要结合run_classifier一起看。 dropoutdef dropout(input_tensor, dropout_prob): &quot;&quot;&quot;Perform dropout. Args: input_tensor: float Tensor. dropout_prob: Python float. The probability of dropping out a value (NOT of *keeping* a dimension as in `tf.nn.dropout`). Returns: A version of `input_tensor` with dropout applied. &quot;&quot;&quot; # 若dropout_prob为空或为0，则不对张量进行dropout，直接返回原张量。 if dropout_prob is None or dropout_prob == 0.0: return input_tensor # 否则，使用tf.nn.dropout对张量进行概率dropout。 output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob) return output 定义输入张量的dropout规则。 layer_normdef layer_norm(input_tensor, name=None): &quot;&quot;&quot;Run layer normalization on the last dimension of the tensor.&quot;&quot;&quot; return tf.contrib.layers.layer_norm( inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name) 该函数用于对张量的最后一个维度进行层级的normalization。* 具体参数意义需进一步查看源码，位置：C:\\Users\\你的用户名\\AppData\\Local\\Programs\\Python\\Python37\\Lib\\site-packages\\tensorflow_core\\contrib\\layers\\python\\layers.py layer_norm_and_dropoutdef layer_norm_and_dropout(input_tensor, dropout_prob, name=None): &quot;&quot;&quot;Runs layer normalization followed by dropout.&quot;&quot;&quot; output_tensor = layer_norm(input_tensor, name) output_tensor = dropout(output_tensor, dropout_prob) return output_tensor 该函数用于层级的normalization以及dropout。 create_initializerdef create_initializer(initializer_range=0.02): &quot;&quot;&quot;Creates a `truncated_normal_initializer` with the given range.&quot;&quot;&quot; return tf.truncated_normal_initializer(stddev=initializer_range) 该函数用于生成一个指定标准差的参数矩阵initializer。 embedding_lookupdef embedding_lookup(input_ids, vocab_size, embedding_size=128, initializer_range=0.02, word_embedding_name=&quot;word_embeddings&quot;, use_one_hot_embeddings=False): &quot;&quot;&quot;Looks up words embeddings for id tensor. Args: input_ids: int32 Tensor of shape [batch_size, seq_length] containing word ids. vocab_size: int. Size of the embedding vocabulary. embedding_size: int. Width of the word embeddings. initializer_range: float. Embedding initialization range. word_embedding_name: string. Name of the embedding table. use_one_hot_embeddings: bool. If True, use one-hot method for word embeddings. If False, use `tf.gather()`. Returns: float Tensor of shape [batch_size, seq_length, embedding_size]. &quot;&quot;&quot; # This function assumes that the input is of shape [batch_size, seq_length, # num_inputs]. # # If the input is a 2D tensor of shape [batch_size, seq_length], we # reshape to [batch_size, seq_length, 1]. # 如果张量的维度为2（[batch_size, seq_length]）， # 则将其扩张为[batch_size, seq_length, 1]。 if input_ids.shape.ndims == 2: input_ids = tf.expand_dims(input_ids, axis=[-1]) # 使用create_initializer创建一个shape为[vocab_size, embedding_size]的参数矩阵 embedding_table = tf.get_variable( name=word_embedding_name, shape=[vocab_size, embedding_size], initializer=create_initializer(initializer_range)) # 将多维度的input_ids扁平化reshape为1维向量 flat_input_ids = tf.reshape(input_ids, [-1]) # 如果使用one hot编码，则将扁平化后的input_ids转化为vocab_size深度的one hot向量。 if use_one_hot_embeddings: one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size) # 将编码后的input_ids与参数矩阵相乘得到输出 output = tf.matmul(one_hot_input_ids, embedding_table) # 否则根据扁平化后的flat_input_ids从embedding_table中抽出与ids对应的向量作为输出 else: output = tf.gather(embedding_table, flat_input_ids) # 获取input_ids的shape input_shape = get_shape_list(input_ids) # 将扁平化的output重新reshape为[batch_size, seq_length, embedding_size]的形式 output = tf.reshape(output, input_shape[0:-1] + [input_shape[-1] * embedding_size]) # 返回输出的word_embedding以及参数矩阵embedding_table return (output, embedding_table) 该函数用于将整型的input_ids转换为hidden_size维的浮点型张量后输出，并获取转换过程中的参数矩阵，即embedding_table。 create_attention_mask_from_input_maskdef create_attention_mask_from_input_mask(from_tensor, to_mask): &quot;&quot;&quot;Create 3D attention mask from a 2D tensor mask. Args: from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...]. to_mask: int32 Tensor of shape [batch_size, to_seq_length]. Returns: float Tensor of shape [batch_size, from_seq_length, to_seq_length]. &quot;&quot;&quot; # 获取输入张量的shape，[batch_size, from_seq_length]。 from_shape = get_shape_list(from_tensor, expected_rank=[2, 3]) # shape的首位是batch_size batch_size = from_shape[0] # shape的次位是from_seq_length from_seq_length = from_shape[1] # 获取to_mask的shape，[batch_size, to_seq_length]。 to_shape = get_shape_list(to_mask, expected_rank=2) # shape的次位是to_seq_length to_seq_length = to_shape[1] # 将to_mask的形状reshape为[batch_size, 1, to_seq_length]，并将元素类型转换为float。 to_mask = tf.cast( tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32) # We don&#39;t assume that `from_tensor` is a mask (although it could be). We # don&#39;t actually care if we attend *from* padding tokens (only *to* padding) # tokens so we create a tensor of all ones. # # `broadcast_ones` = [batch_size, from_seq_length, 1] # 生成一个shape为[batch_size, from_seq_length, 1]的元素均为1的矩阵 broadcast_ones = tf.ones( shape=[batch_size, from_seq_length, 1], dtype=tf.float32) # Here we broadcast along two dimensions to create the mask. # 相乘后，会生成一个[batch_size, from_seq_length,to_seq_length]的张量。 # 即相乘时，两个张量的第一个维度batch_size部分保持不变， # [from_seq_length, 1]和[1, to_seq_length]部分按照矩阵相乘 # 得到一个[from_seq_length,to_seq_length]的矩阵。 mask = broadcast_ones * to_mask # 返回输入张量的mask张量 return mask 该函数基于2D的输入input_ids以及该输入的input_mask，生成3D的attention_mask。相关理解参考 https://www.cnblogs.com/gczr/p/12382240.html 的第四部分，构造 attention_mask。 attention_layerdef attention_layer(from_tensor, to_tensor, attention_mask=None, num_attention_heads=1, size_per_head=512, query_act=None, key_act=None, value_act=None, attention_probs_dropout_prob=0.0, initializer_range=0.02, do_return_2d_tensor=False, batch_size=None, from_seq_length=None, to_seq_length=None): &quot;&quot;&quot;Performs multi-headed attention from `from_tensor` to `to_tensor`. This is an implementation of multi-headed attention based on &quot;Attention is all you Need&quot;. If `from_tensor` and `to_tensor` are the same, then this is self-attention. Each timestep in `from_tensor` attends to the corresponding sequence in `to_tensor`, and returns a fixed-with vector. This function first projects `from_tensor` into a &quot;query&quot; tensor and `to_tensor` into &quot;key&quot; and &quot;value&quot; tensors. These are (effectively) a list of tensors of length `num_attention_heads`, where each tensor is of shape [batch_size, seq_length, size_per_head]. Then, the query and key tensors are dot-producted and scaled. These are softmaxed to obtain attention probabilities. The value tensors are then interpolated by these probabilities, then concatenated back to a single tensor and returned. In practice, the multi-headed attention are done with transposes and reshapes rather than actual separate tensors. Args: from_tensor: float Tensor of shape [batch_size, from_seq_length, from_width]. to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width]. attention_mask: (optional) int32 Tensor of shape [batch_size, from_seq_length, to_seq_length]. The values should be 1 or 0. The attention scores will effectively be set to -infinity for any positions in the mask that are 0, and will be unchanged for positions that are 1. num_attention_heads: int. Number of attention heads. size_per_head: int. Size of each attention head. query_act: (optional) Activation function for the query transform. key_act: (optional) Activation function for the key transform. value_act: (optional) Activation function for the value transform. attention_probs_dropout_prob: (optional) float. Dropout probability of the attention probabilities. initializer_range: float. Range of the weight initializer. do_return_2d_tensor: bool. If True, the output will be of shape [batch_size * from_seq_length, num_attention_heads * size_per_head]. If False, the output will be of shape [batch_size, from_seq_length, num_attention_heads * size_per_head]. batch_size: (Optional) int. If the input is 2D, this might be the batch size of the 3D version of the `from_tensor` and `to_tensor`. from_seq_length: (Optional) If the input is 2D, this might be the seq length of the 3D version of the `from_tensor`. to_seq_length: (Optional) If the input is 2D, this might be the seq length of the 3D version of the `to_tensor`. Returns: float Tensor of shape [batch_size, from_seq_length, num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is true, this will be of shape [batch_size * from_seq_length, num_attention_heads * size_per_head]). Raises: ValueError: Any of the arguments or tensor shapes are invalid. &quot;&quot;&quot; # 为了计算score，将输入的张量的维度顺序进行调整。 # 这里的transpose不是指的转置，而是指的顺序调整。 def transpose_for_scores(input_tensor, batch_size, num_attention_heads, seq_length, width): # 将[batch_size*seq_length, num_attention_heads*width]的相对扁平的张量 # reshape为高维度的[batch_size, seq_length, num_attention_heads, width] output_tensor = tf.reshape( input_tensor, [batch_size, seq_length, num_attention_heads, width]) # 调换张量的两个维度seq_length, num_attention_heads的顺序 output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3]) # 返回输出张量 return output_tensor # 获得from_tensor和to_tensor的shape # 在self-attention中，from_tensor和to_tensor相同。 from_shape = get_shape_list(from_tensor, expected_rank=[2, 3]) to_shape = get_shape_list(to_tensor, expected_rank=[2, 3]) # 若from_tensor和to_tensor的shape不同，则报错。 if len(from_shape) != len(to_shape): raise ValueError( &quot;The rank of `from_tensor` must match the rank of `to_tensor`.&quot;) # 若from_tensor的维度为3， # 则为batch_size、from_seq_length、to_seq_length赋对应值。 if len(from_shape) == 3: batch_size = from_shape[0] from_seq_length = from_shape[1] to_seq_length = to_shape[1] # 若from_tensor的维度为2 elif len(from_shape) == 2: # 这必然报错啊 # batch_size、from_seq_length、to_seq_length都没赋值啊。。。 if (batch_size is None or from_seq_length is None or to_seq_length is None): raise ValueError( &quot;When passing in rank 2 tensors to attention_layer, the values &quot; &quot;for `batch_size`, `from_seq_length`, and `to_seq_length` &quot; &quot;must all be specified.&quot;) # Scalar dimensions referenced here: # B = batch size (number of sequences) # F = `from_tensor` sequence length # T = `to_tensor` sequence length # N = `num_attention_heads` # H = `size_per_head` # F和T虽然是两个变量，但实际值相同 # from_tensor用于生成query # to_tensor用于生成key和value # 保留from_tensor、to_tensor的最后一个维度的信息，其他维度信息按顺序扁平化。 # [ [ [1, 2], [ [1, 2], # [2, 3] ], ---\\ [2, 3], # [ [3, 4], ---/ [3, 4], # [4, 5] ] ] [4, 5] ] # [2, 2, 2] ---&gt; [4, 2] from_tensor_2d = reshape_to_matrix(from_tensor) to_tensor_2d = reshape_to_matrix(to_tensor) # 将原有的每个token的word_embedding宽度，通过一个全连接层， # 输出为query层、key层、value层num_attention_heads * size_per_head的向量宽度。 # `query_layer` = [B*F, N*H] query_layer = tf.layers.dense( from_tensor_2d, num_attention_heads * size_per_head, activation=query_act, name=&quot;query&quot;, kernel_initializer=create_initializer(initializer_range)) # `key_layer` = [B*T, N*H] key_layer = tf.layers.dense( to_tensor_2d, num_attention_heads * size_per_head, activation=key_act, name=&quot;key&quot;, kernel_initializer=create_initializer(initializer_range)) # `value_layer` = [B*T, N*H] value_layer = tf.layers.dense( to_tensor_2d, num_attention_heads * size_per_head, activation=value_act, name=&quot;value&quot;, kernel_initializer=create_initializer(initializer_range)) # 为了计算scores，将query层、key层的张量维度顺序进行调整。 # `query_layer` = [B, N, F, H] query_layer = transpose_for_scores(query_layer, batch_size, num_attention_heads, from_seq_length, size_per_head) # `key_layer` = [B, N, T, H] key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads, to_seq_length, size_per_head) # Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw # attention scores. # 使用公式计算attention_scores # `attention_scores` = [B, N, F, T] attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True) attention_scores = tf.multiply(attention_scores, 1.0 / math.sqrt(float(size_per_head))) # 若attention_mask存在 if attention_mask is not None: # 将attention_mask扩大一个维度 # `attention_mask` = [B, 1, F, T] attention_mask = tf.expand_dims(attention_mask, axis=[1]) # Since attention_mask is 1.0 for positions we want to attend and 0.0 for # masked positions, this operation will create a tensor which is 0.0 for # positions we want to attend and -10000.0 for masked positions. # 若attention_mask为1.0，则adder被设置为0.0； # 若attention_mask为0.0，则adder被设置为-10000.0。 adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0 # Since we are adding it to the raw scores before the softmax, this is # effectively the same as removing these entirely. # 为attention_scores加上这个带有mask信息的adder attention_scores += adder # Normalize the attention scores to probabilities. # 对attention_scores进行softmax归一化 # `attention_probs` = [B, N, F, T] attention_probs = tf.nn.softmax(attention_scores) # This is actually dropping out entire tokens to attend to, which might # seem a bit unusual, but is taken from the original Transformer paper. # *对最终输出的attention_probs进行dropout attention_probs = dropout(attention_probs, attention_probs_dropout_prob) # 将[B*T, N*H]的value_layer张量reshape为[B, T, N, H] # `value_layer` = [B, T, N, H] value_layer = tf.reshape( value_layer, [batch_size, to_seq_length, num_attention_heads, size_per_head]) # 调整value_layer的维度顺序 # `value_layer` = [B, N, T, H] value_layer = tf.transpose(value_layer, [0, 2, 1, 3]) # 将attention_probs和value_layer相乘，得到最终的context_layer。 # `context_layer` = [B, N, F, H] context_layer = tf.matmul(attention_probs, value_layer) # 调整context_layer的维度顺序 # `context_layer` = [B, F, N, H] context_layer = tf.transpose(context_layer, [0, 2, 1, 3]) # 若将context_layer转为2d张量 if do_return_2d_tensor: # 将[B, F, N, H]的context_layer张量reshape为[B*F, N*H] # `context_layer` = [B*F, N*H] context_layer = tf.reshape( context_layer, [batch_size * from_seq_length, num_attention_heads * size_per_head]) # 否则将[B, F, N, H]的context_layer转为[B, F, N*H] else: # `context_layer` = [B, F, N*H] context_layer = tf.reshape( context_layer, [batch_size, from_seq_length, num_attention_heads * size_per_head]) # 返回context_layer return context_layer 该函数用于将上层输入的word_embedding按照Transformer中的Masked Multi-Head Attention部分的原理转换为context_layer并输出。具体原理可以参考 如何理解 Transformer 。 transformer_modeldef transformer_model(input_tensor, attention_mask=None, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, intermediate_act_fn=gelu, hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, initializer_range=0.02, do_return_all_layers=False): &quot;&quot;&quot;Multi-headed, multi-layer Transformer from &quot;Attention is All You Need&quot;. This is almost an exact implementation of the original Transformer encoder. See the original paper: https://arxiv.org/abs/1706.03762 Also see: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py Args: input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size]. attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length, seq_length], with 1 for positions that can be attended to and 0 in positions that should not be. hidden_size: int. Hidden size of the Transformer. num_hidden_layers: int. Number of layers (blocks) in the Transformer. num_attention_heads: int. Number of attention heads in the Transformer. intermediate_size: int. The size of the &quot;intermediate&quot; (a.k.a., feed forward) layer. intermediate_act_fn: function. The non-linear activation function to apply to the output of the intermediate/feed-forward layer. hidden_dropout_prob: float. Dropout probability for the hidden layers. attention_probs_dropout_prob: float. Dropout probability of the attention probabilities. initializer_range: float. Range of the initializer (stddev of truncated normal). do_return_all_layers: Whether to also return all layers or just the final layer. Returns: float Tensor of shape [batch_size, seq_length, hidden_size], the final hidden layer of the Transformer. Raises: ValueError: A Tensor shape or parameter is invalid. &quot;&quot;&quot; # *若hidden_size不是num_attention_heads的整数倍，则报错。 if hidden_size % num_attention_heads != 0: raise ValueError( &quot;The hidden size (%d) is not a multiple of the number of attention &quot; &quot;heads (%d)&quot; % (hidden_size, num_attention_heads)) # 每个attention head的大小 attention_head_size = int(hidden_size / num_attention_heads) # 获取输入张量的shape input_shape = get_shape_list(input_tensor, expected_rank=3) # 为batch_size、seq_length、input_width赋值 batch_size = input_shape[0] seq_length = input_shape[1] input_width = input_shape[2] # The Transformer performs sum residuals on all layers so the input needs # to be the same as the hidden size. # 若input_width和hidden_size不一样大，则报错。 if input_width != hidden_size: raise ValueError(&quot;The width of the input tensor (%d) != hidden size (%d)&quot; % (input_width, hidden_size)) # We keep the representation as a 2D tensor to avoid re-shaping it back and # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on # the GPU/CPU but may not be free on the TPU, so we want to minimize them to # help the optimizer. # 将高维张量保留最后一个维度的信息，降维至二维。 prev_output = reshape_to_matrix(input_tensor) # 建立所有隐藏层输出的列表 all_layer_outputs = [] # 在所有hidden_layer中遍历，layer_idx表示层序号 for layer_idx in range(num_hidden_layers): with tf.variable_scope(&quot;layer_%d&quot; % layer_idx): # 将prev_output输入到该隐藏层中 layer_input = prev_output with tf.variable_scope(&quot;attention&quot;): # 建立attention_heads列表 attention_heads = [] with tf.variable_scope(&quot;self&quot;): # 将word_embedding经过attention_layer输出为context_layer attention_head = attention_layer( from_tensor=layer_input, to_tensor=layer_input, attention_mask=attention_mask, num_attention_heads=num_attention_heads, size_per_head=attention_head_size, attention_probs_dropout_prob=attention_probs_dropout_prob, initializer_range=initializer_range, do_return_2d_tensor=True, batch_size=batch_size, from_seq_length=seq_length, to_seq_length=seq_length) # 将该层的attention_head附到attention_heads列表中 attention_heads.append(attention_head) # 将attention_output清空 attention_output = None # *每次循环都清空attention_heads，然后再append一次attention_head，长度必然为1啊？？？ if len(attention_heads) == 1: # 输出attention_heads[0] attention_output = attention_heads[0] else: # In the case where we have other sequences, we just concatenate # them to the self-attention head before the projection. # *没明白他这里的in case如何触发 # 似乎如果是multi-head，attention_heads.append(attention_head)会将每个head进行append，这样len(attention_heads)就会大于1，但尚未验证。 # 对于multi-head的情况，将所有head进行concatenation。 attention_output = tf.concat(attention_heads, axis=-1) # Run a linear projection of `hidden_size` then add a residual # with `layer_input`. # 将attention_output经过一个全连接层输出一个hidden_size大小的向量， # 再加上原始值进行normalization。 with tf.variable_scope(&quot;output&quot;): # 全连接层将attention_output（不定长，取决于head个数）map到hidden_size大小 # 该操作仅用于mapping，没有激活函数。 attention_output = tf.layers.dense( attention_output, hidden_size, kernel_initializer=create_initializer(initializer_range)) # 对attention_output进行dropout attention_output = dropout(attention_output, hidden_dropout_prob) # 加上传入的word_embedding并进行normalization attention_output = layer_norm(attention_output + layer_input) # The activation is only applied to the &quot;intermediate&quot; hidden layer. # 将attention的输出再传入一个中间层，进一步消化，有激活函数。 with tf.variable_scope(&quot;intermediate&quot;): # 中间层 intermediate_output = tf.layers.dense( attention_output, intermediate_size, activation=intermediate_act_fn, kernel_initializer=create_initializer(initializer_range)) # Down-project back to `hidden_size` then add the residual. # 将高维的中间层输出向量map回hidden_size # Typically，intermediate_size = hidden_size * 4 with tf.variable_scope(&quot;output&quot;): # 全连接层将中间层输出map回hidden_size，无激活函数。 layer_output = tf.layers.dense( intermediate_output, hidden_size, kernel_initializer=create_initializer(initializer_range)) # 将该层的输出进行dropout layer_output = dropout(layer_output, hidden_dropout_prob) # 将该层的输出进行Add &amp; Normalize layer_output = layer_norm(layer_output + attention_output) # 将prev_output赋值为该层的输出，为下一层循环做准备。 prev_output = layer_output # 在all_layer_outputs附上本层的输出 all_layer_outputs.append(layer_output) # 如果返回全部encoder层的输出 if do_return_all_layers: # 建立最终输出列表 final_outputs = [] # 将每层的输出由2d重新调整回高维张量 for layer_output in all_layer_outputs: final_output = reshape_from_matrix(layer_output, input_shape) # 将每层的调整后的张量逐层append到最终输出中 final_outputs.append(final_output) # 返回最终的输出 return final_outputs # 若不返回所有层的输出 else: # 则直接将存在prev_output中的最后一层赋给最终输出 final_output = reshape_from_matrix(prev_output, input_shape) # 返回最终输出 return final_output 该函数用于建立模型中的Transformer部分，用到了上面建立的attention_layer函数帮助建立Transformer。关于Transformer，具体原理可参考 如何理解 Transformer 。 get_shape_listdef get_shape_list(tensor, expected_rank=None, name=None): &quot;&quot;&quot;Returns a list of the shape of tensor, preferring static dimensions. Args: tensor: A tf.Tensor object to find the shape of. expected_rank: (optional) int. The expected rank of `tensor`. If this is specified and the `tensor` has a different rank, and exception will be thrown. name: Optional name of the tensor for the error message. Returns: A list of dimensions of the shape of tensor. All static dimensions will be returned as python integers, and dynamic dimensions will be returned as tf.Tensor scalars. &quot;&quot;&quot; # 若name为空 if name is None: # 则将tensor的名字赋给name name = tensor.name # 若期待的张量维度不为空 if expected_rank is not None: # 则使用assert_rank判断张量维度是否与期待一致 assert_rank(tensor, expected_rank, name) # 获取张量的形状列表（静态） shape = tensor.shape.as_list() # 建立一个存储序号的列表 non_static_indexes = [] # 循环遍历shape列表 for (index, dim) in enumerate(shape): # 若tensor的静态维度大小为空， if dim is None: # 则记在non_static_indexes列表中，指示第几个维度为动态维度大小。 non_static_indexes.append(index) # 若不存在动态维度大小， if not non_static_indexes: # 则直接返回张量形状。 return shape # 若存在动态维度大小， # 则使用tf.shape()获得动态形状 dyn_shape = tf.shape(tensor) # 循环遍历non_static_indexes列表。 for index in non_static_indexes: # 将对应的动态维度大小赋给形状的相应维度 shape[index] = dyn_shape[index] # 返回张量的形状 return shape 该函数用于获取张量的shape，判断其维度是否满足要求。函数将返回一个列表描述每个维度的大小。参考博文 shape相关函数 。 reshape_to_matrixdef reshape_to_matrix(input_tensor): &quot;&quot;&quot;Reshapes a &gt;= rank 2 tensor to a rank 2 tensor (i.e., a matrix).&quot;&quot;&quot; # 维度赋值 ndims = input_tensor.shape.ndims # 若维度小于2， if ndims &lt; 2: # 则报错。 raise ValueError(&quot;Input tensor must have at least rank 2. Shape = %s&quot; % (input_tensor.shape)) # 若维度等于2， if ndims == 2: # 则直接返回张量。 return input_tensor # 若维度大于2 # 获取张量中最小向量的长度，即最后一个维度的大小 width = input_tensor.shape[-1] # 将张量转换为行矩阵。 # 保留张量最后一个维度的信息，其他维度信息按顺序扁平化。 # [ [ [1, 2], [ [1, 2], # [2, 3] ], ---\\ [2, 3], # [ [3, 4], ---/ [3, 4], # [4, 5] ] ] [4, 5] ] # [2, 2, 2] ---&gt; [4, 2] output_tensor = tf.reshape(input_tensor, [-1, width]) # 返回输出张量 return output_tensor 该函数用于将高维张量在保留最后一个维度信息的情况下，扁平化其他维度实现降维。 reshape_from_matrixdef reshape_from_matrix(output_tensor, orig_shape_list): &quot;&quot;&quot;Reshapes a rank 2 tensor back to its original rank &gt;= 2 tensor.&quot;&quot;&quot; # 若原始shape_list的长度为2，即张量的原始维度为2， if len(orig_shape_list) == 2: # 则直接返回张量 return output_tensor # 若原始shape_list的长度超过2 # （小于的情况不可能，因为该函数用于将降维的张量还原） # 获取2d张量形状 output_shape = get_shape_list(output_tensor) # 获取原始张量的[0:-1]维度的大小 orig_dims = orig_shape_list[0:-1] # 获取2d张量的最后一维大小 width = output_shape[-1] # 返回reshape后还原为原始的张量 return tf.reshape(output_tensor, orig_dims + [width]) 该函数用于将前面reshape_to_matrix(input_tensor)后降维至2d的张量还原至原始的高维形状。 assert_rankdef assert_rank(tensor, expected_rank, name=None): &quot;&quot;&quot;Raises an exception if the tensor rank is not of the expected rank. Args: tensor: A tf.Tensor to check the rank of. expected_rank: Python integer or list of integers, expected rank. name: Optional name of the tensor for the error message. Raises: ValueError: If the expected shape doesn&#39;t match the actual shape. &quot;&quot;&quot; # 若name为空， if name is None: # 则赋值为张量的name。 name = tensor.name # 建立期望维度数的字典 expected_rank_dict = {} # 建立{expected_rank0: True, expected_rank1: True}的字典文件 if isinstance(expected_rank, six.integer_types): expected_rank_dict[expected_rank] = True else: for x in expected_rank: expected_rank_dict[x] = True # 获取张量的实际维度数 actual_rank = tensor.shape.ndims # 若实际维度不在期待维度数字典中， if actual_rank not in expected_rank_dict: scope_name = tf.get_variable_scope().name # 则报错。 raise ValueError( &quot;For the tensor `%s` in scope `%s`, the actual rank &quot; &quot;`%d` (shape = %s) is not equal to the expected rank `%s`&quot; % (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank))) 该函数用于判断张量维度是否符合预期，不符合预期则会报错提示。 后记modeling.py的官方代码给出了987行，我从2020.08.29陆陆续续看了小一个礼拜到今天2020.09.03才完成。 这段代码详细描述了BERT模型主体功能的代码实际实现，加深了我对于BERT模型、Tranformer建模的理解。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"python","slug":"python","permalink":"/tags/python/"},{"name":"NLP","slug":"NLP","permalink":"/tags/NLP/"},{"name":"BERT","slug":"BERT","permalink":"/tags/BERT/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 6.1 正式表达（である、ではない）","slug":"日语语法 6.1 正式表达（である、ではない）","date":"2020-09-04T14:17:36.000Z","updated":"2020-09-04T14:17:36.000Z","comments":true,"path":"2020/09/04/日语语法 6.1 正式表达（である、ではない）/","link":"","permalink":"/2020/09/04/日语语法 6.1 正式表达（である、ではない）/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 6 高级话题6.1 正式表达（である、ではない）6.1.1 用「である」作为正式的状态表达用法公式： です/でございます + である用于正式场合的状态表达，语气上比较正式、官方、具有权威。 示例： 混合物(こんごうぶつ, mixture) とは、2種類以上の純物質が混じりあっている物質である。混合物是两种及以上纯物质的混合。 示例： A：混合物は 何？B：混合物は、2 種類以上の純物質が混じりあっている物質 だ。 A：混合物は 何ですか？B：混合物は、2 種類以上の純物質が混じりあっている物質 です。 A：混合物は 何でしょうか。B：混合物は、2 種類以上の純物質が混じりあっている物質 でございます。 A：混合物 とは？B：混合物は、2 種類以上の純物質が混じりあっている物質 である。 6.1.2 「である」的活用用法公式：（「である」的活用） 肯定 否定 学生である 是学生 学生ではない 不是学生 学生であった 曾是学生 ではなかった 曾不是学生 示例： それは不公平ではないでしょうか。这不公平，对吗？ 言語は簡単にマスターできることではない。语言不是能够容易掌握的东西。 6.1.3 在正式场合用词根代替动词て形示例： 花火（はなび）は、火薬と金属の粉末を混ぜたものに火を付け、燃焼時の火花を楽しむためのもの。（Wikipedia - 花火, August 2004）焰火是，把混合火药和金属用火点燃后，用来欣赏的燃烧时的火花。 企業内の顧客データを利用し、彼の行方を調べることが出来た。用一下企业内部的客户数据，就能知道查明他的行踪。 使用「ており」代替「〜ている」，表示一直进行的状态。 示例： 封筒には写真が数枚入っており、手紙が添えられていた。信封里放了几照片，被附了一封信。 このファイルにはパスワードが設定されており、開く際にはそれを入力する必要がある。这个文件正被设定着一个密码，打开的时候需要输入。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 6.6 趋势（〜がち、〜つつ、きらいがある）","slug":"日语语法 6.6 趋势（〜がち、〜つつ、きらいがある）","date":"2020-09-04T14:15:29.000Z","updated":"2021-11-26T18:04:44.804Z","comments":true,"path":"2020/09/04/日语语法 6.6 趋势（〜がち、〜つつ、きらいがある）/","link":"","permalink":"/2020/09/04/日语语法 6.6 趋势（〜がち、〜つつ、きらいがある）/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 6 高级话题6.6 趋势（〜がち、〜つつ、きらいがある）6.6.1 用「〜がち」来表示某动作/某事物倾向于要发生用法公式： （动词）动词词根 + がち （名词）名词 + がち/动词词根/名词/与「〜がち」连用表示该/动作/名词/倾向于发生。连用后，显名词/な形容词性。 「がち」的活用 肯定 否定 非过去 なりがち 倾向于变为 なりがちじゃない 不倾向于变为 过去 なりがちだった 曾倾向于变为 なりがちじゃなかった 不曾倾向于变为 示例： 確定申告は忘れがちな手続のひとつだ。纳税申告是倾向于忘记的手续之一。 留守がちなご家庭には、犬よりも、猫の方がおすすめです。对于经常没人在家的家庭来说，比起狗，更建议养猫。 父親は病気がちで、みんなが心配している。父亲像是要生病了，大家都很担心。 6.6.2 用「〜つつ」描述正在发生的事情用法公式： 动词词根 + つつ动词词根与「つつ」连用表示该动作正在进行。 动词词根 + つつ + ある动词词根与「つつある」连用表示该动作有发生的趋势。* 该用法通常会伴随着另一个动作的进行，或者是时间的进行。 示例： 二日酔いで痛む頭を押さえつつ、トイレに入った。边带着宿醉之后的头痛，边进了洗手间。 体によくないと思いつつ、最近は全然運動してない。一边觉得这样对身体不好，一边最近还是完全没运动。 電気製品の発展につれて、ハードディスクの容量はますます大きくなりつつある。伴随着电子产品的发展，硬盘的容量正不断扩大。 今の日本では、終身雇用や年功序列という雇用慣行が崩れつつある。在今日的日本，终身雇佣制以及论资排辈这样的雇佣惯例正在趋于解体中。 6.6.3 用「きらいがある」表示负面的趋势用法公式： 句子 + きらいがある句子与「きらいがある」连用，表示前面句子的趋势是不好的/负面的。 名词 + の + きらい + がある名词与「のきらいがある」连用，表示该名词发生的趋势是不好的/负面的。此处用法中的「きらい」显名词性。 示例： 多くの大学生は、締切日ぎりぎりまで、宿題をやらないきらいがある。很多大学生有在将将到截止日期还不做作业的坏习惯。 コーディングが好きな開発者は、ちゃんとしたドキュメント作成と十分なテストを怠るきらいがある。喜欢写代码的开发者有逃避制作文档以及完备测试的坏习惯。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 6.5 不可行的正式表达","slug":"日语语法 6.5 不可行的正式表达","date":"2020-09-04T14:15:07.000Z","updated":"2021-11-26T18:04:17.621Z","comments":true,"path":"2020/09/04/日语语法 6.5 不可行的正式表达/","link":"","permalink":"/2020/09/04/日语语法 6.5 不可行的正式表达/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 6 高级话题6.5 不可行的正式表达6.5.1 用「〜ざるを得ない」表达「不得不」用法公式： 动词否定式去掉ない + ざる + を + 得ない双重否定表示不得不进行该动作。 （例外）する → せざる → せざるをえない （例外）くる → こざる → こざるをえない 示例： このテレビがこれ以上壊れたら、新しいのを買わざるを得ないな。如果电视再坏了的话，就不得不买新的了呀。 ずっと我慢してきたが、この状態だと歯医者さんに行かざるを得ない。尽管一直在忍，不过现在这情况不得不去看牙医了。 上司の話を聞くと、どうしても海外に出張をせざるを得ないようです。跟老板谈完之后，看样子我是不得不去国外出差了。 6.5.2 用「やむを得ない」表示停不下来某事用法公式： やむを得ない + 名词用「やむを得ない」作为从句修饰该名词，表示该名词的发生进行不可被停止。 示例： やむを得ない事由により手続が遅れた場合、必ずご連絡下さい。因为不可控因素迟办手续的情况下，请务必联系我们。 この仕事は厳しいかもしれませんが、最近の不景気では新しい仕事が見つからないのでやむを得ない状態です。这份工作也许不好，但因为经济衰退找不到别的工作，我也无能为力。 6.5.3 用「〜かねる」描述做不成的事用法公式： 动词词根 + かねる动词词根与「かねる」连用，表示该动作不能完成。 动词词根 + かね + ない动词词根与「かねる」的否定式连用，表示该动作可能发生（一般为不好的事情）。* 「かねる」是个る动词，满足る动词的活用规则。 示例： この場ではちょっと決めかねますので、また別途会議を設けましょう。既然在这里有点没法决定，那就再安排另一次会议吧。 このままでは、個人情報が漏洩しかねないので、速やかに対応をお願い致します。因为照这样下去可能个人信息就会泄漏了，我请求及时应对这件事。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 6.3 表达最低期望（でさえ、ですら、おろか）","slug":"日语语法 6.3 表达最低期望（でさえ、ですら、おろか）","date":"2020-09-04T14:14:21.000Z","updated":"2021-11-26T18:01:50.698Z","comments":true,"path":"2020/09/04/日语语法 6.3 表达最低期望（でさえ、ですら、おろか）/","link":"","permalink":"/2020/09/04/日语语法 6.3 表达最低期望（でさえ、ですら、おろか）/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 6 高级话题6.3 表达最低期望（でさえ、ですら、おろか）6.3.1 用「（で）さえ」表达最低需求用法公式： （名词）名词 + さえ/でさえ/すら/ですら （动词）动词词根+さえ/动词て形 + さえ用于表达达成某件事情的最低要求。すら/ですら通常只与名词连用，较少出现。 示例： 宿題が多すぎて、トイレに行く時間さえなかった。作业那么多，我连上厕所的时间都没有了。 お金さえあれば、何でも出来るよ。只要有了钱，什么都能办。 お弁当を買うお金さえなかった。我连买便当的钱都没有。 动词词根与「さえ」连用显名词性质（详见 3.5.3 动词的分类附），可以直接跟「する」表示完成/未完成。 示例： ビタミンを食べさえすれば、健康が保証されますよ。只吃维他命的话，健康可以被保证哦。 自分の過【あやま】ちを認めさえしなければ、問題は解決しないよ。如果你连自己的错误都不能承认，问题没法解决哦。 教科書をもっとちゃんと読んでさえいれば、合格できたのに。明明只要是再认真一点看书就能及格了。 一言言ってさえくれればこんなことにならなかった。只要是再说点什么事情也不会变成这样。 6.3.2 用「おろか」表示不值得思考用法公式： 名词 + は + おろか、句子。表示前面的名词不值得考虑。 名词/形容词/动词 + どころか、句子。表示前面的/名词/形容词/动作/不值得考虑。 示例： 漢字は おろか、ひらがな さえ 読めないよ！别提汉字了，我连平假名都不会读！ 結婚はおろか、2ヶ月付き合って、結局別れてしまった。别提结婚了，没想到我们交往两个月就分手了。 大学は おろか、高校 すら 卒業しなかった。别提大学了，我连中学都没毕业。 漢字 どころか、ひらがな さえ 読めないよ！别提汉字了，我连平假名都不会哦！","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 6.2 事情本应该是这样（はず、べき、べく、べからず）","slug":"日语语法 6.2 事情本应该是这样（はず、べき、べく、べからず）","date":"2020-09-04T14:13:46.000Z","updated":"2021-11-26T18:01:08.125Z","comments":true,"path":"2020/09/04/日语语法 6.2 事情本应该是这样（はず、べき、べく、べからず）/","link":"","permalink":"/2020/09/04/日语语法 6.2 事情本应该是这样（はず、べき、べく、べからず）/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 6 高级话题6.2 事情本应该是这样（はず、べき、べく、べからず）6.2.1 用「はず」表达一种期待用法公式： 名词 + の + はず な形容词 + な + はず い形容词 + はず 动词辞书形 + はず （否定）はず + がない连用后表示本应该是怎样，表达一种自己对事物的期待，显名词性。 示例： 彼は漫画マニア（Mania）だから、これらをもう全部読んだはずだよ。他是漫画狂人，所以我觉得这些他应该都看过了。 この料理はおいしいはずだったが、焦げちゃって、まずくなった。这料理本应该很好吃，但不小心被煮焦了，所以变得很难吃。 色々予定してあるから、今年は楽しいクリスマスのはず。因为很多事情都被计划好了，所以我觉得今年的圣诞节会很欢乐。 そう簡単に直せるはずがないよ。不应该那么简单就能修好。 打合せは毎週２時から始まるはずじゃないですか？会议不应该是每周两点开始吗？ 6.2.2 用「べき」来描述应该做什么用法公式： 动词辞书形 + べき动词辞书形与「べき」连用表示应该怎么做。* 作者原文：In Japanese, you might define it as meaning 「絶対ではないが、強く推奨されている」.（不绝对但是强烈推荐） 示例： 何かを買う前に本当に必要かどうかをよく考えるべきだ。买东西之前应该想好是否确实有必要买。* 「どうか」表示是否。 例え国のためであっても、国民を騙【だま】すべきではないと思う。即使是有，比如国家的目的，我也觉得国民不应该被欺骗。 預金者が大手銀行を相手取って訴訟を起こすケースも出ており、金融庁は被害者の救済を優先させて、金融機関に犯罪防止対策の強化を促すべきだと判断。（朝日新聞）正出现储户起诉大银行的案件，金融厅判断应该让被害者的救济优先，督促金融机构强化防止犯罪的对策。 6.2.3 用「べく」来描述尝试做什么用法公式： 动词辞书形 + べく连用表示为了尝试该动作。 示例： 試験に合格すべく、皆一生懸命に勉強している。为了考试合格，大家都拼尽了全力学习。 今後もお客様との対話の窓口として、より充実していくべく努力してまいります。今后也会作为和客人沟通的窗口，为了更加充实而努力。 6.2.4 用「べからず」描述不能做的事用法公式： 动词辞书形 + べからず连用表示对于该动作的禁止。 示例： ゴミ捨【す】てるべからず。你不能扔掉垃圾。 安全措置を忘れるべからず。你不能忘记安全措施。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"opencv添加水印 C++实现","slug":"opencv添加水印 C++实现","date":"2020-08-07T09:46:18.000Z","updated":"2020-08-07T09:46:18.000Z","comments":true,"path":"2020/08/07/opencv添加水印 C++实现/","link":"","permalink":"/2020/08/07/opencv添加水印 C++实现/","excerpt":"","text":"1 opencv添加水印 C++实现1.1 动机动机是想给自己画的画加个水印，虽然p图软件可以轻松做到但是感觉自定义程度并不高，这种复杂度的C++我应该还可以胜任。 1.2 直接加权的水印要输入的水印图片是一张透明的png格式图片。Yuk1n0.png： #include &lt;opencv2/opencv.hpp&gt; //opencv头文件 #include &lt;iostream&gt; //C++基本输入输出 //名空间声明 using namespace cv; using namespace std; int main() { Mat image = imread(&quot;E:\\\\Input.png&quot;); Mat logo = imread(&quot;E:\\\\Yuk1n0.png&quot;); Mat imageROI; imageROI = image(Rect(1700, 2139, logo.cols, logo.rows)); //Rect(x, y, delta_x, delta_y)表示图像变量image的以(x, y)，(x + delta_x, y)，(x, y + delta_y)，(x + delta_x, y + delta_y)围成的区域 addWeighted(imageROI, 0.2, logo, 0.8, 0, imageROI); //对该区域加权赋给imageROI，由于opencv中Mat的本质是指针，故imageROI会直接反映在image的相应位置 waitKey(); imwrite(&quot;E:\\\\Example.png&quot;, image); return 0; } Example.png： 但是由于图片的透明部分经过imread读进来之后就是黑色，因此我们要想个办法去掉黑色的部分。 1.3 经过灰度图mask后的水印#include &lt;opencv2/opencv.hpp&gt; //opencv头文件 #include &lt;iostream&gt; //C++基本输入输出 //名空间声明 using namespace cv; using namespace std; int main() { Mat image = imread(&quot;E:\\\\Input.png&quot;); Mat logo = imread(&quot;E:\\\\Yuk1n0.png&quot;); Mat mask = imread(&quot;E:\\\\Yuk1n0.png&quot;, 0); //取灰度图 Mat imageROI; Mat imageROI_mask; logo = 255 - logo; //取反色为了不与底色混淆 image(Rect(1700, 2139, logo.cols, logo.rows)).copyTo(imageROI_mask); imageROI = image(Rect(1700, 2139, logo.cols, logo.rows)); logo.copyTo(imageROI, mask); //只有mask像素值大于0的位置，才进行copy操作 addWeighted(imageROI, 0.8, imageROI_mask, 0.2, 0, imageROI); //此处不直接将logo和imageROI加权的原因是会降低背景不透明度，将贴有水印的背景和未贴有水印的背景加权可以保持背景不透明度为100% waitKey(); imwrite(&quot;E:\\\\Output.png&quot;, image); return 0; } Output.png： 1.4 附录阳菜便服Ver. 出自bilibili版 天气之子 37:34 手残不会画，全靠兴趣。 初次尝试base64图片嵌入，试试效果如何。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"opencv","slug":"opencv","permalink":"/tags/opencv/"},{"name":"C++","slug":"C","permalink":"/tags/C/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 5.13 保持现状（まま、っぱなし）","slug":"日语语法 5.13 保持现状（まま、っぱなし）","date":"2020-07-21T14:40:18.000Z","updated":"2020-07-21T14:40:18.000Z","comments":true,"path":"2020/07/21/日语语法 5.13 保持现状（まま、っぱなし）/","link":"","permalink":"/2020/07/21/日语语法 5.13 保持现状（まま、っぱなし）/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 5 特殊表达5.13 保持现状（まま、っぱなし）5.13.1 用「まま」表示缺少变化用法公式： 状态 + まま + で表示保持这个状态的状态。「まま」显名词性。 示例： このままで宜しいですか？像这样就可以吗？ 半分しか食べてないままで捨てちゃダメ！你不能就这样吃一半就扔掉！ 今日だけは悲しいままでいさせてほしい。只有今天，希望你就让我自己这样伤心吧。 その格好のままでクラブに入れないよ。不能穿成那样就去俱乐部哦。 5.13.2 用「っぱなし」让某物保持不变用法公式： 动词词根 + っぱなし动词词根与「っぱなし」连用表示保持该动作。 示例： テレビをつけっぱなしにしなければ眠れない人は、結構いる。不一直开着电视就不能睡着的人，相当多呢。 窓が開けっ放しだったので、蚊がいっぱい入ってしまった。窗户就这么一直开着，很多蚊子不注意飞了进来。 2020.02.17-2020.07.21 日语语法中译本p23-p251 还差最后一章啦哈哈哈看到希望了！","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 5.12 表达特定时间动作","slug":"日语语法 5.12 表达特定时间动作","date":"2020-07-21T14:39:46.000Z","updated":"2020-07-21T14:39:46.000Z","comments":true,"path":"2020/07/21/日语语法 5.12 表达特定时间动作/","link":"","permalink":"/2020/07/21/日语语法 5.12 表达特定时间动作/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 5 特殊表达5.12 表达特定时间动作5.12.1 用「〜ばかり」表示刚刚发生的动作用法公式： 动词过去式 + ばかり/ばっか（口语）动词过去式与「〜ばかり」连用表示刚刚做了该动作，「〜ばっか」可用于口语。 「〜ばかり」活用规则同名词 肯定 否定 食べたばかり（だ） 刚吃 食べたばかりじゃない 刚没吃 示例： すみません、今食べたばかりなので、お腹がいっぱいです。不好意思，因为我刚吃了，肚子饱了。 10キロを走ったばかりで、凄【すご】く疲れた。我刚跑了十公里，很累了。 今、家に帰ったばかりです。我刚到家。 示例： 昼ご飯を食べたばっかなのに、もうお腹が空いた。明明刚吃了午饭，就已经饿了。 まさか、今起きたばっかなの？不会吧，你刚起吗？ 5.12.2 用「とたん」表示紧接着发生的事情（外因）用法公式： 动词过去式 + とたん + （に）动词过去式与「とたん」连用表示紧接着该动作发生的事情，「に」用作指示该时间点。* 需要注意的是，此处紧接着发生的动作应由外因引起，而非自己。 示例： 窓を開けたとたんに、猫が跳んでいった。窗户一开，猫就跳了出去。 映画を観たとたんに、眠くなりました。一看电影就变困了。 5.12.3 用「ながら」表示同时发生的动作用法公式： 动词词根/动词否定式 + ながら/动词词根/动词否定式/与「ながら」连用表示后面即将发生的动作与前面的动作同时。 示例： テレビを観ながら、宿題をする。边看电视，边写作业。 音楽を聴きながら、学校へ歩くのが好き。喜欢边听音乐边走去学校。 相手に何も言わないながら、自分の気持ちをわかってほしいのは単なるわがままだと思わない？一边对别人什么都不说，一边又希望别人理解自己的想法，不觉得这就是单纯的自私吗？ 由于两个动作同时发生，故「ながら」没有时态，句子的事态取决于另一个具有时态的动词。 示例： ポップコーンを食べながら、映画を観る。边吃爆米花，边看电影。 ポップコーンを食べながら、映画を観た。边吃爆米花，边看电影。（过去） 口笛をしながら、手紙を書いていた。边吹口哨，边正在写信。（过去） 5.12.4 「ながら」表示状态用法公式： 状态 + ながら表示事物的状态，一般用作补充语。 状态 + ながらも表示即便处于该状态。 示例： 仕事がいっぱい入って、残念ながら、今日は行けなくなりました。来了很多工作，很不幸，变得今天没法去了。 貧乏ながらも、高級なバッグを買っちゃったよ。虽然贫穷，但还是不小心买了一个高档包。 彼は、初心者ながらも、実力はプロと同じだ。虽然他是只个初学者，但实力跟专家一样。 5.12.5 用「まくる」表示放弃一切地重复某件事用法公式： 动词词根 + まくる动词词根与「まくる」表示近乎疯狂的一直在进行该动作。* 因为是一个一直进行的动作，所以常用其进行时及进行时地活用进行表达。 「まくる」的进行时及其活用 肯定 否定 非过去 やりまくっている 一直在做 やりまくっていない 没有一直在做 过去 やりまくっていた 曾经一直在做 やりまくっていなかった 没有一直在做过 示例： ゲームにはまっちゃって、最近パソコンを使いまくっているよ。不小心陷入了游戏中，最近除了用电脑啥也没干。 アメリカにいた時はコーラを飲みまくっていた。在美国的时候总是喝可乐。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 5.11 假设和总结（わけ、〜とする）","slug":"日语语法 5.11 假设和总结（わけ、〜とする）","date":"2020-07-21T14:39:22.000Z","updated":"2020-07-21T14:39:22.000Z","comments":true,"path":"2020/07/21/日语语法 5.11 假设和总结（わけ、〜とする）/","link":"","permalink":"/2020/07/21/日语语法 5.11 假设和总结（わけ、〜とする）/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 5 特殊表达5.11 假设和总结（わけ、〜とする）5.11.1 用「わけ」得出结论示例： 直子：いくら英語を勉強しても、うまくならないの。直子：不管我怎么学习英语，就是学不好啊。ジム：つまり、語学には、能力がないという訳【わけ】か。Jim：简而言之，这意味着你的语言能力不行。直子：失礼ね。直子：好无礼呐。 示例： 中国語が読める わけ がない。没理由读懂中文。 直子：広子の家に行ったことある？直子：你有去过广子家吗？一郎：ある わけない でしょう。一郎：我没理由去过她家的，对吧？ 直子：微積分は分かる？直子：你会微积分吗？一郎：分かる わけない よ！一郎：我怎么可能懂！ 示例： ここの試験に合格するのは わけない。这里的考试及格是没有理由的。（无需解释，表示简单） 示例： 今度は負ける わけ には いかない。这一次我没理由再输了。 ここまできて、あきらめる わけ には いかない。到了这个地步，我没理由再放弃了。 5.11.2 用 「とする」做出假设示例： 明日に行くとする。假设我们明天去。 今から行くとしたら、９時に着くと思います。如果我们现在走，我想会在九点到达。 示例： 観客として参加させてもらった。受人帮忙以观众的身份参加。 被害者としては、非常に幸いだった。作为受害人，曾经十分幸运。 朝ご飯を食べたとしても、もう昼だからお腹が空いたでしょう。尽管觉得应该吃过早饭了，但因为现在已经中午了，你应该还是饿了对吧？","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"惊奇发现Latex嵌入文本的新用法哈哈哈","slug":"惊奇发现Latex嵌入文本的新用法哈哈哈","date":"2020-07-13T16:11:14.000Z","updated":"2020-07-13T16:11:14.000Z","comments":true,"path":"2020/07/14/惊奇发现Latex嵌入文本的新用法哈哈哈/","link":"","permalink":"/2020/07/14/惊奇发现Latex嵌入文本的新用法哈哈哈/","excerpt":"","text":"惊奇发现Latex嵌入文本的新用法哈哈哈完整公式的嵌入在进行 文献阅读 TINYBERT DISTILLING BERT FOR NATURAL LANGUAGE UNDERSTANDING 这篇文章的校对时，我发现公式是可以嵌入Markdown语法的。 Markdown的Raw文本： 1. 欧拉公式： \\begin{equation} e^{i\\pi} + 1 = 0 \\end{equation} 实际效果： 欧拉公式：\\begin{equation}e^{i\\pi} + 1 = 0\\end{equation} 但之前我没发现，因为我怕不换行不触发Latex语法。 以前Markdown的Raw文本： 1. 欧拉公式： \\begin{equation} e^{i\\pi} + 1 = 0 \\end{equation} 实际效果： 欧拉公式： \\begin{equation}e^{i\\pi} + 1 = 0\\end{equation} 不换行的公式嵌入经过了上个实验之后我发现Latex是可以和Markdown同生共死的，那么能不能把公式植入文本中呢？毕竟直接写矩阵H^S实在是太丑了。 于是有了如下结果。 Markdown的Raw文本： 1. 欧拉 $ e^{i\\pi} + 1 = 0 $ 公式： \\begin{equation} e^{i\\pi} + 1 = 0 \\end{equation} 实际效果： 欧拉 $ e^{i\\pi} + 1 = 0 $ 公式：\\begin{equation}e^{i\\pi} + 1 = 0\\end{equation} 这样我就可以愉快地在文本中加一些优美的符号了，非常快乐！ 关于转义由于转义符号\\在Latex当中有作用，所以当我们想在Latex中转义时要双写。 Markdown的Raw文本： 1. raw_input： \\begin{equation} raw\\\\_input \\end{equation} 实际效果： raw_input：\\begin{equation}raw\\_input\\end{equation} 如果单写的话不行。 单写反斜杠Markdown的Raw文本： 1. raw_input： \\begin{equation} raw\\_input \\end{equation} 实际效果： raw_input：\\begin{equation}raw_input\\end{equation} 后记虽然看上去是关于Markdown和Latex语法，但是算是生活小实验吧，就归到生活了。其实是因为生活分类一直没水文章有点强迫症。","categories":[{"name":"生活","slug":"生活","permalink":"/categories/生活/"}],"tags":[{"name":"Latex","slug":"Latex","permalink":"/tags/Latex/"}],"keywords":[{"name":"生活","slug":"生活","permalink":"/categories/生活/"}]},{"title":"日语语法 5.10 更多的否定动词","slug":"日语语法 5.10 更多的否定动词","date":"2020-07-10T14:24:14.000Z","updated":"2020-07-10T14:24:14.000Z","comments":true,"path":"2020/07/10/日语语法 5.10 更多的否定动词/","link":"","permalink":"/2020/07/10/日语语法 5.10 更多的否定动词/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 5 特殊表达5.10 更多的否定动词动词ない形详见 3.6 动词的否定式 。 5.10.1 没做一件事就做了另一件事用法公式： 动词1ない形 + で + 动词2动词否定式与「で」合用，表示还没做动词1就去做了动词2。个人感觉，这个搭配想表达，动词1是常识上会和动词2相伴发生的动作，而没有发生。 示例： 何も食べないで寝ました。没吃任何东西就去睡觉了。 歯を磨かないで、学校に行っちゃいました。不小心没刷牙就去学校了。 宿題をしないで、授業に行くのは、やめた方がいいよ。最好不要不做作业就去听课。 先生と相談しないで、この授業を取ることは出来ない。不能没跟老师谈过就上这课。 用法公式： 动词1ない形（去掉结尾ない） + ず + 动词2表示还没做动词1就去做了动词2，该用法相比上文的用法更加正式。 （例外）する→せず （例外）くる→こず 示例： 彼は何も言わず、帰ってしまった。他好像什么也没说就回家了。 何も食べずにそんなにお酒を飲むと当然酔っ払いますよ。你要是什么也不吃就喝那么多酒，当然会醉。 勉強せずに東大に入れると思わないな。我觉得你不可能不学习就进东大啊。 5.10.2 一种以「ん」结尾和一种以「ぬ」否定式用法用法公式： 动词ない形（去掉结尾ない） + ん用「ん」替换动词ない形结尾的ない，用于表示一种男性化的否定说法。 （例外）する→せん （例外）くる→こん 示例： すまん。对不起。 そんなことはさせん！我不会允许你做这样的事！ 用法公式： 动词ない形（去掉结尾ない） + ぬ用「ぬ」替换动词ない形结尾的ない，用于表示一种传统的否定说法，多用于老人。 （例外）する→せぬ （例外）くる→こぬ 示例： 模擬試験に何回も失敗して、実際に受けてみたら 思わぬ 結果が出た。模拟考失败多次之后参加真正的考试时，没想过的结果出现了。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 5.9 表达事情的难易","slug":"日语语法 5.9 表达事情的难易","date":"2020-07-10T14:23:32.000Z","updated":"2020-07-10T14:23:32.000Z","comments":true,"path":"2020/07/10/日语语法 5.9 表达事情的难易/","link":"","permalink":"/2020/07/10/日语语法 5.9 表达事情的难易/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 5 特殊表达5.9 表达事情的难易5.9.1 用「〜やすい、〜にくい」来形容动作的难易用法公式： 动词词根 + やすい动词词根与「〜やすい」连用表示动作的容易，显い形容词性。 动词词根 + にくい动词词根与「〜にくい」连用表示动作的困难，显い形容词性。* 词根活用详见 4.1.2 动词词根 。 肯定形 否定式 非过去形 食べにくい 食べにくくない 过去形 食べにくかった 食べにくくなかった 示例： この字は読みにくい这个字迹难读懂。 カクテルはビールより飲みやすい。鸡尾酒比啤酒容易下口。 部屋が暗かったので、見にくかった。因为房间里暗，所以难看清。 示例： その肉は食べにくい。那肉难以吃。 その肉を食べるのは難しい。吃那块肉是困难的。 5.9.2 「〜にくい」的变形：「〜がたい」和「〜づらい」用法公式： 动词词根 + がたい/づらい动词词根与「がたい/づらい」连用表示动作的困难，显い形容词性。* 「にくい」的汉字写法其实来自「難い」，它也可以被读作「かたい」，用法上使用「〜がたい」。* 另一个更为随意的变种是「づらい」，它源自「辛い」【つらい】，译为艰辛。「辛い」的另一种读法【から・い】表示辣。 示例： 彼との忘れがたい思い出を大切にしている。重视和他一起的难以忘记的回忆。 とても信じがたい話だが、本当に起こったらしい。虽然这个故事很难让人信服，但是看样子（听闻）真的发生过。 日本語は読みづらいな。日语真难读啊。 待ち合わせは、分かりづらい場所にしないでね。请不要挑一个难理解的地点来安排会议。 我感觉作者又强行水了一节哈哈哈。。。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"如何理解 BERT","slug":"如何理解 BERT","date":"2020-07-10T13:02:26.000Z","updated":"2020-07-10T13:02:26.000Z","comments":true,"path":"2020/07/10/如何理解 BERT/","link":"","permalink":"/2020/07/10/如何理解 BERT/","excerpt":"","text":"本文参考[1] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding[2] The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)[3] 图解BERT模型：从零开始构建BERT 1 如何理解BERT1.1 什么是BERT 图1.1 BERT的Pre-training和Fine-Tuning[1] 图1.2 BERT的内部Embedding结构[1] 标题理解：BERT：Bidirectional Encoder Representations from Transformers Bidirectional：该模型的双向性体现在Masked LM部分。该部分通过Mask掉部分Token，再重新预测这些Token，实现模型对于上下文的学习。故称模型具有双向性。* 开始我以为BERT引入了类似Bi-LSTM的结构，但实际上这个双向性是通过Pre-training中的Masked LM获得的上下文理解。原文[1]：In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. Encoder Representation：该模型的本质是一个用来生成Representation的Encoder。 Transformer：该模型基于Transformer架构进行搭建。 符号理解： Token Embedding：经过Word2Vec或其他模型生成的Word Embedding。 Position Embedding：详见 如何理解 Transformer 1.2.1 Positional Encoding Segment Embedding：用于区分Token的不同句子所属。 Input Representation：最终的输入向量将会是Token Embedding + Position Embedding + Segment Embedding。 [CLS] Token：位于输入始端的向量，用于表征整个输入，可用于Classification任务。 [SEP] Token：位于句子结尾的向量，用于指示一个句子的结束，Separate两个句子。 1.2 模型架构 图1.3 BERT的模型架构[2] 如 图1.3 所示，在经过了外部对Token Embedding + Position Embedding + Segment Embedding相加得到Input Representation后，将其输入到N层Transformer的Encoder中得到输出结果。 BERT官方最开始Release了两个BERT版本： BERT_BASE(L=12, H=768, A=12, Total Parameters=110M) BERT_LARGE(L=24, H=1024, A=16, Total Parameters=340M 符号解释： L（Layer）：指Transformer的Encoder层数。 H（Hidden Size）：等同于输入输出以及中间变量的维度。实际指代前馈神经网络中的神经元数量，原文中将所有前馈神经网络的神经元数量都设置为4H。 A：指Transformer的Multi-Head Attention中Head的数量。 Total Parameters：模型中所有可训练参数数量之和。 1.3 Pre-training部分预训练部分主要阐述了模型如何合理运用Transformer进行自监督学习。个人认为该部分包含了BERT模型的核心思想。 1.3.1 Masked LM目的：使模型学会deep bidirectional representation。 表1.1 Token的Mask规则 15%的Token用于预测 - 85%的Token 80% 10% 10% - 替换为[MASK] 替换为随机Token 保持不变 - 保持不变 实现方法：（如 表1.1 所示） 首先，随机选取15%的Token用于Mask以及之后的预测。 由于Token的缺失，Pre-training（Token缺失）以及Fine-tuning（Token全部保留）将出现一个Mismatch的Gap。 为了削弱这个Gap，文章将所选的15%再随机分为80%、10%、10%三部分。其中，80%的Token用[MASK]Token替换；10%的Token用随机Token替换；另10%的Token保持不变。 1.3.2 Next Sentence Prediction (NSP)目的：使模型学会句子之间的关系。 表1.2 NSP的训练规则 50% - 50% B是A的下一句话 - B不是A的下一句话 实现方法：（如 表1.2 所示） 该任务使用句子对A和B进行训练。 50%的时间，B是A的下一句话，标注为IsNext。 50%的时间，B为文章中随机抽选的一句话，并非A的下一句话，标注为NotNext。 1.4 Fine-tuning部分该部分原文中并没有提及太多。我的理解是： 与Pre-training阶段不同，Fine-tuning阶段是监督学习阶段，需要模型在实际任务的数据集上进行进一步学习。 BERT的Pre-training为后面提供了一个接口，在Fine-tuning阶段，我们只需要调用这个强大的语言理解接口并对他的输入输出稍作修改就可以适应不同的语言学习任务。 BERT自带的[CLS]Token可以很好的表征整个输入向量，可以用于分类任务的输出代表整个文本。 在实际使用中，我发现很多论文在Fine-tuning之前会在已经Pre-training好的BERT模型上，使用任务数据集再进行Pre-training。这样相比直接在任务数据集上进行Pre-training，我认为既省时间（对于相同语言，大部分特征是相似的，从头开始预训练会消耗大量时间），效果又好（Google给出的预训练模型本身就使用了大量的数据集进行预训练，在此基础上再进行与训练可以采百家之长）。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"/tags/NLP/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 5.8 使用「方」和「よる」来实现比较和其他功能","slug":"日语语法 5.8 使用「方」和「よる」来实现比较和其他功能","date":"2020-07-10T12:39:34.000Z","updated":"2020-07-10T12:39:34.000Z","comments":true,"path":"2020/07/10/日语语法 5.8 使用「方」和「よる」来实现比较和其他功能/","link":"","permalink":"/2020/07/10/日语语法 5.8 使用「方」和「よる」来实现比较和其他功能/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 5 特殊表达5.8 使用「方」和「よる」来实现比较和其他功能5.8.1 用「方」来做比较名词「方」表示方向的时候读作「ほう」，但作为「人」的礼貌版本时它读作「かた」。语法上来说，它跟其他普通名词用法一样。 用法公式： 名词+の/从句 + 方 + が + 形容词表示/名词/从句/更怎么样。/名词/从句/与「方」连用的用法详见 3.10 关系从句和语序 。 示例： ご飯の方【ほう】がおいしい米饭更好吃。 鈴木さんの方【かた】が若い。铃木桑更年轻。 学生じゃない方がいいよ。不是学生更好。 赤ちゃんは、静かな方が好き。对于宝宝来说，更喜欢安静的。 ゆっくり食べた方が健康にいいよ。吃慢点对健康更好。 こちらから行った方が早かった。从这边走更快。 怖い映画は観ない方がいいよ。不看恐怖电影更好。* 否定式动词不能这么用，8. 中的否定式过去形态可以这样用。 そんなに飲まなかった方がよかった。别喝那么多更好。 5.8.2 用「より」来做比较用法公式： 名词/从句（动词辞书原形） + より表示比起/名词/从句/，其他的东西更怎么样。/名词/从句/与「より」连用的用法详见 3.10 关系从句和语序 。* 常与「方」的用法连用表示两个事物的比较。 示例： 花より団子。与其要花，不如要团子。（日本著名谚语） ご飯の方が、パンよりおいしい。米饭比面包好吃。 キムさんより鈴木さんの方が若い。比起Kim桑，鈴木桑更年轻。 鈴木：毎日仕事に行くのが嫌だ。铃木：我不喜欢每天上班。スミス：仕事がないよりましだよ。Smith：这总好过没有工作。 ゆっくり食べた方が早く 食べる よりいい。吃慢点比吃快点好。* 「より」前的动词保持原形。 用法公式： 疑问词（誰/何/どこ） + より + （も）疑问词与「より」合用表示主题的最高级。 示例： 商品の品質を何より大切にしています。要比任何事情更重视商品质量。（最重视） この仕事は誰よりも早くできます。对于这项工作能比任何人做的都快。（最快） 5.8.3 用「方」表示方法用法公式： 动词词根 + 方【かた】动词词根与「方」连用表示该动作的方法，显名词性。 示例： 新宿の行き方は分かりますか。知道去新宿的方法吗？ そういう食べ方は体によくないよ。那种吃法对身体不好。 漢字の書き方を教えてくれますか？能教我汉字的写法吗？ パソコンの使い方は、みんな知っているでしょう。大家都知道电脑的用法，对吧。 5.8.4 用「によって」来表示取决于用法公式： 名词 + によって名词与「によって」连用表示主题取决于该名词。 示例： 人によって話が違う。这个故事因人而异。 季節によって果物はおいしくなったり、まずくなったりする。水果好吃难吃取决于季节。 和子：今日は飲みに行こうか？和子：今天去喝酒吗？大樹：それは、裕子に よる ね。大树：这要看裕子了。* 「よって」是「よる」的て形。 5.8.5 用「によると」表示消息来源用法公式： 名词 + によると名词与「によって」连用表示源自该名词。 示例： 天気予報によると、今日は雨だそうだ。据天气预报说，今天会下雨。* 「だそうだ」用法见 5.7.4 用「〜そうだ」表示传闻 。 友達の話によると、朋子はやっとボーイフレンドを見つけたらしい。据朋友的说法，智子好像终于找到男朋友了。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 5.7 表达相似性和传闻的多种说法","slug":"日语语法 5.7 表达相似性和传闻的多种说法","date":"2020-07-10T12:39:26.000Z","updated":"2020-07-10T12:39:26.000Z","comments":true,"path":"2020/07/10/日语语法 5.7 表达相似性和传闻的多种说法/","link":"","permalink":"/2020/07/10/日语语法 5.7 表达相似性和传闻的多种说法/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 5 特殊表达5.7 表达相似性和传闻的多种说法5.7.1 用よう（様）表达相似性用法公式： 句子 + よう + だ/です/でございます句子与「よう」连用表示发生的事态和前面句子相似。 名词 + よう + だ/です/でございます名词与「よう」连用表示主题和该名词相似。 な形容词+な/い形容词 + よう + だ/です/でございます形容词与「よう」连用表示主题和该形容词相似。 * 1中的情况可以包括2、3中的情况，2、3是1中的细节补充。 示例： ここには、誰もいないようだ。看样子是这里没人。 映画を観たようです。看样子是看过这部电影。 学生のようだ。看样子是学生。 ここは静かなようだ。看样子是安静的。 用法公式： 句子 + よう + な + 名词句子与「ような」连用表示「像句子一样的」，显な形容词性。 句子 + よう + に + 聞こえる/言う表示听起来像/说的像是。 示例： あの人を見たような気がした。有一种以前好像见过那个人的感觉。 彼は学生のような雰囲気ですね。他有点学生气。 ちょっと怒ったように聞こえた。听起来好像有点生气了。 何も起こらなかったように言った。说的好像什么都没发生过似的。 5.7.2 用「みたい」表示某物看起来像是别的东西用法公式：（みたい的活用） 肯定 否定 非过去 犬みたい 犬じゃないみたい 过去 犬だったみたい 犬じゃなかったみたい 名词/形容词/动词与「みたい」及其活用式连用表示某物看起来像该名词一样，但实际情况可能只是看上去像。 示例： もう売り切れみたい。看样子已经卖光了。* 「売【う】り切れ」是名词。 制服を着ている姿をみると、学生みたいです。看着那个穿制服的身影，像学生一样。 5.7.3 用「〜そう」猜测结果用法公式： 动词词根 + そう动词词根与「そう」连用表示该动作看起来要发生。动词词根变形见 4.1.2 动词词根 い形容词去掉结尾い + そう な形容词 + そう形容词与「そう」连用表示主题看起来是这个形容词。 （特例）いい -&gt; よさ 动词否定式去掉结尾い/「〜ない」的衍生い形容词去掉结尾い + さ + そう将动词否定式或者「〜ない」的衍生い形容词结尾的「い」替换为「さ」再与「そう」连用表示对否定情况的猜测。 * 该语法不适用于普通名词的推测。 示例： バランスが崩【くず】れて、一瞬倒【たお】れそうだった。失去了平衡，一瞬间好像要倒下。 この辺【あた】りにありそうだけどな。不过好像应该在这周围啊。 この漬物はおいしそう！这酱菜看起来很好吃！ これも結構よさそうだけど、やっぱり高いよね。这个看起来也不错，不过果然应该很贵，对吧？ お前なら、金髪の女が好きそうだな。你的话，看起来像是喜欢金发女生。 もう10 時になったから、来なさそうだね。既然已经10:00了，看来不会来了。 これはただの試合じゃなさそうだ。看样子这不仅仅是一场比赛。 5.7.4 用「〜そうだ」表示传闻用法公式： 动词/い形容词 + そう + だ/です/でございます 名词/な形容词 + だ + そう + だ/です/でございます在句子结尾加上「〜そうだ」表示前面的句子是自己听来的传闻，根据句子结尾是/动词/い形容词/名词/な形容词/判断具体表达形式。 示例： 明日、雨が降るそうだ。听说明天要下雨。 毎日会いに行ったそうです。听说每天都去见面。 彼は、高校生だそうです。听说他是个高中生。 A：今日、田中さんはこないの？A：田中桑今天不来吗？B：だそうです。B：听说是的。 5.7.5 用「〜らしい」表示较为确定的传闻用法公式： 名词/动词/形容词 + らしい将/名词/动词/形容词/直接与「〜らしい」连用，表示根据自己得到的信息较为确定的传闻。「〜らしい」的活用形式与い形容词一样，详见 3.4.3 い形容词 。 示例： A：今日、田中さんはこないの？A：田中桑今天不来了吗？B：こないらしい。B：（据我所知，）应该是的。 A：あの人は何なの？A：那边那个人是干嘛的呢？B：美由紀さんの友達らしいですよ。B：（据我所知，）应该是美由纪桑的朋友哦。 あの子は子供らしくない。那小孩的举动看上去不像是小孩。 大人らしくするつもりだったのに、大【おお】騒【さわ】ぎしてしまった。虽然我想要表现得像个成年人，但结果我还是不小心引发了大骚动。 5.7.6 「っぽい」：用来表示相似性的俚语用法公式： 名词/动词/形容词 + っぽい将/名词/动词/形容词/直接与「っぽい」连用，表示像是……的样子。「っぽい」的活用形式与い形容词一样，详见 3.4.3 い形容词 。 示例： あの人はちょっと韓国人っぽいよね。那人看上去有点像韩国人，是吧？ みんなで、もう全部食べてしまったっぽいよ。大家已经把东西全吃光了的样子。 恭子は全然女っぽくないね。恭子一点也没有女人样啊。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"如何理解 Transformer","slug":"如何理解 Transformer","date":"2020-07-03T14:44:50.000Z","updated":"2020-07-03T14:44:50.000Z","comments":true,"path":"2020/07/03/如何理解 Transformer/","link":"","permalink":"/2020/07/03/如何理解 Transformer/","excerpt":"","text":"本文参考[1] Attention Is All You Need[2] The Illustrated Transformer 1 如何理解Transformer1.1 什么是TransformerTransformer模型 是在NIPS2017由谷歌发表 Attention is All You Need。 图1.1 Transformer模型架构[1] 图1.1 为论文原文[1]中的配图。 架构： 模型分为左半侧的N个Encoder和右半侧的Decoder两部分。 模型主要有几个主要模块：Multi-Head Attention、相加&amp;标准化部分、前馈神经网络。 将分为Encoder和Decoder两部分进行理解。 1.2 Encoder部分 图1.2 Encoder总览[2] 图1.2 展示了Encoder部分的架构逻辑： 将Token转化为Word Embedding。 将经过位置编码的向量x输入到Multi-Head Attention模块中，得到输出z。 将原始的x与z相加后经过Normalization，输出到前馈神经网络中。 将前馈神经网络的输出与前馈神经网络的输入相加后经过Normalization，输出到下个Encoder中。 1.2.1 Positional Encoding 图1.3 Positional Embedding[1] 如 图1.3 所示，模型使用相同维度的Positional Encoding与Word Embedding相加进行位置编码。 图1.4 Positional Encoding计算方法[2] * 其中，pos为Token位置，i为Positional Encoding的第i维的值，d_model为向量维度。 根据 图1.4 所示，对于第pos位置的Token的第i维Positional Encoding的值，使用三角函数进行计算。 1.2.2 Multi-Head Self-Attention 图1.5 Transformer中的Self-Attention机制[2] 图1.6 Self-Attention的直观定义[2] Self-Attention的公式定义： \\begin{equation}z = softmax(\\frac{QK^{T}}{\\sqrt{d_k}})V\\end{equation} 图1.7 Q、K、V的来历[2] 如 图1.7 所示，QKV可经由Position Encoding后的输入向量与可训练的权重矩阵相乘得到。 图1.8 Multi-Head的Self-Attention[2] 图1.9 Multi-Head输出的Concatenation[2] 如 图1.8 图1.9 所示，Multi-Head的Self-Attention机制将会生成多个Attention，在输出时将所有输出Concatenate成一个高维度向量，并乘上一个可训练的参数矩阵W，最后输出一个和输入向量同维度的向量Z。 1.2.3 剩下的部分 图1.2 Encoder总览[2] 在得到了Multi-Head Self-Attention模块的输出之后： 将该模块的输入和输出向量相加并进行Normalization，得到Add&amp;Normalize的输出。 将上一层结果输入到一个position-wise fully connected feed-forward network中。 将该前馈神经网络的输入和输出相加并进行Normalization，得到该Encoder的输出。 Position-wise Feed-Forward Networks的公式定义： \\begin{equation}FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\\end{equation} 至此，Encoder部分的结构结束，Encoder部分通常会Stack多个Encoder。 1.3 Decoder部分 图1.10 Decoder总览[2] 图1.10 展示了Decoder部分的架构逻辑： Decoder是时序输入的，而不像Encoder是并行输入运算的。 首先将Previous Output经过Positional Encoding输入到Decoder中，经过一个Mask掉Future Position的Multi-Head Self-Attention，得到输出并进行Add&amp;Normalize。 将上一层的输出输入到Encoder-Decoder连接的Multi-Head Self-Attention中，得到输出并进行Add&amp;Normalize。 将上一层的输出输入到下一个Decoder中。 1.3.1 Mask掉Future Position的Multi-Head Self-Attention 图1.11 Decoder的时序过程[2] 如 图1.11 所示，每个Decoder需要重复Token个数的Decode过程。每次Decode只有Previous Output是实际输入进Decoder的，而剩下的Word Embedding及其相对应的QKV在Mask掉Future Position的Multi-Head Self-Attention中都是被Mask掉的（设置为Inf）。 1.3.2 Encoder-Decoder连接的Multi-Head Self-Attention 图1.12 Encoder-Decoder之间的参数传递[2] 如 图1.12 所示，Encoder-Decoder连接的Multi-Head Self-Attention的KV是由前面的Encoder传递过来的KV，而Q矩阵则由Decoder中Mask掉Future Position的Multi-Head Self-Attention传过来。 1.4 最终线性层和Softmax层 图1.13 最终线性层和Softmax层[2] 如 图1.13 所示，在得到Docoder Stack的输出后，向量先经过一个线性层Map到和Vocabulary想同维度的向量空间上，在经过一个Softmax计算概率，最终得到概率最高为本轮输出的单词。 1.5 Transformer的优缺点优点： 脱离了RNN、LSTM的束缚，仅使用Attention机制完成了模型的构建。这使得模型具有了一定程度的并行计算能力，适用于现代的GPU、TPU，大幅提升训练效率。 编不出来了。 缺点： 编不出来，它在当时的结果就是好。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"/tags/NLP/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"如何理解 Attention","slug":"如何理解 Attention","date":"2020-07-03T13:54:38.000Z","updated":"2020-07-03T13:54:38.000Z","comments":true,"path":"2020/07/03/如何理解 Attention/","link":"","permalink":"/2020/07/03/如何理解 Attention/","excerpt":"","text":"本文参考[1] NEURAL MACHINE TRANSLATIONBY JOINTLY LEARNING TO ALIGN AND TRANSLATE[2] Visualizing A Neural Machine Translation Model[3] CS224n: Natural Language Processing with Deep Learning /Stanford 1 如何理解Attention1.1 什么是AttentionAttention机制 通常被认为始于ICLR2015的文章 NEURAL MACHINE TRANSLATIONBY JOINTLY LEARNING TO ALIGN AND TRANSLATE。 图1.1 基于Bi-RNN encoder的Attention[1] 图1.1 为原始文章[1]中的配图，描述了一个基于双向RNN Encoder-RNN Decoder的Attention模型。（（具体公式定义又没看懂）现在又稍微看懂点了） 符号理解： X_T为输入句子的对应Token的Word Embedding。 h_T为双向RNN结构，用于Word Embedding的输入。 y_t为模型输出的预测单词。 s_t为单RNN结构，用于输出预测单词。 α为每个隐藏层的权重。 图1.2 基于RNN encoder的Attention[2] 图1.2 为CS224课程ppt的一张插图，该图描述了一个基于单向RNN的Attention模型，相比论文的配图 图1.1 更好理解。 两张图描述的模型有何区别： 原文中输出函数以y_i-1，s_i，c_i作为变量经过函数输出y_i（应该是矩阵变换）。CS224课程ppt中采用的方法为现在常用的Concatenation方法，将c_i和s_i的矢量直接相连，之后经过神经网络（其实就是训练好的矩阵变换）得到y_i输出。 公式定义： 计算Softmax后的Attention Score： \\begin{equation}e_{ij} = h_j \\cdot s_i\\end{equation} \\begin{equation}a_{ij} = \\frac{exp(e_{ij})}{\\sum_{j=1}^{J}exp(e_{ij})}\\end{equation} 计算Context Vector（Attention Output）： \\begin{equation} c_i = \\sum_{j=1}^{J}a_{ij}⋅h_{j}\\end{equation} Concatenation： \\begin{equation}o_i = [c_i; s_i]\\end{equation} 计算输出： \\begin{equation}y_i = f(o_i)\\end{equation} * 其中，i表示Decoder部分第i个隐藏状态，j表示encoder部分第j个隐藏状态，⋅ 表示点乘。f( )为一个神经网络，输入为级联向量o_i，输出为y_i。 具体示意图如下所示： 图1.3 Attention Weights的计算[3] 图1.4 Context Vector的计算与Decoder的传播[3] 1.2 Attention机制的优缺点优点： 通过打分机制确定输出，让模型能够在训练时自己学会句子的对齐方式。 在单次输出时，整个Encoder的Token都会参与贡献，一定程度消除了长距离依赖的问题，让句子的每个部分都可以参与到输出，而不是仅限于最后一个hidden state。 缺点： 我不知道有啥缺点，现在的大部分模型都是基于Attention机制的延伸提高模型性能。 1.3 More general definition of attentionDefinition： Given a set of vector values (h_t), and a vector query (s_i), attention is a technique to compute a weighted sum of the values, dependent on the query. 在机器翻译中，根据Query（Decoder隐藏层信息），通过权重确定Value（Encoder隐藏层信息），而学习到的对齐方法（Query-Value匹配）。 1.4 不同的Attention计算方法三种主要的注意力计算方法： 点乘Attention： \\begin{equation}e_{ij} = s_i^{T}h_j\\end{equation} 加权的点乘Attention： \\begin{equation}e_{ij} = s_i^{T}Wh_j\\end{equation} 加法Attention： \\begin{equation}e_{ij} = v^{T}tanh(W_1h_{j} + W_2s_{i})\\end{equation} * 其中，W；v，W1，W2均为权重矩阵。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"/tags/NLP/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"如何理解 RNN和LSTM","slug":"如何理解 RNN和LSTM","date":"2020-07-03T13:48:42.000Z","updated":"2020-07-03T13:48:42.000Z","comments":true,"path":"2020/07/03/如何理解 RNN和LSTM/","link":"","permalink":"/2020/07/03/如何理解 RNN和LSTM/","excerpt":"","text":"本文参考[1] Dan Jurafsky和James H. Martin合著的Speech and Language Processing第三版[2] 知乎专栏 深入理解RNN与LSTM[3] 知乎问答 谁能用比较通俗有趣的语言解释RNN和LSTM？[4] CS224n: Natural Language Processing with Deep Learning /Stanford 如何理解RNN和LSTM1 如何理解RNN1.1 什么是RNN 图1.1 单层RNN[1] 图1.1 为单层RNN的图示表示。很多教材、课程、博客都会给出这个浅显易懂的图，然而这图虽然简洁但并不容易理解。（反正我没看懂） 符号理解： x_t为时间顺序文本序列，通常一个x_t为一个Word Embedding。 h_t为时间t的隐藏状态，是x_t和h_t-1输入信息的中间层输出。 y_t为输出结果，是h_t输入信息的最终输出。 蓝色箭头的循环意思是同一层RNN的参数矩阵不变。 1.2 RNN的前向传播 图1.2 单层RNN的前向传播[1] 图1.2 同为单层RNN，理解上要比 图1.1 好很多。 公式定义： 隐藏状态： \\begin{equation}h_t = g(Wx_t + Vh_{t-1} + b)\\end{equation} 最终输出： \\begin{equation}y_t = f(Uh_t)\\end{equation} 其中，g( )，f( )为激活函数，通常为softmax函数；W，V，U为参数矩阵；b为偏置。 图1.3 单层RNN的时间序列表示[1] 图1.3 在 图1.2 基础上进行了时间轴方向的拓展。可以看到尽管时间序列输入x_t以及隐藏状态h_t会根据时间变化而改变，但参数矩阵在单次前向传播的过程中不会改变。 1.3 RNN的反向传播数学不好就不BP了。 1.4 RNN的优缺点优点： 相比传统神经网络，由于引入了基于时间t的隐藏状态，能够理解时间序列文本的前后位置关系。 相比传统神经网络，单次前向传播的所有隐藏状态均使用同一参数矩阵降低了参数矩阵的规模。 缺点： 梯度消失（Gradient Vanishing）问题，反向传播误差随着文本长度的增加而变弱，使得参数矩阵不能很好地学习到文本的长距离依赖。 由于RNN时间序列间的前后依赖特性，在长文本情况下训练不能并行化导致效率降低。 2 如何理解LSTM2.1 什么是LSTM 图2.1 单层LSTM[2] 符号理解： x_t为t时刻输入的文本序列。 c_t为t时刻的cell state，负责存储记忆信息，受t时刻输入x_t和t-1时刻隐藏状态h_t-1控制。 h_t为t时刻的隐藏状态，受t时刻输入x_t和c_t控制。 f_t为遗忘门输出，负责控制记忆信息的遗忘和保留。 i_t为输入门输出，负责控制输入信息写入cell state。 o_t为输出门输出，负责控制cell state信息写入新的隐藏状态。 2.2 LSTM的前向传播 图2.1 单层LSTM[2] 公式定义： 遗忘门： \\begin{equation}f_t = g(W_fh_{t-1} + U_fx_t + b_f)\\end{equation} 输入门： \\begin{equation}i_t = g(W_ih_{t-1} + U_ix_t + b_i)\\end{equation} 输出门： \\begin{equation}o_t = g(W_oh_{t-1} + U_ox_t + b_o)\\end{equation} New Cell Content： \\begin{equation}\\tilde{c_t} = tanh(W_ch_{t-1} + U_cx_t + b_c)\\end{equation} Cell State： \\begin{equation}c_t = f_t \\otimes c_{t-1} + i_t \\otimes \\tilde{c_t}\\end{equation} 隐藏状态： \\begin{equation}h_t = o_t \\otimes tanh(c_t)\\end{equation} 其中，g( )通常选用sigmoid函数，⊗表示元素逐项相乘。 图2.2 LSTM的时间序列表示[2] 2.3 LSTM的反向传播数学不好就不BP了。 2.4 LSTM的优缺点优点： 通过引入Cell State作为记忆，一定程度削弱了传统RNN的梯度消失问题，相比RNN能够更好的学习长文本的长距离依赖。 缺点： 非要说缺点就是，现在，LSTM在某些任务上的性能指标表现不如Transformer。 2.5 LSTM需要注意的几个点 LSTM使用sigmoid作为门限，sigmoid输出在(0, 1)区间，是为了进行数据的选择；使用tanh，是为了记忆的long term保存。 LSTM不是解决了梯度消失问题，而是通过自身机制削弱了梯度消失。 还有个类似的GRU改天再看。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"/tags/NLP/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 5.6 表示数量和程度","slug":"日语语法 5.6 表示数量和程度","date":"2020-06-28T16:16:48.000Z","updated":"2020-06-28T16:16:48.000Z","comments":true,"path":"2020/06/29/日语语法 5.6 表示数量和程度/","link":"","permalink":"/2020/06/29/日语语法 5.6 表示数量和程度/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 5 特殊表达5.6 表示数量和程度5.6.1 用「だけ」表示只有用法公式： 名词 + だけ名词与「だけ」连用表示除该名词没有其他东西了，即只有该名词。 示例： りんごだけ。只有苹果了。 これとそれだけ。只有那个和这个了。 それだけは、食べないでください。只有这个，请不要吃。 この歌だけを歌わなかった。不只唱了这首歌。 その人だけが好きだったんだ。只喜欢那个人。（解释性语气） この販売機だけでは、五百円玉が使えない。只有这台售货机不能用500日元硬币。 用法公式： 动词辞书形 + だけ动词辞书形与「だけ」连用表示除该动作没有其他动作了，即只做该动作。 示例： 準備が終わったから、これからは食べるだけだ。既然准备好了，现在开始只管吃就是了。 ここに名前を書くだけでいいですか？只在这里写名字是否就可以？ 5.6.2 用「だけ」的正式版本「のみ」用法公式： 名词 + のみ名词与「のみ」连用表示除该名词没有其他东西了，即只有该名词。 示例： この乗車券は発売当日のみ有効です。这种车票只在购买当日有效。 アンケート対象は大学生のみです。这个调查问卷的对象仅限大学生。 5.6.3 用「しか」表示除此之外别的都（except for）用法公式： 名词 + しか + 否定式动词名词与「しか」连用表示除该名词之外别的都不能进行该动作。（看例句吧还是。。。） 示例： これ だけ 見る。只看这个。 これ だけ 見ない。别只看这个。 これ しか 見ない。除了这个别的都不要看。 今日は忙しくて、朝ご飯しか食べられなかった。今天很忙，除了早饭别的都没法吃。 これしかある。 これだけある。只有这个。 A：全部買うの？你要全买下来？B：ううん、これしか。 ううん、これしか買わない。不是，除此之外别的都不买。 用法公式： 名词 + しか + ない动词与「しかない」连用表示除该动作之外没别的办法了。 示例： これから頑張るしかない！除了从现在开始努力之外没别的办法了！ こうなったら、逃げるしかない。一旦变成这样之后，除了逃跑没有别的选择了。 もう腐っているから、捨てるしかないよ。因为已经烂掉了，除了扔掉没别的办法了哦。 用法公式： 名词 + っきゃ + 否定式动词名词与「っきゃ」连用表示除该名词之外别的都不能被进行该动作。 动词 + っきゃ + ない动词与「っきゃない」连用表示除该动作之外没别的办法了。 「っきゃ」是「しか」的另一个版本，比「しか」语气更重。 示例： これは買うっきゃない！除了这个别的都不买！ こうなったら、もうやるっきゃない！如果变成这样，除了做下去也没别的办法了！ 5.6.4 用「ばかり」表示全都是用法公式： 名词 + ばかり + 动词名词与「っきゃ」连用表示全都是该名词在被进行该动作。 动词 + ばかり动词与「っきゃない」连用表示进行的全都是该动作。 示例： 彼は麻雀ばかりです。他除了打麻将不干别的。 直美ちゃんと遊ぶばっかりでしょう！就光想着跟直美一起玩，是不是！ 最近は仕事ばっかだよ。最近只顾工作了哦。 崇君は漫画ばっかり読んでてさ。かっこ悪い。崇只顾着看漫画书了啊…没意思。* 「読んでて」是「読んでいる」（进行时）的て形。 5.6.5 用「すぎる」表示太过了用法公式： （动词）动词词根 + すぎる （な形容词）な形容词 + すぎる （い形容词）い形容词结尾的い + すぎる （否定式动词和形容词）否定式动词和形容词结尾的い + さ + すぎる动词/形容词与「すぎる」表示动词/形容词的程度过分多。 示例： 佐藤さんは料理が上手で、また食べ過ぎました。佐藤桑擅长料理，于是我再一次吃多了。 お酒を飲みすぎないように気をつけてね。别喝太多酒，注意点哦。 大きすぎるからトランクに入らないぞ。太大了，装不进后备箱啊。 静かすぎる。罠かもしれないよ。太安静了，也许是个陷阱哦。 時間が足りなさすぎて、何もできなかった。因为太缺时间了，我什么也做不了。 彼には、彼女がもったいなさすぎるよ。他真是配不上她。 用法公式： 动词词根 + すぎ将「すぎる」活用为词根与动词连用，表示名词性。 示例： A：昨晩のこと、全然覚えてないな。A：昨晚的事，完全记不起来了啊。B：それは飲みすぎだよ。B：那是喝太多了哦。 5.6.6 用「も」表示过量用法公式： 量词 + も量词与「も」合用表示过量。 示例： 昨日、電話三回もしたよ！昨天打了三次电话哦！ 試験のために三時間も勉強した。为了考试学习了整整三个小时。 今年、十キロも太っちゃった！今年整整长了十公斤！ 5.6.7 用「ほど」表示事情的程度用法公式： 名词/动词 + ほど「ほど」与名词/动词合用表示事情的程度。 示例： 今日の天気はそれほど暑くない。今天的天气没有热到那种程度。 寝る時間がないほど忙しい。忙到没时间睡觉的程度。 用法公式： 动词/形容词 + 条件语 + 相同动词/相同形容词 + ほど、句子用于表达「越…越…」句式。具体用法见示例。* な形容词，不能用条件语「ば」。并且在与「ほど」连用时，需要加上「な」，因为「ほど」为名词。 示例： 韓国料理は食べれば食べるほど、おいしくなる。韩国料理越吃越变得好吃。 歩いたら歩くほど、迷ってしまった。越走越觉得不小心迷路了。 勉強をすればするほど、頭がよくなるよ。学习越多，头脑变得越好。 示例： iPod は、ハードディスクの容量が大きければ大きいほどもっとたくさんの曲が保存できます。iPod硬盘容量越大，能保存的歌曲越多。 航空券は安ければ安いほどいいとは限らない。机票越便宜越好是不受限制的。（无关系） 文章は、短ければ短いほど、簡単なら簡単なほどよいです。文章越短越简单，是越好的。 5.6.8 形容词的名词化用法公式： （い形容词）い形容词结尾的い + さ去掉い形容词结尾的「い」加上「さ」，表示い形容词的名词化。 （な形容词）な形容词 + さな形容词与「さ」连用，表示な形容词的名词化。 示例： このビルの高さは何ですか？这座楼有多高？ 犬の聴覚の敏感さを人間と比べると、はるかに上だ。犬类的听觉敏感性与人类相比，远胜。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 5.5 表达不同程度的确定性（かもしれない、でしょう、だろう）","slug":"日语语法 5.5 表达不同程度的确定性（かもしれない、でしょう、だろう）","date":"2020-06-28T10:38:28.000Z","updated":"2020-06-28T10:38:28.000Z","comments":true,"path":"2020/06/28/日语语法 5.5 表达不同程度的确定性（かもしれない、でしょう、だろう）/","link":"","permalink":"/2020/06/28/日语语法 5.5 表达不同程度的确定性（かもしれない、でしょう、だろう）/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 5 特殊表达5.5 表达不同程度的确定性（かもしれない、でしょう、だろう）5.5.1 用「かもしれない」用法公式： 从句 + かもしれない从句与「かもしれない」合用表示对于前面从句的不确定，确定性小于「多分」。* 「しれない」是「知らない」的可能形。 名词和な形容词形成的从句不能带表状态的「だ」。 示例： 先生だかもしれない → 先生かもしれない 退屈だかもしれない → 退屈かもしれない 口语里面可以简写成「かも」。 示例： 面白いかもしれない → 面白いかも 示例： 映画を観たかもしれない没准看了电影。 彼は学生かもしれない他没准是学生。 それは面白いかもしれない这没准有趣。 スミスさんは食堂に行ったかもしれません。Smith桑也许去了食堂。 雨で試合は中止になるかもしれないね。比赛也许会因为下雨取消，对吧？ この映画は一回観たことあるかも！我也许已经看过一次这部电影了。 あそこが代々木公園かもしれない。那边也许就是代代木公园。 もう逃げられないかもしれんぞ。也许不能再逃避了，是吧。（男性化） 5.5.2 用「でしょう」礼貌地表达一定程度的确定性用法公式： 句子 + でしょう句子与「でしょう」合用表示对于前面句子的不确定，确定性与「多分」类似。* 可将肯定句子中的「〜です（か）」换成「〜でしょう（か）」表示不确定语气。 示例： 明日も雨でしょう。明天也许也会下雨。 学生さんでしょうか。学生是吗？ これからどこへ行くんでしょうか？之后要去哪里，是吗？（解释性语气） 如果你想显得特别特别礼貌，甚至可以把「〜でしょうか」加到「〜ます」后面。 示例： 休ませていただけますでしょうか。我可以得到休息的机会吗，也许？ 5.5.3 用「でしょう」和「だろう」在口语中表示很大程度的确定性用法公式： （口语）句子 + でしょう在口语中，句子与「でしょう」连用通常为说话者向对方确认。通常结果上希望得到肯定回答，或者事实上就是如句子所言。 （口语）句子 + だろう「でしょう」的男性化版本。 示例： A：あっ！遅刻しちゃう！A：啊！要迟到了！B：だから、時間がないって言ったでしょう！B：所以啊，都跟你说过没时间了对吧！ A：これから食べに行くんでしょ。A：你现在就要开始吃对吧？B：だったら？B：是啊怎么了？ A：掃除、手伝ってくれるでしょう。A：你会帮我做扫除的，对吧？B：え？そうなの？B：哎？是这样吗？ A：アリスはどこだ？A：Alice 在哪里？B：もう寝ているだろう。B：应该已经睡了。 A：もう家に帰るんだろう。A：你已经要回家了，对吧？B：そうよ。B：是的。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 5.4 通用名词的特殊表达（こと、ところ、もの）","slug":"日语语法 5.4 通用名词的特殊表达（こと、ところ、もの）","date":"2020-06-28T10:38:00.000Z","updated":"2020-06-28T10:38:00.000Z","comments":true,"path":"2020/06/28/日语语法 5.4 通用名词的特殊表达（こと、ところ、もの）/","link":"","permalink":"/2020/06/28/日语语法 5.4 通用名词的特殊表达（こと、ところ、もの）/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 5 特殊表达5.4 通用名词的特殊表达（こと、ところ、もの）5.4.1 用表示某件事是否发生过用法公式： 动词辞典形 + こと + 助词 + ある使用动词辞典形与「こと」连用，表示动词的名词化。* 《大家的日本语》中，此处的助词通常为「が」。不明白此处使用「が」与「は」有何语气区别？ 示例： 徹夜して、宿題することはある。通宵做作业是有的。 一人で行くことはありません。从不独自走。 示例： パリに行ったことはありますか。有去过巴黎吗？ お寿司を食べたことがある。吃过寿司。 ⽇本の映画を観たことないの？没看过日本电影吗？ ヨーロッパに行ったことがあったらいいな。如果有去过欧洲的话就好了。（羡慕） そういうのを見たことがなかった。没有见到过这样的东西。 一度行ったこともないんです。一次也没去过。 5.4.2 用「ところ」表示一个抽象的地方用法公式： い形容词/な形容词+な/动词辞典形 + ところい形容词/な形容词+な/动词辞典形与「ところ」连用，可用于表示物理上的位置，也可以表示抽象的个性/时间段。 示例： 早くきて。映画は今ちょうどいいところだよ。快点来。电影正在精彩之处呦。 彼は優しいところもあるよ。他也有温柔的一面。 今は授業が終ったところです。刚刚下课。 これから行くところでした。当时正准备要走。 5.4.3 用「もの」表示女性化口语中的强调示例： どうしてこなかったの？为什么没来？ 授業があったの。有课。（女性化、解释口吻） 授業があったもの。有课。（女性化、解释口吻） 授業があったもん。有课啦。（女性化、解释口吻） 我感觉作者强行水了一节。。。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 5.3 无意发生的事情","slug":"日语语法 5.3 无意发生的事情","date":"2020-06-24T12:33:54.000Z","updated":"2020-06-24T12:33:54.000Z","comments":true,"path":"2020/06/24/日语语法 5.3 无意发生的事情/","link":"","permalink":"/2020/06/24/日语语法 5.3 无意发生的事情/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 5 特殊表达5.3 无意发生的事情5.3.1 与其他动词合用「しまう」用法公式： 动词て形 + しまう动词て形与「しまう」合用表示动作并非本意。通常事情结果不尽人意。 示例： そのケーキを全部食べてしまった。我不小心把整个蛋糕都吃了。 毎日ケーキを食べて、２キロ太【ふと】ってしまいました。我每天吃蛋糕，一不小心胖了两公斤。 ちゃんと食べないと、痩【や】せてしまいますよ。如果饮食不正确的话，你会无意中变瘦的哦。 結局、嫌なことをさせてしまった。结果，我还是无意中让别人做了令人讨厌的事。 ごめん、待たせてしまって！不好意思，不小心让你久等了！ 金魚がもう死んでしまった。那条金鱼已经死了。（残念） 5.3.2 使用「〜てしまう」的口语版本用法公式： 动词て形（去掉て/で） + しまう + ちゃう/じゃう将动词て形中的て/で去掉，并将「しまう」替换为口语版本「ちゃう/じゃう」。用于在口语中表示动作并非本意。通常事情结果不尽人意。 示例： 金魚がもう死んじゃった。那只金鱼已经死了。 もう帰っちゃっていい？我现在回家可以吗？ みんな、どっか行っちゃったよ。每个人都去了某个地方。 そろそろ遅くなっちゃうよ。就快要迟到了。 早く呼び出さないと、死んじゃうよ、お兄ちゃん。如果不快点召唤出来的话，会不小心死掉的哦，小哥哥。* 这个口语用法我的第一反应就是这句伊莉雅对士郎经典台词。（加入私货） 用法公式： 动词て形（去掉て/で） + しまう + ちまう/じまう将动词て形中的て/で去掉，并将「しまう」替换为口语版本「ちまう/じまう」。该版本为中年男子用，较粗俗。 示例： また遅刻しちまったよ。又不小心迟到了。 ごめん、ついお前を呼んじまった。不好意思，不小心打了你的电话。 5.3.3 「しまう」的另一个意思用法公式： 动词て形 + しまう动词て形与「しまう」合用表示动作的完成。 示例： 宿題をやってしまいなさい。请把你的作业做完。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 5.2 尊敬语和谦逊语","slug":"日语语法 5.2 尊敬语和谦逊语","date":"2020-06-24T12:33:36.000Z","updated":"2020-06-24T12:33:36.000Z","comments":true,"path":"2020/06/24/日语语法 5.2 尊敬语和谦逊语/","link":"","permalink":"/2020/06/24/日语语法 5.2 尊敬语和谦逊语/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 5 特殊表达5.2 尊敬语和谦逊语5.2.1 固定搭配一些动词的尊敬语和谦逊语 字典形 尊敬语 谦逊语 する なさる 致す【いた・す】 行く いらっしゃる∕おいでになる 参る【まい・る】 来る いらっしゃる∕おいでになる 参る【まい・る】 いる いらっしゃる∕おいでになる おる 見る ご覧【らん】になる 拝見する【はい・けん・する】 聞く － 伺う【うかが・う】 言う おっしゃる 申【もう】す∕申し上げる あげる － 差【さ】し上げる くれる 下【くだ】さる － もらう － いただく 食べる 召【め】し上がる いただく 飲む 召【め】し上がる いただく 知っている ご存【ぞん】知（です） 存じる 用法公式：（普通尊敬语动词的ます形活用） 参见 4.1 敬语和动词词根 用法公式：（特殊尊敬语动词的ます形活用） （ます形）尊敬语动词 + る + います （过去ます形）尊敬语动词 + る + いました （否定ます形）尊敬语动词 + る + いません （过去否定ます形）尊敬语动词 + る + いませんでした 特殊尊敬语动词的ます形活用 字典形 ます形 过去ます形 否定ます形 过去否定ます形 なさる なさいます なさいました なさいません なさいませんでした いらっしゃる いらっしゃいます いらっしゃいました いらっしゃいません いらっしゃいませんでした おっしゃる おっしゃいます おっしゃいました おっしゃいません おっしゃいませんでした 下さる 下さいます 下さいました 下さいません 下さいませんでした ござる ございます ございました ございません ございませんでした 尊敬语的主题一般为别人，即说话人谈论对方的时候使用。 示例： アリスさん、もう召し上がりましたか。Alice 桑，您已经吃了吗？ 仕事で何をなさっているんですか。您上班做什么？ 推薦状を書いてくださるんですか。您可以帮我写一封推荐信吗？ どちらからいらっしゃいましたか。您从哪里来？ 今日は、どちらへいらっしゃいますか。您今天去哪儿？ 谦逊语的主题一般为自己，即说话人谈论自己的时候使用。 示例： 私はキムと申【もう】します。至于我，（人们）说Kim。（我叫Kim） 私が書いたレポートを見ていただけますか。我可以收到帮我检查报告的荣幸吗？ 失礼致【いた】します。失陪。 5.2.2 其他替代说法用法公式： あります（ある） + ございます（ござる）「ござる」是「ある」的礼貌版本，通常以丁宁形「ございます」出现。 です + でございます「です」的礼貌版本「でございます」是「でござる」的ます形活用，从「である」而来，字面意思是「以…的形式存在」。 示例： こちらは、私の部屋です。这边是我的房间。 こちらは、私の部屋でございます。这边是我的房间。（礼貌） お手洗いはこのビルの二階にあります。洗手间在这座楼的二层。 お手洗いはこのビルの二階にございます。洗手间在这座楼的二层。（礼貌） 用法公式：（对不起的六种说法） ごめん。 ごめんなさい。 すみません。 申し訳ありません。【もう・し・わけ・ありません】（申し訳是言い訳的谦逊形） 恐れ入ります。【おそ・れ・い・ります】 恐縮です。【きょう・しゅく・です】 5.2.3 尊敬形活用用法公式：（不存在固定搭配的动词的尊敬形活用） お + 词根 + に + なる お + 词根 + です表示对说话对方的尊敬。词根变形详见 4.1.2 动词词根。 示例： 先生はお見えになりますか。见着老师了吗？* 不明白这句话是对老师的尊敬还是对说话对方的尊敬？ もうお帰りですか。您已经要回家了吗？ 店内でお召し上がりですか。您要在店内用餐吗？* 服务员使用的二重敬语，即在「召【め】し上がる」的基础上再进行尊敬语活用。 用法公式： （对于不存在固定搭配的动词的尊敬形）お + 词根 + になる + 下さい对于不存在固定搭配的动词的尊敬形，我们也可以将上文中的「になる」替换为「ください」。 （对于5.2.1中的某些「になる」结尾的固定搭配）ご覧 + になる + 下さい对于某些「になる」结尾的固定搭配，我们也可以将结尾的「になる」替换为「ください」。 示例： 少々お待ちください。请稍候。 こちらにご覧下さい。请看这边。 閉まるドアにご注意下さい。关门请注意。 5.2.4 谦逊形活用用法公式： お + 词根 + する 示例： よろしくお願いします。请多关照。 先生、お聞きしたいことがありますが。老师，我有想问的事情。* 此处的「が」不明白意为何用，可能是用于增强语气。 すみません、お待たせしました。抱歉，刚刚让您久等了。（使役形） 千円からお預【あず】かりいたします。我们拿着您的1000日元。（二重敬语） 5.2.5 做出尊敬性的请求用法公式：（对于5.2.1中的固定搭配ます形） 尊敬语动词 + る + います + いませ 尊敬语动词 + る + います + い用上述表达做出尊敬性的请求。 示例： いらっしゃいませ。请进！ いらっしゃい！请进！ ありがとうございました。またお越【こ】しくださいませ。非常感谢，请再次光临。 どうぞ、ごゆっくりなさいませ。请放松。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 5.1 使役动词和被动词","slug":"日语语法 5.1 使役动词和被动词","date":"2020-06-24T12:32:39.000Z","updated":"2021-11-26T17:57:30.997Z","comments":true,"path":"2020/06/24/日语语法 5.1 使役动词和被动词/","link":"","permalink":"/2020/06/24/日语语法 5.1 使役动词和被动词/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 5 特殊表达5.1 使役动词和被动词5.1.1 使役动词用法公式： （る动词）る + させる对于る动词，将「る」换成「させる」表示对他人的使役。 （う动词）う段 + あ段 + せる对于う动词，将动词末位从う段改为あ段，并与「せる」合用表示对他人的使役。 （例外）する -&gt; させる （例外）くる -&gt; こさせる 示例： 先生が学生に宿題をたくさんさせた。老师使学生们做很多作业。 先生が質問をたくさん聞かせてくれた。老师让（某人）问很多问题。 今日は仕事を休ませてください。请让我从今天的工作中休息（请让我今天请假）。 その部長は、よく長時間働かせる。部长经常使（员工）长时间工作。 用法公式： （る动词）る + さす （う动词）う段 + あ段 + す （例外）する -&gt; さす （例外）くる -&gt; こさす以上几种表达为使役动作的简短表达，一句话总结即为将完整版中的「せる」替换为「す」。 5.1.2 被动词用法公式： （る动词）る + られる对于る动词，将「る」换成「られる」表示动作的被动。 （う动词）う段 + あ段 + れる对于う动词，将动词末位从う段改为あ段，并与「れる」合用表示对他人的使役。 （例外）する -&gt; される （例外）くる -&gt; こられる 示例： ポリッジが誰かに食べられた！粥是被某个人吃了！ みんなに変だと言われます。我被大家说奇怪。 光の速さを超えるのは、不可能だと思われる。超过光速被认为是不可能的。 この教科書は多くの [1] 人に読まれている。这本教科书正在被很多人读。*「多くの」可以用于修饰名词，「多い」只能用作谓语。 外国人に質問を聞かれたが、答えられなかった。我被一个外国人问了一个问题，但我没能回答上来。*前半句「聞かれた」中的「かれた」表示被动语态；而后半句「答えられなかった」中的「られなかった」则为可能形，表示没能够回答上来。 このパッケージには、あらゆるものが含まれている。所有东西都被包含在了这个包裹中。 5.1.3 用被动形表示礼貌用法公式：（要怎么做？） どう + する？同级间 どう + しますか？一般礼貌 どう + されますか？被动形的礼貌 どう + なさいますか？敬语 どう + なさいますでしょうか？敬语+ 少一些的确定性 示例： 領収証はどうされますか？收据怎么办呢？ 明日の会議に行かれるんですか？你参加明天的会吗？ 5.1.4 使役被动形用法公式： （る动词）る + させ + られる将る动词活用为使役形，在此基础上再活用为被动形，表示被使役。 （う动词）う段 + あ段 + せ + られる将う动词活用为使役形，在此基础上再活用为被动形，表示被使役。 示例： 朝ご飯は⻝べたくなかったのに、食べさせられた。明明不想吃早饭，还是被要求吃了。 日本では、お酒を飲ませられることが多い。在日本，被迫喝酒这种事很多。 あいつに二時間も待たせられた。那家伙让我等了2小时。 親に毎日宿題をさせられる。我每天被爸妈要求做作业。 用法公式： （う动词）う段 + あ段 + される不是「す」结尾的う动词可用上述简短表达。* 「す」结尾的う动词以及る动词如此变形都会出现「さされる」这样的表达，故不能这样简化。 示例： 学生が廊下に立たされた。那个学生被要求站在走廊。 日本では、お酒を飲まされることが多い。在日本，被迫喝酒这种事很多。 あいつに二時間も待たされた。那家伙让我等了2小时。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 4.17 口语习惯及俚语","slug":"日语语法 4.17 口语习惯及俚语","date":"2020-04-28T14:03:30.000Z","updated":"2020-04-28T14:03:30.000Z","comments":true,"path":"2020/04/28/日语语法 4.17 口语习惯及俚语/","link":"","permalink":"/2020/04/28/日语语法 4.17 口语习惯及俚语/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 4 语法精要4.17 口语习惯4.17.1 用「じゃん」代替反问「じゃない」来进行确认用法公式： じゃない + じゃん当「じゃない」表示反问寻求肯定时，可以用「じゃん」代替。* 从语气上说，「じゃん」的语气似乎要略强一点，并不是寻求对方肯定的意思。 示例： サラリーマンだから、残業はたくさんするんじゃない？他是职员，所以应该加班很多吧？ まあ、いいじゃない。嗯，也许没事吧。（你不觉得吗？） ほら、やっぱりレポートを書かないとだめじゃん。看吧，果然你得写报告是不是。 誰もいないからここで着替えてもいいじゃん。既然没人，那就可以在这里换衣服对吧。 4.17.2 用「つ」替代「という」用法公式： という + つ/つう使用「つ」或「つう」代替「という」。 示例： つうか、なんでお前がここにいんのよ！倒不如说，你为什么在这里？！ 宿題で時間がないつってんのに、みきちゃんとデートしにいったと聞いたよ。明明他说因为要做作业而没时间了，我却听说他去跟Miki-chan 约会了。 明日は試験だぞ。つっても、勉強はしてないだろうな。喂，明天考试了哦。不过就算我提醒你，你也不会学习对吧。 だから、違うんだつうの！所以啊，我说你错了啊！ 如果你想更强调一点，可以再加上一个小「つ」。这时候你应该已经到了忍耐的极限了。 示例： だから、違うんだ っ つうの！所以啊，你错了，我说了啊！ 4.17.3 用「ってば」和「ったら」表示愤怒用法公式： という + ってば/ったら使用「ってば」或「ったら」代替「という」，用于表示说话者的强烈语气甚至是愤怒。* 二者的原型是「という」的条件句「といえば」和「といったら」。 示例 もう行くってば！我都告诉过你我要过去了！ あなたったら、いつも忘れるんだから。因为啊，你啊，总是忘记啊！ 4.17.4 哪里都能用的「なんか」用法公式： なにか + なんか「なんか」用于在口语中代替「なにか」。 句子成分1 + なんか + 句子成分2「なんか」用于在口语中用作填充词。 示例： なにか食べる？吃东西吗？ なんか食べる？吃东西吗？ 示例： なんかね。お風呂って超気持ちいいよね！那啥，洗澡的超舒服，是吧？ お母さんが、なんか明日まで戻らないんだってよ。妈妈说过她要到，嗯，明天才能回来。 なんかさ。ボブは、私のことなんか本当に好きかな？怎么说呢。Bob对我，嗯，真的喜欢吗？ 今日は、なんか忙しいみたいよ。今天，怎么说呢，好像有点忙呢。 4.17.5 用「〜やがる」表示对某个举动的鄙视用法公式： 动词词根 + やがる动词词根与「やがる」合用，表示对于对于该动作的厌恶，常用于漫画。 动词词根与「やがる」的合用体可以像う动词一样被活用。 示例： あんなやつに負けやがって。じゃ、どうすんだよ？输给了你这样的家伙。喂，你想怎样？ やる気か？だったらさっさと来やがれ！想打架？那么想打就赶紧的！","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 4.18 句尾助词","slug":"日语语法 4.18 句尾助词","date":"2020-04-28T13:58:38.000Z","updated":"2020-04-28T13:58:38.000Z","comments":true,"path":"2020/04/28/日语语法 4.18 句尾助词/","link":"","permalink":"/2020/04/28/日语语法 4.18 句尾助词/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 4 语法精要4.18 句尾助词4.18.1 句尾助词「な」和「さ」用法公式： 句子 + な/さ将「な」和「さ」加在句子结尾用于表示语气。* 个人感觉这个语气很难用语言描述，个人感觉「な」可以译作「呀」，「さ」可以译作「啊」（单纯因为语调相似）。 示例： 洋介：今、図書館に行くんだよな。洋介：你这是要去图书馆呢啊？（想让对方解释）智子：うん、なんで？智子：嗯，怎么？ ボブ：日本語は、たくさん勉強したけどな。まだ全然わからない。Bob：我学日语很努力了呀。但我还是完全搞不明白。アリス：大丈夫よ。きっとわかるようになるからさ。Alice：没关系哦，因为你一定会变得明白的啊。ボブ：ならいいけどな。Bob：要这样就好了呀。 句尾助词「な」经常跟提问词「か」合用，表示说话者正在考虑什么。 示例： 今日は雨が降るかな？今天会不会下雨呢？ いい大学に行けるかな？能否上个好大学呢？ 4.18.2 句尾助词「かい」和「だい」用法公式： 句子 + かい/だい「かい」和「だい」是用来提问的句尾助词，「かい」用来问是非题，而「だい」则用于开放性问题。该用法较为男性化。 示例： おい、どこに行くんだい？喂，要去哪里？ さきちゃんって呼んでもいいかい？可以叫你Saki-chan 吗？ 一体何時に帰ってくるつもりだったんだい？到底打算何时回家？ 俺は土曜日、映画を見に行くけど、一緒に行くかい？我打算周六去看电影，要一起去吗？ 4.18.3 适用于不同性别的句尾助词用法公式： 句子 + よ + わ用「わ」代替「よ」表达同样的加强语气，该用法较为女性化。 句子 + よ + ぞ/ぜ用「ぞ」或「ぜ」代替「よ」表达同样的加强语气，该用法较为男性化，比较直率。 句子 + かな + かしら用「かしら」代替「かな」表达同样的不确定语气，该用法较为女性化。 示例： もう時間がないわ。已经没时间了啊。 おい、行くぞ！嗨，我们走吧！ これで、もう終わりだぜ。这样的话，那就已经结束了啊。 いい大学に入れるかしら？我在想自己能否上个好大学。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 4.16 数字和计数","slug":"日语语法 4.16 数字和计数","date":"2020-04-23T15:01:01.000Z","updated":"2020-04-23T13:48:58.000Z","comments":true,"path":"2020/04/23/日语语法 4.16 数字和计数/","link":"","permalink":"/2020/04/23/日语语法 4.16 数字和计数/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 4 语法精要4.16 数字和计数4.16.1 数字系统1-10的汉字以及读音 数字 1 2 3 4 5 6 7 8 9 10 漢字 一 二 三 四 五 六 七 八 九 十 假名 いち に さん し∕よん ご ろく しち∕なな はち きゅう じゅう 示例： 三十一（さんじゅういち）= 31 五十四（ごじゅうよん）= 54 七十七（ななじゅうなな）= 77 二十（にじゅう）= 20 超过99的数字 数字 100 1,000 10,000 10e^8 10e^12 漢字 百 千 万 億 兆 平假名 ひゃく せん まん おく ちょう 但是，有些情况下数字与表示位数的两次连读时会十分拗口，为了避免这些情况，有一些特例情况。 特例 数字 漢字 ひらがな 300 三百 さんびゃく 600 六百 ろっぴゃく 800 八百 はっぴゃく 3000 三千 さんぜん 8000 八千 はっせん 10e^12 一兆 いっちょう 示例： 四万三千七十六（よんまんさんぜんななじゅうろく）43,076 七億六百二十四万九千二百二十二（ななおくろっぴゃくにじゅうよんまんきゅうせんにひゃくにじゅうに）706,249,222 五百兆二万一（ごひゃくちょうにまんいち）500,000,000,020,001 比1小的数字 表示 0 .（小数点） -（负号） 发音 零（れい）/ ゼロ / マル 点（てん） マイナス 示例： ゼロ、点、ゼロ、ゼロ、二、一 = 0.0021 マイナス二十九 = - 29 4.16.2 年月日的表示用法公式：（年份） 数字 + 年（ねん） 用法公式：（月份） 数字 + 月（がつ） （例外）四月 - しがつ （例外）七月 - しちがつ （例外）九月 - くがつ 用法公式：（日期） 序号 汉字 读音 哪天 何日 なん・にち 1 一日 ついたち 2 二日 ふつ・か 3 三日 みっ・か 4 四日 よっ・か 5 五日 いつ・か 6 六日 むい・か 7 七日 なの・か 8 八日 よう・か 9 九日 ここの・か 10 十日 とお・か 11 十一日 じゅう・いち・にち 12 十二日 じゅう・に・にち 13 十三日 じゅう・さん・にち 14 十四日 じゅう・よっ・か 15 十五日 じゅう・ご・にち 16 十六日 じゅう・ろく・にち 17 十七日 じゅう・しち・にち 18 十八日 じゅう・はち・にち 19 十九日 じゅう・く・にち 20 二十日 はつ・か 21 二十一日 に・じゅう・いち・にち 22 二十二日 に・じゅう・に・にち 23 二十三日 に・じゅう・さん・にち 24 二十四日 に・じゅう・よっ・か 25 二十五日 に・じゅう・ご・にち 26 二十六日 に・じゅう・ろく・にち 27 二十七日 に・じゅう・しち・にち 28 二十八日 に・じゅう・はち・にち 29 二十九日 に・じゅう・く・にち 30 三十日 さん・じゅう・にち 31 三十一日 さん・じゅう・いち・にち 4.16.3 时间用法公式：（时针） 数字 + 時（じ）数字与「時」合用表示时针，即几点。 （例外）四点 - よじ （例外）七時 - しちじ （例外）九時 - くじ 用法公式：（分针） 数字 + 分（ふん）数字与「分」合用表示分针，即几分。 （例外）一分 - いっぷん （例外）三分 - さんぷん （例外）四分 - よんぷん （例外）六分 - ろっぷん （例外）八分 - はっぷん （例外）十分 - じゅっぷん 用法公式：（秒针） 数字 + 秒（びょう） 示例： 1時24分（いちじ・にじゅうよんぷん）1:24 午後4時10分（ごご・よじ・じゅっぷん）4:10 PM 午前9時16分（ごぜん・くじ・じゅうろっぷん）9:16 AM 13時16分（じゅうさんじ・じゅうろっぷん）13:16 2時18分13秒（にじ・じゅうはっぷん・じゅうさんびょう）2:18:13 4.16.4 时间跨度用法公式： 数字 + 時/日/週/年 + 間（かん）時/日/週/年与「かん」合用表示时间跨度。 数字 + 分/秒分/秒可以直接表示时间跨度，不需要加「かん」。 （特例）一日（いちにち） （特例）一週間（いっしゅうかん） （特例）八週間（はっしゅうかん） 示例： 二時間四十分（にじかん・よんじゅっぷん）2小时40分钟 二十日間（はつかかん）20天 十五日間（じゅうごにちかん）15天 二年間（にねんかん）两年 三週間（さんしゅうかん）三周 一日（いちにち）一天 用法公式： 数字 + ヶ月（かげつ）数字与「ヶ月」合用表示月份的时间跨度。*其中，「ヶ」与片假名「ケ」无关，而是这个量词对应的汉字「箇」的简写。 （特例）一ヶ月（いっかげつ） （特例）六ヶ月（ろっかげつ） （特例）十ヶ月（じゅっかげつ） 示例： 十一ヶ月（じゅういっかげつ）11 个月 二十ヶ月（にじゅっかげつ）20 个月 三十三ヶ月（さんじゅうさんかげつ）33 个月 4.16.5 其他量词量词的用法 量词 用法 人 用来数人 本 用来数⻓的或圆柱形的物体，例如瓶子或筷子 枚 用来数薄的物体，比如纸或是衬衫 冊 用来数带封面的物体，一般是书 匹 用来数小动物，例如阿猫阿狗 歳 用来数岁数 個 用来数小（通常是圆的）物体 回 用来数次数 ヶ所（箇所） 用来数地点 つ 用来数量词罕用、或没有量词的物体 量词的1-10 人 本 枚 冊 匹 歳 個 回 ヶ所（箇所） つ 1 ひとり いっぽん いちまい いっさつ いっぴき いっさい いっこ いっかい いっかしょ ひとつ 2 ふたり にほん にまい にさつ にひき にさい にこ にかい にかしょ ふたつ 3 さんにん さんぼん さんまい さんさつ さんびき さんさい さんこ さんかい さんかしょ みっつ 4 よにん よんほん よんまい よんさつ よんひき よんさい よんこ よんかい よんかしょ よっつ 5 ごにん ごほん ごまい ごさつ ごひき ごさい ごこ ごかい ごかしょ いつつ 6 ろくにん ろっぽん ろくまい ろくさつ ろっぴき ろくさい ろっこ ろっかい ろっかしょ むっつ 7 しちにん ななほん ななまい ななさつ ななひき ななさい ななこ ななかい ななかしょ ななつ 8 はちにん はちほん はちまい はっさつ はっぴき はっさい はっこ はちかい はっかしょ やっつ 9 きゅうにん きゅうほん きゅうまい きゅうさつ きゅうひき きゅうさい きゅうこ きゅうかい きゅうかしょ ここのつ 10 じゅうにん じゅっぽん じゅうまい じゅっさつ じゅっぴき じゅっさい じゅっこ じゅっかい じゅっかしょ とお 4.16.6 用「目」来表示顺序用法公式： 数字 + 量词 + 目（め）量词与「目」合用表示第几个。 示例： 二回目第二次 四人目第四个人","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 4.15 做出请求","slug":"日语语法 4.15 做出请求","date":"2020-04-23T13:45:06.000Z","updated":"2020-04-23T13:45:06.000Z","comments":true,"path":"2020/04/23/日语语法 4.15 做出请求/","link":"","permalink":"/2020/04/23/日语语法 4.15 做出请求/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 4 语法精要4.15 做出请求4.15.1 「〜ください」用于表示请求用法公式： 名词+を/动词て形 + ください「〜ください」表示对前面名词或动作的请求，是「くださる」的一种特殊活用形。*与 4.14.2 「くれる」的用法 中「くれる」用于请求帮助的区别是：「くれる」是以问句形式进行请求提问「可以帮我…吗？」，「ください」则是以类似祈使句的形式进行请求「请你帮我…」。 示例： それをください。请把那个给我。 それをくれる？可以把那个给我吗？ 示例： 漢字で書いてください。请用汉字写。 ゆっくり話してください。请慢慢讲。 落書きを書かないでください。请不要涂鸦。 ここにこないでください。请不要来这里。 4.15.2 口语里用「〜ちょうだい」发出请求用法公式： 名词+を/动词て形 + ちょうだい「〜ちょうだい」表示对前面名词或动作的请求，与「〜ください」用法相同。*「〜ちょうだい」在语气上有点女性化和孩子气，并且总是用片假名书写。写成汉字的时候，它一般用在非常正式的表达里面，例如「頂戴致します」。 示例： スプーンをちょうだい。请把调羹给我。 ここに名前を書いてちょうだい。请把名字写在这里。 4.15.3 用「〜なさい」做出礼貌而坚决的请求用法公式： 动词词根 + なさい动词词根与「〜なさい」合用表示礼貌而语气较为强硬的请求。动词词根详见 4.1.2 动词词根 。 示例： よく聞きなさい！听好了！ ここに座りなさい。坐这里。 口语中会去掉「なさい」中的「さい」。 示例： まだいっぱいあるから、たくさん食べな。还是有很多，请多吃点吧。 それでいいと思うなら、そうしなよ。觉得没问题的话，就放手去做吧。 4.15.4 命令形用法公式： （对る动词）る + ろ （对う动词）动词末尾假名う段 + え段 （例外）する→ しろ （例外）くる→ こい （例外）くれる→ くれ 示例る动词 字典形 命令形 ⻝べる ⻝べろ 着る 着ろ 信じる 信じろ 寝る 寝ろ 起きる 起きろ 出る 出ろ 掛ける 掛けろ 捨てる 捨てろ 示例う动词 字典形 命令形 話す 話せ 聞く 聞け 遊ぶ 遊べ 待つ 待て 飲む 飲め 直る 直れ 死ぬ 死ね 買う 買え 例外动词 字典形 命令形 する しろ くる こい くれる くれ 示例： 好きにしろ。随你自己喜欢。 あっち行け！走开！ 早く酒を持ってきてくれ。赶紧来，再带点酒。 4.15.5 否定命令用法公式： 动词辞书形 + な动词辞书形与「な」连用表示否定命令。 示例： それを⻝べるな！别吃那个！ 変なことを言うな！别说这种奇怪的话！","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 4.13 尝试或者试图做成某事","slug":"日语语法 4.13 尝试或者试图做成某事","date":"2020-04-14T15:27:30.000Z","updated":"2020-04-14T15:27:30.000Z","comments":true,"path":"2020/04/14/日语语法 4.13 尝试或者试图做成某事/","link":"","permalink":"/2020/04/14/日语语法 4.13 尝试或者试图做成某事/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 4 语法精要4.13 尝试或者试图做成[1]某事[1] 此处中文版翻译有误。 4.13.1 尝试某事用法公式： 动词て形 + みる表示尝试前面的动作。 示例： お好み焼きを初めて食べてみたけど、とてもおいしかった！我第一次试了煎菜饼，很好吃！ お酒を飲んでみましたが、すごく眠くなりました。我试了些酒，然后变得非常困。 新しいデパートに行ってみる。我要去看看新开的百货大楼。 広島のお好み焼きを食べてみたい！我想去试试广岛的菜煎饼！ 4.13.2 试图做成某事用法公式： 动词意向形 + とする表示试图做成某事。动词意向形用法见 4.10.3 用意向形来提议做某事。 示例： 毎日、勉強を避けようとする。她每天都试图逃避学习。 無理矢理に部屋に入ろうとしている。他试着强迫自己进房间。 早く寝ようとしたけど、結局は徹夜した。我试着早睡，但结果却失眠了。 お酒を飲もうとしたが、奥さんが止めた。他尝试喝酒，但他妻子制止了他。 还有其他动词可用来表示类似的意思，比如「決める」的意思是「决定做某事」。 示例： 勉強をなるべく避けようと思った。我想我会尽可能的逃避学习。 毎日ジムに行こうと決めた。决定每天去健身房。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 4.14 授受动词","slug":"日语语法 4.14 授受动词","date":"2020-04-14T15:08:20.000Z","updated":"2021-11-26T17:57:04.702Z","comments":true,"path":"2020/04/14/日语语法 4.14 授受动词/","link":"","permalink":"/2020/04/14/日语语法 4.14 授受动词/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 4 语法精要4.14 授受动词4.14.1 「あげる」的用法用法公式： 授予发起者 + が + 被授予对象 + に + 被授予物品 + を + あげる「あげる」用于表示授予发起者向被授予对象授予物品这个动作。一般认为授予发起者向被授予对象为内向外[1]的关系，或者说话者站在授予发起者的角度讲话。 帮助发起者 + が + 被帮助对象 + に + 动词て形 + あげる动词て形与「あげる」合用于表示帮助发起者会帮被帮助对象做某事。一般认为帮助发起者向被帮助对象为内向外的关系，或者说话者站在帮助发起者的角度讲话。 [1] 此处Tae Kim的语法书并没有加入有关内外向关系的详细说明。书中只提到了自己和别人的内外关系，没有涉及家人朋友这种关系由近到远的关系。实际上「あげる」是能够体现说话者对于动作双方的私人感情的。 内外关系图： 自己 家人 朋友 其他 内— —— —— —— —— —— —&gt;外 示例： 私が友達にプレゼントをあげた。我给了朋友礼物。 これは先生にあげる。要把这个给老师。 車を買ってあげるよ。我会帮你买车的哦。 代わりに行ってあげる。我会帮你顶替的。 如果是第三者使用这个动词，那说话者是站在授予发起者/帮助发起者的角度上说的。 示例： 学生がこれを先生にあげる。学生把这个给了老师。（站在学生的角度） 友達が父にいいことを教えてあげた。朋友帮忙教了我爸好东西。（站在朋友的角度）*个人认为例2的用法稍有不妥，但原文仅作为与后文「くれる」的用法做对比使用。 4.14.2 「くれる」的用法 授予发起者 + が + 被授予对象 + に + 被授予物品 + を + くれる「くれる」用于表示授予发起者向被授予对象授予物品这个动作。一般认为授予发起者向被授予对象为外向内的关系，或者说话者站在被授予对象的角度讲话。 帮助发起者 + が + 被帮助对象 + に + 动词て形 + くれる动词て形与「くれる」合用于表示帮助发起者会帮被帮助对象做某事。一般认为帮助发起者向被帮助对象为外向内的关系，或者说话者站在被帮助对象的角度讲话。 示例： 友達が私にプレゼントをくれた。朋友给了我礼物。 これは、先生がくれた。老师把这个给了我。 車を買ってくれるの？你会帮我买车吗？ 代わりに行ってくれる？能帮忙顶替我吗？ 第三者使用的时候，站在被授予对象/被帮助对象的角度说的。 示例： 先生がこれを学生にくれる。老师把这个给了学生（站在学生的角度）。 友達が父にいいことを教えてくれた。朋友帮忙教了我爸好东西。（站在父亲的角度） 有些时候，「あげる」和「くれる」会导致句意模棱两可。 示例： 先生が教えてあげるんですか。老师，你能不能帮忙教…？（除了说话者以外的任何人）*此处站在老师角度对其他人说，如果包括自己就不能用「あげる」。 先生が教えてくれるんですか。老师，你能不能帮忙教…？（任何人，包括说话者）*此处因为包括说话者，关系为外向内，故使用「くれる」。 4.14.3 「やる」的用法 授予发起者 + が + 被授予对象 + に + 被授予物品 + を + やる当被授予对象为动物时，可以用「やる」代替「あげる」。被授予对象为人时不能这么用。 示例： 犬に餌をやった？你喂这只狗了吗？ 4.14.4 「もらう」的用法用法公式： 被授予对象 + が + 授予发起者 + に/から + 被授予物品 + を + もらう「もらう」表示被授予对象接受了授予发起者授予的物品。 被帮助对象 + が + 帮助发起者 + に + 动词て形 + もらう动词て形与「もらう」表示被帮助对象接受了帮助发起者提供的某种动作的帮助。 示例： 私が友達にプレゼントをもらった。我从朋友那里收到了礼物。 友達からプレゼントをもらった。我从朋友那里收到了礼物。 これは友達に買ってもらった。这个是朋友帮忙买的。 宿題をチェックしてもらいたかったけど、時間がなくて無理だった。我想别人帮我检查作业，不过没时间了，不可能了。 4.14.5 用「くれる」或「もらえる」请人帮忙使用上文中的两种动词て形的帮助表达方法表示请求。 示例： あなたが、私に千円を貸してくれる？你能否帮忙借出1000日元给我? 私が、あなたに千円を貸してもらえる？我能收到你借出1000日元的帮忙吗? 在原用法公式下，「くれる」或「もらえる」否定式可以让请求的语气温柔些。 示例： ちょっと静かにしてくれない？你可以安静一点吗，不行吗？ 漢字で書いてもらえませんか。你能帮我用汉字写这个吗，不行吗？ 动词て形的否定式与「くれる」或「もらえる」合用可以表示请求别人不做什么。 示例： 全部⻝べないでくれますか。你能不能别全吃了？ 高い物を買わないでくれる？你能不能别买贵的东西？","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 4.12 围绕「いう」定义和描述","slug":"日语语法 4.12 围绕「いう」定义和描述","date":"2020-04-09T17:43:02.000Z","updated":"2020-04-09T17:43:02.000Z","comments":true,"path":"2020/04/10/日语语法 4.12 围绕「いう」定义和描述/","link":"","permalink":"/2020/04/10/日语语法 4.12 围绕「いう」定义和描述/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 4 语法精要4.12 围绕「いう」定义和描述4.12.1 用「いう」来定义用法公式： 句子 + と + いいます此处「と + いいます」用来指示前面的句子带有定义或者说是描述色彩。 物1 + と + いう + 物2此处「と + いう」用来指示物1是为了形容或者说是定义物2的。可译为「物1这样的物2」。 示例： これは、なんという魚ですか。这是什么样的⻥？ この魚は、鯛といいます。这种⻥被称为「鲷」。 ルミネというデパートはどこにあるか、知っていますか？你知道叫”Lumine” 的百货大楼在哪里吗？ 「友達」は、英語で「friend」という意味です。「友達」英语里的意思是”friend”。 4.12.2 用「いう」来描述抽象事物用法公式： 句子/事物 + と + いう表示像前面所说的句子/事物。 示例： 主人公が犯人だったというのが一番面白かった。有趣的是，主人公就是那个罪犯。*个人理解「が」一般强调后面，故在中文语序中靠前。 日本人はお酒に弱いというのは本当？日本人真的不能喝酒吗？ 独身だというのは、嘘だったの？你单身的说法是说谎的？ リブートというのは、パソコンを再起動するということです。重启的意思是重新启动你的电脑。 我们还可以用「こう」、「そう」、「ああ」和「どう」跟「いう」合用，表示「这样」、「那样」、「那样（远）」和「怎样」。 示例： あんたは、いつもこういう時に来るんだから、困るんだよ。你总是在这种时候来，这让我很为难。 そういう人と一緒に仕事をするのは、嫌だよね。跟那种人一起工作，不喜欢对吧？ ああいう人と結婚できたら、幸せになれると思います。我想如果你跟那种人结婚的话会变得幸福的。 大学に行かないって、どういう意味なの？你说「你不去上大学？」是什么意思？ 4.12.3 用「というか」改述和总结我们可以把提问词「か」加到「という」后面来提问。这种结构用在你想改述或重定义什么东西的时候，可译为「也就是说」。 示例： お酒は好きというか、ないと生きていけない。我喜欢喝酒，甚至可以说，没酒活不下去。 多分行かないと思う。というか、お金がないから、行けない。我觉得大概走不了了吧。也可以说，是因为没钱我走不了。 というか、もう帰らないとだめですけど。也就是说，不回去已经不行了啊。*此处的「けど」应为缓和语气的用法。 与其用「か」来改述别人的结论，我们也可以只用「こと」来做总结，而不是改述什么 [1] 。 [1] 个人感觉此处使用「か」和「こと」并没有很大意思上的区别，都可以翻译为「也就是说」，或者是「换句话说」。理解为总结或者是委婉改述都可以。 示例：A：みきちゃんが洋介と別れたんだって。A：我听说Miki-chan 跟洋介分手了。B：ということは、みきちゃんは、今彼氏がいないということ？B：意思是Miki-chan 现在没有男朋友了？A：そう。そういうこと。A：是的，是这个意思。 4.12.4 用「って」或「て」替代「という」用法公式： 句子/事物 + という+助词 + って「って」可用于替代「という」并省略后面的助词。用它的时候通常上下文意思很明确，即在对话环境中出现。 示例： 来年留学するというのは、智子のこと？明年留学那件事，是说智子吗？ 来年留学するって智子のこと？明年留学那件事，是说智子吗？ 「だって」意思是「即便如此」。 示例： A：しないとだめだよ。A：不得不做啊。B：だって、時間がないからできないよ。B：但没时间了，做不了。 A：行かなくてもいいよ。A：不是一定要去的。B：だって、みんな行くって。私も行かないと。B：但别人都说要去。我也得去。 有时候，小「つ」也会被省略掉，「って」就变成了「て」。 示例: てことは、みきちゃんは、今彼氏がいないてこと？意思是Miki-chan 现在没有男朋友？ ていうか、もう帰らないとだめですけど。也就是说，不回去已经不行了啊。。 不过在引述别人说出来的话的时候通常不用「て」。 示例： みきちゃんが、明日こないて。Miki-chan 明天不来了。（对于已经说出的话不能用「て」。如果用「て」，例句1似乎会出现「请不要来」的歧义？） みきちゃんが、明日こないって。Miki-chan 说她明天不来了。 4.12.5 用「ゆう」替代「いう」用法公式： 句子/事物 + と + いう + ゆう「ゆう」在口语中可用于替代「いう」。 示例： てゆうか、もう帰らないとだめですけど。其实我应该已经往家走了。 そうゆうことじゃないって！我说了不是那样的！","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 4.11 将关系从句作为宾语","slug":"日语语法 4.11 将关系从句作为宾语","date":"2020-04-09T17:33:14.000Z","updated":"2020-04-09T17:33:14.000Z","comments":true,"path":"2020/04/10/日语语法 4.11 将关系从句作为宾语/","link":"","permalink":"/2020/04/10/日语语法 4.11 将关系从句作为宾语/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 4 语法精要4.11 将关系从句作为宾语4.11.1 直接引用用法公式： 「直接引用语」 + と + 动词「と」用来提示前方为引用语。 示例： アリスが、「寒い」と言った。Alice 说「冷」。 「今日は授業がない」と先生から聞いたんだけど。我听老师说「今天没课」。 「寒い」とアリスが田中に言った。「冷」，Alice 对田中说。 4.11.2 转述引用用法公式： 非直接引用语 + と + 动词「と」用来提示前方为引用语。 示例： 先生から今日は授業がないと聞いたんだけど。我听老师说今天没课。 これは、日本語で何と言いますか。这个用日语怎么说？ 私は、アリスと言います。别人叫我Alice。 カレーを⻝べようと思ったけど、⻝べる時間がなかった。我想过出去吃咖喱，但是没有时间去吃。 今、どこに行こうかと考えている。现在我在想该去哪里。 彼は、これは何だと言いました。他说这是什么? 彼は高校生だと聞いたけど、信じられない。我听说他是个高中生，但我没法相信。 4.11.3 「と」的口语版本：「って」用法公式： 引用语 + って「って」用来提示前方为引用语，这种情况可以省略后面的动词。 示例： 智子は来年、海外に行くんだって。智子说她明年要出国。 もうお金がないって。我已经告诉你我没钱了。 え？何だって？嗯？你说什么来着？ 今、時間がないって聞いたんだけど、本当？我听说你现在没时间，是吗？ 今、時間がないって、本当？你现在没时间（听说），对吧？ 「って」基本上可以用来谈论任何事情，不仅仅是引用别人的话。日常对话中，它是用来代替「は」以提出一个新的话题。 示例： 明日って、雨が降るんだって。关于明天，我听说要下雨。 アリスって、すごくいい人でしょ？关于Alice，她是个很好的人，对吧？","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 4.10 表达希望和建议","slug":"日语语法 4.10 表达希望和建议","date":"2020-04-09T17:32:30.000Z","updated":"2021-11-26T17:55:46.734Z","comments":true,"path":"2020/04/10/日语语法 4.10 表达希望和建议/","link":"","permalink":"/2020/04/10/日语语法 4.10 表达希望和建议/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 4 语法精要4.10 表达希望和建议4.10.1 用「たい」表达你想做的事用法公式：「たい」形活用规则 肯定 否定 字典形 行きたい 行きたくない 过去形 行きたかった 行きたくなかった 示例： 何をしたいですか。你想做什么？ 温泉に行きたい。我想去泡温泉。 ケーキ、食べたくないの？你不想吃蛋糕？ 食べたくなかったけど食べたくなった。我过去不想吃，但后来变得想吃了。*组合「〜たくない」和「なる」的过去形：「⻝べたくなくなった」，意思是「（过去）变得不想吃」。 4.10.2 用「欲しい」表示你想要的东西，或想要完成的事用法公式： 想要的东西 + が + 欲しい表示想要的东西。 动词的て形 + 欲しい表示想做的事情。 示例： 大きい縫いぐるみが欲しい！我想要个大公仔！ 全部食べてほしいんだけど・・・。想全部被吃掉，不过… 部屋をきれいにしてほしいのよ。我想要这房间被打扫干净哦。 4.10.3 用意向形来提议做某事用法公式： （る动词）る + よう去掉「る」再加上「よう」表示る动词的意向形。 （う动词）る + ろ + う把末尾假名从う段改为同行お段，再加上「う」表示う动词的意向形。 示例： 今日は何をしようか？今天做点什么？ テーマパークに行こう！去主题公园吧！ 明日は何を食べようか？明天吃什么？ カレーを食べよう！吃咖喱吧！ 用法公式： ~ます + しょう动词ます形去掉「す」加上「しょう」表示丁宁语的意向形。 示例： 今日は何をしましょうか？今天做点什么？ テーマパークに行きましょう！去主题公园吧！ 明日は何を食べましょうか？明天吃什么？ カレーを食べましょう！吃咖喱吧！ 4.10.4 用条件语「ば」或「たら」加上「どう」提建议用法公式： 条件 + ば/たら + どう用来提出建议，直译可理解为询问对方对条件的看法。 示例： 銀行に行ったらどうですか。去银行如何？ たまにご両親と話せばどう？偶尔跟你父⺟聊聊如何？","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 4.9 表达「必须」或「不得不」","slug":"日语语法 4.9 表达「必须」或「不得不」","date":"2020-04-04T12:11:04.000Z","updated":"2020-04-04T12:11:04.000Z","comments":true,"path":"2020/04/04/日语语法 4.9 表达「必须」或「不得不」/","link":"","permalink":"/2020/04/04/日语语法 4.9 表达「必须」或「不得不」/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 4 语法精要4.9 表达「必须」或「不得不」4.9.1 用「だめ」、「いけない」和「ならない」表示不能做的事用法公式： 动词て形 + は + だめ/いけない/ならない表示不能做。 示例： ここに入ってはいけません。你不能进到这里。 それを食べてはだめ！你不能吃那个！ 夜、遅くまで電話してはならない。不到深夜不能用电话。 早く寝てはなりませんでした。不被允许早睡。 「だめ」、「いけない」和「ならない」的区别在于「だめ」比较口语化，「いけない」和「ならない」基本上一样，但「ならない」更常用在那些适用于众人的要求，例如政策和规定。 4.9.2 表达必须做的事用法公式： 否定式的て形 + は + だめ∕いけない∕ならない表示必须做。 动词否定式+ 表示条件的「と」+ だめ∕いけない∕ならない使用了4.8.1中的「と」条件语表示自然结果，语气较平和。 动词否定式（去掉い）+ 表示条件的「ば」+ だめ∕いけない∕ならない注意因为动词是否定式的，所以用条件语「ば」之前要把「い」去掉再加上「ければ」。 示例： 毎日学校に行かなくてはなりません。不每天上学是不行的。 宿題をしなくてはいけなかった不做作业是不行的。 毎日学校に行かないとだめです。不每天上学，不可以。 宿題をしないといけない。不做作业，不可以。 毎日学校に行かなければいけません。不每天上学的话是不行的。 宿題をしなければだめだった。不做作业的话是不行的。 4.9.3 偷懒的说法用法公式： ては/では + ちゃ/じゃ + だめ∕いけない∕ならない将动词て形中的「ては/では」换成「ちゃ/じゃ」，表示相同的意思。 示例： ここに入っちゃだめだよ。你不能进这里。 死んじゃだめだよ！你不能死！ 用法公式： なくて + なくちゃ将否定式的て形中的「なくて」换成「なくちゃ」表示对后面否定词的省略。 动词否定式 + と条件语「と」本身也可以用来表示对后面否定词的省略。 なければ + なきゃ将条件句否定式中的「なければ」换成「なきゃ」表示对后面否定词的省略。 示例： 勉強しなくちゃ。不学习是不行的。 学校に行かないと。不去上学，不行。 ご飯を⻝べなきゃ。不吃饭的话是不行的。 最后说明一点，总体来说「ちゃ」听起来与其比较和蔼，就像「ちゃん」一样。 4.9.4 表达可以做或可以不做什么用法公式： 动词て形/动词て形否定式 + ても + いい/大丈夫/構わない表示即使做什么或者不做什么是可以的。口语中，「〜てもいい」有时候还省略成「〜ていい」。 示例： 全部⻝べてもいいよ。你可以放心全吃了。（字面：即使你全吃了，也可以哦） 全部⻝べなくてもいいよ。你不用全吃了。（字面：即使你不全吃掉，也可以哦） 全部飲んでも大丈夫だよ。全喝了吧没事。（字面：即使你全喝了，也没问题哦） 全部飲んでも構わないよ。我不介意你全喝掉。（字面：即使你全喝了，我也不介意哦） 示例：「も」的省略 もう帰っていい？我是不是已经可以回家了？ これ、ちょっと見ていい？能否让我稍微看一眼这个？","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 4.8 条件语","slug":"日语语法 4.8 条件语","date":"2020-04-04T12:09:55.000Z","updated":"2021-11-26T17:54:51.502Z","comments":true,"path":"2020/04/04/日语语法 4.8 条件语/","link":"","permalink":"/2020/04/04/日语语法 4.8 条件语/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 4 语法精要4.8 条件语4.8.1 用「と」表达自然的结果用法公式： 动词 + と + 结果「と」表示前面的动作自然会产生后面的结果。 状态表示（名词/な形容词） + だと + 结果「と」表示前面的状态自然会产生后面的结果。 示例： ボールを落すと落ちる。如果扔一个球，它会掉下。 電気を消すと暗くなる。如果关了灯，会变暗。 学校に行かないと友達と会えないよ。如果你不去学校，你就⻅不到朋友们了哦。 たくさん⻝べると太るよ。如果你吃很多，你就会变胖哦。 先生だと、きっと年上なんじゃないですか？如果他是老师，那他一定年纪比较大，不是吗？ 4.8.2 用「なら（ば）」表示前提条件用法公式： 假设会发生的前提条件 + なら（ば）+ 结果「なら」表示前面的前提条件会产生后面的结果[1]。 [1] 该用法个人感觉，有种前提条件未发生则结果不会发生的感觉。而4.8.1中，用「と」表达自然的结果则感觉没有其他语气，比较自然。 你也可以用「ならば」替代「なら」，两者意思完全一样，但「ならば」让人感觉更正式。 示例： みんなが行くなら私も行く。如果大家都去的话我也去。 アリスさんが言うなら問題ないよ。如果Alice都这么说的话，那没问题。 4.8.3 一般性条件语「ば」用法公式： （对动词）（る动词）⻝べる→ ⻝べれ→ ⻝べれば（う动词）待つ→ 待て→ 待てば把最后假名从う段改为同行え段，再加上「ば」，表示一般性条件。 （对い形容词以及「ない」结尾的否定式）おかしい→ おかしければ把最后的「い」换成「ければ」，表示一般性条件。 （对名词和な形容词）名词/な形容词 + であれば名词或な形容词加「であれば」，表示一般性条件。 示例： 友達に会えれば、買い物に行きます。要是能⻅到我的朋友们，我们就去逛街。 お金があればいいね。我要是有钱就好了，哈？ 楽しければ、私も行く。要是有意思我也去。 楽しくなければ、私も行かない。要是没意思我也不去。 ⻝べなければ病気になるよ。你要是不吃就会生病。 4.8.4 过去形条件语「たら（ば）」用法公式： 名词、形容词或动词的过去形 + ら名词、形容词或动词的过去形加「ら」，表示过去形条件语。 示例： 暇だったら、遊びに行くよ。如果我有空，我就去玩。 学生だったら、学生割引で買えます。如果你是学生，就可以享受学生折扣。 对于い形容词和动词来说，这两种条件语很难区分开来，图省事的话你可以认为他们是一样的。不过二者之间有一个小区别，那就是「たら」的重点在于满足条件后发生的事，这也是我叫它过去形条件语的原因，因为假设条件已经发生了，我们感兴趣的是结果。而「ば」则侧重在条件那一侧。 示例： A：友達に会えれば、買い物に行きます。A：我和朋友们会去逛街，如果我能⻅到他们。B：友達に会えたら、買い物に行きます。B：如果我能⻅到朋友们的话，我们就去逛街。 A：お金があればいいね。A：情况会很好，如果我有钱，对吧？B：お金があったらいいね。B：如果我有钱的话，那情况会很好，对吧？ 从上面两组例子的场景来看，用「〜たら」显得更自然，因为我们貌似不关心条件本身。我们更感兴趣的，应该是⻅到朋友后会做什么，或者有了钱会变怎样。 过去形条件语是唯一可以用过去形结果的用法。这听起来也许很奇怪：如果结果已经发生了，还谈何「如果」？实际上这种用法里面就已经没有「如果」了，这只是对某种条件下发生的结果表示惊讶，跟条件没啥关系，不过因为语法一样，所以在这里一并介绍。 示例： 家に帰ったら、誰もいなかった。我回到家的时候，一个人也没有（意外的结果）。 アメリカに行ったら、たくさん太りました。去了美国的结果是，我胖了（意外的结果）。 你也可以用「たらば」替代「たら」。类似「ならば」，二者意思完全相同，只是「たらば」感觉更正式。 4.8.5 不确定假设条件词「もし」用法公式： もし + 条件、结果使用「もし」，表示对后面的条件发生的可能不确定。 示例： もしよかったら、映画を観に行きますか？如果没问题的话，一起去看电影？ もし時間がないなら、明日でもいいよ。如果没时间的话，明天也可以（不确定是否有时间）。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 4.7 合用する、なる和助词に","slug":"日语语法 4.7 合用する、なる和助词に","date":"2020-03-27T17:07:30.000Z","updated":"2020-03-27T17:07:30.000Z","comments":true,"path":"2020/03/28/日语语法 4.7 合用する、なる和助词に/","link":"","permalink":"/2020/03/28/日语语法 4.7 合用する、なる和助词に/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 4 语法精要4.7 合用する、なる和助词に4.7.1 与名词以及な形容词合用「なる」和「する」用法公式： 名词/な形容词 + に + なる表示向某个方向改变。在 3.8.2 目标助词「に」 曾经有过相关说明。 名词 + に + する表示决定，也可以表示当作。 示例： 彼の日本語が上手になった。他的日语变得越来越好了。 私は医者になった。我成了一个医生。 示例： 私は、ハンバーガーとサラダにします。我打算要汉堡和沙拉。 他にいいものがたくさんあるけど、やっぱりこれにする。好东西还有很多，但对我来说，还是打算要这个。 本を枕にして昼寝した。把书当枕头睡了午觉。 その话は闻かなかったことにしましょう。这句话就当我没听见吧。 4.7.2 与い形容词合用「なる」用法公式： い形容词的否定形去掉ない + なる去掉い形容词否定形后的「ない」，加上「なる」，表示向い形容词方向改变。 示例： 去年から背が高くなったね。你比去年高多了哈？ 運動しているから、強くなる。因为锻炼，我会变得更强壮。 勉強をたくさんしたから、頭がよくなった。因为我学了很多，我变得更聪明了 4.7.3 合用动词[1]与「なる」和「する」[1] 此处的动词其实是动词的名词化形式，故名词化后用法实际上与4.7.1中的用法一致。 用法公式： 辞书形动词 + こと（事）/よう（様） + なる表示向某个方向改变， 辞书形动词 + こと/よう 可以视为名词从句。 辞书形动词 + こと/よう + する表示决定。 示例： 海外に行くことになった。变成了要去国外这件事。 毎日、肉を⻝べるようになった。变成了每天吃肉的样子。 海外に行くことにした。决定了要去国外这件事。 毎日、肉を⻝べるようにする。决定了每天吃肉的样子。 因为可能形动词描述的是可能性而非具体动作（记住，所以不能跟助词「を」一起用），所以它常常跟「〜ようになる」一起用，来描述变为某种状态的可能性。趁这个机会我们来复习一下。 示例： 日本に来て、寿司が食べられるようになった。来日本以后，我变得能吃寿司了。 一年間練習したから、ピアノがけるようになった。因为练习了一年，我会弹钢琴了。 地下に入って、富士山が見えなくなった。进入地下以后，富士山便看不到了。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 4.6 可能形","slug":"日语语法 4.6 可能形","date":"2020-03-27T17:06:42.000Z","updated":"2020-03-27T17:06:42.000Z","comments":true,"path":"2020/03/28/日语语法 4.6 可能形/","link":"","permalink":"/2020/03/28/日语语法 4.6 可能形/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 4 语法精要4.6 可能形4.6.1 可能形用法公式： （る动词）見る→ 見られる对る动词，直接将「る」替换为为「られる」变为可能形。 （う动词）遊ぶ→ 遊べ→ 遊べる对う动词，将最后的假名从う段改为同行え段假名，再加上「る」。 （例外）する→できるくる→こられる る动词的可能形 字典形 可能形 ⻝べる ⻝べられる 着る 着られる 信じる 信じられる 寝る 寝られる 起きる 起きられる 出る 出られる 掛ける 掛けられる 調べる 調べられる う动词的可能形 字典形 可能形 話す 話せる 書く 書ける 遊ぶ 遊べる 待つ 待てる 飲む 飲める 取る 取れる 死ぬ 死ねる 買う 買える 例外 字典形 可能形 する できる くる こられる 对于る动词，也可以把「られる」简写为「れる」。例如，「⻝べる」的可能形可以是「⻝べれる」，这是「⻝べられる」的简写。 示例： 漢字は書けますか？你会写汉字吗？ 残念だが、今週末は行けない。不幸的是，我这周末不能去了。 もう信じられない。我已经不相信了。 4.6.2 可能形动词不会有直接对象可能形表达的是可能性，但实际的动作并没有发生。所以尽管可能形动词仍然是动词，但它描述的是状态，所以你不能用直接对象助词「を」。 示例： 富士山 を が 登れた。曾经可以爬富士山。 重い荷物 を は 持てます。可以拿重的行李。 4.6.3 「見える」和「聞こえる」与普通可能形变形的区别「見える」和「聞こえる」这两个单词的意思分别表示某物是可⻅的、可听⻅的。如果你想说你可以看到或者听到什么，就得用这个两个词。但是，如果你想说的是你被赋予看⻅或听⻅某物的 机会 的话，就得用普通的可能形，不过这种情况下，一般会用示例3的表达方式。 示例： 今日は晴れて、富士山が見える。今天放晴了，富士山都能看得⻅。 友達のおかげで、映画はただで見られた。多亏了朋友，才有机会免费看了电影。 友達のおかげで、映画をただで見ることができた。多亏了朋友，才有机会免费看了电影。 示例： 久しぶりに彼の声が聞けた。这么久以来我第一次有机会能听到他的声音。 周りがうるさくて、彼が言っていることがあんまり聞こえなかった。周围很吵，他说什么我听得不是很清楚。 4.6.4 「ある」的可能形如果想说某物可能存在，你可以把「ある」和动词「得る」组合起来得到「あり得る」，这本质上的意思是「あることができる」。这个动词既可以读作「ありうる」，也可以读作「ありえる」。但是其活用形例如「ありえない」、「ありえた」和「ありえなかった」都只有一种读法，中间是「え」。 示例 そんなことはありうる。那种事情是可能的。 そんなことはありえる。那种事情是可能的。 そんなことはありえない。那种事情是不可能的。 彼が寝坊したこともありうるね。也可能是他睡过头了。 それは、ありえない話だよ。那是不可能的事。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 4.4 复合句","slug":"日语语法 4.4 复合句","date":"2020-03-24T15:36:12.000Z","updated":"2020-03-24T15:36:12.000Z","comments":true,"path":"2020/03/24/日语语法 4.4 复合句/","link":"","permalink":"/2020/03/24/日语语法 4.4 复合句/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 4 语法精要4.4 复合句4.4.1 表达一系列的状态用法公式： （名词和な形容词）名词/な形容词 + で用于连接多个名词/な形容词的状态。 （い形容词、否定式名词或形容词） い + くて替换い形容词、否定式名词或形容词末尾的「い」为「くて」*我认为此处之所以い形容词、否定式名词或形容词的变换方法相同，是因为可以将名词和な形容词的否定「じゃない」形视为特殊的い形容词。 （特殊）いい→よくて 示例： 私の部屋は、きれいで、静かで、とても好き。我的房间干净、漂亮，我非常喜欢。 彼女は、学生じゃなくて、先生だ。她不是学生，她是老师。 田中さんは、お金持ちで、かっこよくて、魅力的ですね。田中桑是一个富有、帅气且有魅力的人，不是吗？ 4.4.2 用て形连接一串动词用法公式：（动词的て形变换） （肯定） た + て/だ + で把动词活用成过去形，然后把「た」换成「て」，或把「だ」换成「で」。这通常被称为て形——虽然也可能是で。 （否定） い + くて 活用示例 过去形 て形 否定式 て形 ⻝べた ⻝べて ⻝べない ⻝べなくて 行った 行って 行かない 行かなくて した して しないし なくて 遊んだ 遊んで 遊ばない 遊ばなくて 飲んだ 飲んで 飲まない 飲まなくて 示例: ⻝堂に行って、昼ご飯を食べて、昼寝をする。我要去餐厅，吃午饭，然后午睡。 ⻝堂に行って、昼ご飯を食べて、昼寝をした。我去了餐厅，吃了午饭，睡了午觉。 時間がありまして、映画を見ました。还有时间，我去看了电影。 4.4.3 用「から」和「ので」表达原因或因果关系用法公式： 原因句（以非名词/な形容词结尾） + から + 结果句使用「から」连接两个句子指示前句为后句的原因。 名词/な形容词 + だ + から在「から」前加上「だ」表示前面的名词/な形容词为原因。「だ」用来避免和「から」from的意思混淆。 示例： 時間がなかったからパーティーに行きませんでした。没时间了，所以我没去聚会。 友達からプレゼントが来た。礼物来自朋友。 友達だからプレゼントが来た。因为是朋友，所以收到礼物。 有上下文的时候，原因和结果都可以省略掉。另外如果是在用丁宁语的话，应该把「から」当成普通名词，在后面加「です」。 示例： 田中さん：どうしてパーティーに行きませんでしたか。田中桑：你为什么没去聚会？山田さん：時間がなかったからです。山田桑：因为我没时间。 一郎：パーティーに行かなかったの？一郎：你没去聚会？直子：うん、時間がなかったから。直子：嗯，因为我没时间。 直子：時間がなかった。直子：我没时间。一郎：だからパーティーに行かなかったの？一郎：所以你没去聚会？ 实际上，「ので」基本上跟「から」可以互换使用，除了一些微妙的区别。「から」显式的表明了前面的句子是原因，而「ので」这仅仅是把两个句子连了起来，然后把前一个句子变味了解释的口吻。我称之为因果关系，即发生了[X]，导致发生了[Y]。这跟「から」有点不同，用后者的时候，我们声明发生了[Y] 的直接原因是发生了[X]。这种区别使得「ので」听起来口气更柔和，也更礼貌，所以在解释做了失礼之事的原因的时候，人们更倾向于用它而非「から」。 用法公式： 原因句（以非名词/な形容词结尾） + ので + 结果句使用「ので」连接两个句子指示前句为后句的原因，其语气要轻于「から」。 名词/な形容词 + な + ので解释语气的「の」前面如果是字典形名词或者な形容词的话，一定要加「な」。详情见 3.11.4 助词「の」用作解释。 示例： 私は学生なので、お金がないんです。我是学生，所以我没有钱。（字面意思：没有钱） ここは静かなので、とても穏やかです。因为安静，所以这里很平静。 なので、友達に会う時間がない。那就是没时间⻅朋友的原因。 如同解释口气的「の」可以简写成「ん」一样，说话的时候「ので」也可以改成「んで」，因为这样可以省略音节/ o / 的发音，形成连读。详情见 3.11.4 助词「の」用作解释 示例： 時間がなかったんでパーティーに行かなかった。因为没时间，所以没去聚会。 ここは静かなんで、とても穏やかです。这里安静，所以很平静。 なんで、友達に会う時間がない。那就是没时间⻅朋友的原因。 4.4.4 用「のに」表达「尽管」用法公式： 名词+な/な形容词+な/第一句 + のに + 第二句「のに」连接前后句表示：尽管第一句，但还是第二句。 示例： 毎日運動したのに、全然痩せなかった。明明每天都运动，我还是没瘦下来。 学生なのに、彼女は勉強しない。明明是学生，她却不学习。*此处，我认为将「のに」译为「明明」而非「尽管」更符合中-日的语言映射关系。 4.4.5 用「が」和「けど」表达转折用法公式： 名词+だ/な形容词+だ/第一句 + が/けど + 第二句「が」和「けど」表示语义的转折。「が」的语气要轻于「けど」。 示例： デパートに行きましたが、何も欲しくなかったです。我去了百货，但却没有我想要的东西。 友達に聞いたけど、知らなかった。我问了一个朋友，但他不知道。 今日は暇だけど、明日は忙しい。我今天有空，但明天忙。 だけど、彼がまだ好きなの。即便如此，还是喜欢他。（解释的、女性口吻） 用法公式： 名词+だ/な形容词+だ/第一句 + が/けど + 第二句「が」和「けど」用来连接句子。 示例： デパートに行きましたが、いい物がたくさんありました。我去了百货，那里有很多好东西。 マトリックスを見たけど、面白かった。我看了《黑客帝国》，有意思。 4.4.6 用「し」表示多种原因用法公式： 名词+だ/な形容词+だ/原因 + し、「し」用于连接多个原因。 示例： Ａ：どうして友達じゃないんですか？A：为什么（他/她）不是朋友？Ｂ：先生だし、年上だし・・・。B：这个，他/她是老师，比我年纪大…… Ａ：どうして彼が好きなの？A：（你）为什么喜欢他？Ｂ：優しいし、かっこいいし、面白いから。B：因为他人好，有魅力，还有趣（等等）。*注意你也可以说「優しくて、かっこよくて、面白いから。」，但就像助词「と」和「や」的区别一样，「し」暗示可能还有其他原因。 4.4.7 「〜たり」表达多个动作或状态用法公式： 动词过去形/状态表示的过去形 + り「〜たり」中的「た」包含在过去形中与「り」相连，用于连接多个动作或状态。 示例： 映画を見たり、本を読んだり、昼寝したりする。我做一些事，例如看电影、看书和午睡（等等）。 この大学の授業は簡単だったり、難しかったりする。这所大学的课程有时候简单，有时候难（可能还有其他情况）。 用「〜たり」连接多个动作时，末尾的活用可以用于表示语态[1]。 [1] 在《大家的日本语》中，上文的示例1表述为 映画を見たり、本を読みます。 下文的活用在《大家的日本语》中末尾也不包含「〜たり」，而是直接的敬语活用状态。 示例： 映画を見たり、本を読んだりした。我做了一些事，例如看电影、看书和午睡（等等）。 映画を見たり、本を読んだりしない。我不做一些事，例如看电影、看书和午睡（等等）。 映画を見たり、本を読んだりしなかった。我没做一些事，例如看电影、看书和午睡（等等）。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 4.5 て形的其他用法","slug":"日语语法 4.5 て形的其他用法","date":"2020-03-24T15:34:54.000Z","updated":"2020-03-24T15:34:54.000Z","comments":true,"path":"2020/03/24/日语语法 4.5 て形的其他用法/","link":"","permalink":"/2020/03/24/日语语法 4.5 て形的其他用法/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 4 语法精要4.5 て形的其他用法4.5.1 使用「〜ている」表示动作进行时用法公式： 动词て形 + いる动词的て形与「いる」连用表示动作正在进行。 示例： Ａ：友達は何をしているの？A：朋友在做什么？Ｂ：昼ご飯を⻝べている。B：（朋友）在吃午饭。 Ａ：何を読んでいる？A：你在读什么？Ｂ：教科書を読んでいます。B：我在读课本。 Ａ：話を聞いていますか。A：你在听我说话吗？Ｂ：ううん、聞いていない。B：不，我没在听。 用法公式：「いる」的活用 肯定 否定 非过去形 読んでいる 在读 読んでいない 不在读 过去形 読んでいた 当时在读 読んでいなかった 当时不在读 4.5.2 持续的状态对比进行中的动作「〜ている」也可以用来表示持续的状态。 示例： 今日、知りました。我今天才知道。 この歌を知っていますか？知道这首歌吗？ 道は分かりますか。知道路吗？ はい、はい、分かった、分かった。嗯，嗯，我明白了，明白了。 4.5.3 用「〜てある」表示结果的状态 动词て形 + ある动词的て形与「ある」连用表示动作结果的状态，可以理解为完成时。 示例： Ａ：準備はどうですか。A：准备的如何了？Ｂ：準備は、もうしてあるよ。B：已经准备好了。 Ａ：旅行の計画は終わった？A：旅行计划做好了吗？Ｂ：うん、切符を買ったし、ホテルの予約もしてある。B：嗯，不光买好了票，还订好了宾馆。 4.5.4 用「〜ておく」形表示为未来做准备 动词て形 + おく动词的て形与「おく」连用表示为未来做准备。 示例： 晩ご飯を作っておく。做晚饭（准备吃）。 電池を買っておきます。我要买些电池（以后用）。 「ておく」有些时候也简写成「〜とく」。 示例： 晩ご飯を作っとく。做晚饭（准备吃）。 電池を買っときます。我要买些电池（以后用）。 4.5.5 将动作动词（行く、来る）与て形合用用法公式： 动词て形 + 行く/来る动词的て形与「行く」/「来る」连用表示为动作的去/来方向。 示例： えんぴつを持っている？有铅笔吗？ 鉛筆を学校へ持っていく？要带铅笔去学校吗？ 鉛筆を家に持ってくる？要带铅笔回家吗？ 用法公式： 动词て形 + 行く/来る动词的て形与「行く」/「来る」连用表示时间的延续或者终止，即表示未来趋势或迄今为止。 示例： 冬に入って、コートを着ている人が増えていきます。入冬以后，穿大衣的人将会增加。 一生懸命、頑張っていく！将会尽全力的！ 色々な人と付き合ってきたけど、いい人はまだ見つからない。迄今为止跟各色人等出去过，但却未遇⻅过一个好人。 日本語をずっと前から勉強してきて、結局はやめた。很早以前就开始学习日语，但最终放弃了。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 4.3 提问词","slug":"日语语法 4.3 提问词","date":"2020-03-21T11:11:54.000Z","updated":"2020-03-21T11:11:54.000Z","comments":true,"path":"2020/03/21/日语语法 4.3 提问词/","link":"","permalink":"/2020/03/21/日语语法 4.3 提问词/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 4 语法精要4.3 提问词4.3.1 用丁宁语提问用法公式： 非丁宁语陈述句 + です + か。「です」表示丁宁语的状态，「か」为疑问词，后面不需要问号。 动词「ます」形的否定状态 + か表示反问。 示例： 田中さん：お⺟さんはどこですか。田中桑：（你）⺟亲在哪里？鈴木さん：⺟は買い物に行きました。铃木桑：（我）妈去买东西了。 キムさん：イタリア料理を⻝べに行きませんか。Kim 桑：不去吃意大利菜吗？鈴木さん：すみません。ちょっと、お腹がいっぱいです。铃木桑：对不起，（我的）肚子有点饱。 4.3.2 口语中的提问词提问词「か」用在口语里面的时候一般不是用来提问，而通常用来表示对某件事真假的不确定。根据不同的上下文，它也可以用作反问或表示讽刺。 示例： こんなのを本当に⻝べるか？这种东西真的可以吃吗？ そんなのは、あるかよ！有那种事吗? 口语里真正的提问不是用「か」，而是用表解释的助词の或直接用声调来表示的。 示例： こんなのを本当に⻝べる？这种东西真的可以吃吗？ そんなのは、あるの？有那种事吗? 4.3.3 用在关系从句里的「か」用法公式： 完整句子 + か + 动词我认为这可以理解为条件状语从句。 示例： 昨日何を食べたか忘れた。忘记了昨天吃了什么。 彼は何を言ったか分からない。不明白他说了什么。 先生が学校に行ったか教えない？你不能告诉我老师去没去学校吗？ 对于像上面第三个例句这样的问题，回答里面一般会加上「どうか」，这基本上跟英语里面”whether or not” 一个意思。你也可以把正反两种猜测都加上来表示相同的意思。 示例： 先生が学校に行ったかどうか知らない。不知道老师是否去了学校。 先生が学校に行ったか行かなかったか知らない。不知道老师是去学校了还是没去。（这里用的是行く的过去否定式加が） 4.3.4 使用疑问词疑问词 单词+ 提问词 含义 誰か 某人 何か 某物 いつか 某时 どこか 某处 どれか 某个 示例： 誰かがおいしいクッキーを全部⻝べた。某人把好吃的曲奇全给吃了。 誰が盗んだのか、誰か知りませんか。谁知道谁偷了它吗？ 犯人をどこかで見ましたか。你有在哪里看到过嫌犯吗？ この中からどれかを選ぶの。（解释口吻）你要从这里面选一个出来。 4.3.5 表示包含的疑问词包含单词 单词+も 含义 誰も 所有人/ 没人 何も 没东西（只在否定句用） いつも 总是 どこも 到处 どれも 任一（都） 示例： この質問の答えは、誰も知らない。没人知道这个问题的答案。 友達はいつも遅れる。朋友总是迟到。 ここにあるレストランはどれもおいしくない。任一家在这里的参观都不好吃。 今週末は、どこにも行かなかった。这周末哪里也没去。 对于本节，我认为这只是包含主题助词「も」对于「か」的替代，而包含主题动词前的部分才是疑问词的主干。 4.3.6 表示「任何」的疑问词疑问词主干后还可以改用「でも」，意思是「任何」。要注意「何でも」念作「なんでも」而非「なにでも」 表示「任何」的单词 单词+でも 含义 誰でも 任何人 何でも 任何东西 いつでも 任何时间 どこでも 任何地方 どれでも 任一 示例： この質問の答えは、誰でも分かる。任何人都明白这个问题的答案。 昼ご飯は、どこでもいいです。关于午饭，在哪里吃都好。 あの人は、本当に何でも⻝べる。那个人真的什么都吃。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 4.2 称呼别人","slug":"日语语法 4.2 称呼别人","date":"2020-03-21T11:10:58.000Z","updated":"2020-03-21T11:10:58.000Z","comments":true,"path":"2020/03/21/日语语法 4.2 称呼别人/","link":"","permalink":"/2020/03/21/日语语法 4.2 称呼别人/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 4 语法精要4.2 称呼别人4.2.1 称呼自己以下是「我」的常用说法，以及使用场合： 私【わたくし】-正式场合使用，男女通用。 私【わたし】-普通场合的礼貌用语，男女通用。 僕【ぼく】-主要是男性使用，相当讲礼貌或相当随意的场合都可以用。 俺【おれ】-非常随意的口语，基本只有男性使用。 あたし-很女性化很口语的说法。但很多女生会用「わたし」，因为「あたし」听起来太娘了。 自己的名字-这也是很女性化同时很幼稚的自我称呼。 わし-通常是中年大叔使用。 示例： 私の名前はキムです。我的名字是Kim。（中性，礼貌） 僕の名前はキムです。我的名字是Kim。（男性化，礼貌） 僕の名前はボブだ。我的名字是Bob。（男性化，口语） 俺の名前はボブだ。我的名字是Bob。（男性化，口语） あたしの名前はアリス。我的名字是Alice。（女性化，口语） 4.2.2 用名字称呼别人用法公式： 姓氏 + さん敬称。 名字 + さん礼貌而不失亲近。 姓氏 + 头衔头衔比如「社⻑」、「課⻑」、「先生」等。 名字 + くん对象为平级/年下[1]男性。 名字 + ちゃん对象为平级/年下[1]女性。 [1] 此处我用了年下，原文为社会地位低。个人不喜欢用社会地位去形容，故使用了年下一词代指后辈，年龄小，在上下关系中位于下阶的群体。 4.2.3 用「你」来称呼别人日语中，通常用名字互称。通一般只有在特殊情绪或者不知道对方姓名的情况下使用「你」来称呼别人。以下是等同于英语里面”you”的一些说法： あなた- 比きみ稍尊敬的称呼。也可用作亲密男女之间称呼，尤指妻子称呼丈夫。 君【きみ】- 男性对同辈及晚辈的称呼。 お前【お・まえ】- 很粗鲁的称呼对方的方法。通常是男人使用，且会说成「おめえ」。 あんた- 熟人之间很不客气的称呼，说话的人一般是生气了。 ⼿前【て・めえ】- 非常粗鲁的说法，类似「お前」，并且更凶，一般会说成「てめ〜〜」，听起来像是准备开打。我只在动漫和电影里⻅过这种用法。实际上如果你这么称呼你的朋友，他们反而可能会哈哈大笑，说你看漫画说看多了。 貴様【き・さま】- 非常、非常粗鲁的说法。听起来像是你要干掉对方。我还是只在漫画书里⻅过，这里介绍只是为了让你能看懂漫画书。 *1.和2.的原文解释我不太认同，故使用了《新世纪日汉双解大辞典》的解释。 4.2.4 第三人称日语中的第三人称还有男女朋友的意思，故需要通过上下文以及第三人称后的称呼区分两种意思： 彼【かれ】- 他；男朋友 彼女【かの・じょ】- 她；女朋友 4.2.5 称呼家庭成员 ⺟【はは】- ⺟亲 お⺟さん【お・かあ・さん】- ⺟亲（尊称） 両親【りょう・しん】- 父⺟ 父【ちち】- 父亲 お父さん【お・とう・さん】- 父亲（尊称） 妻【つま】- 妻子 奥さん【おく・さん】- 妻子（尊称） 夫【おっと】- 丈夫 主人【しゅ・じん】- 丈夫 姉【あね】-姐姐 お姉さん【お・ねえ・さん】-姐姐（尊称） 兄【あに】-哥哥 お兄さん【お・にい・さん】-哥哥（尊称） 妹【いもうと】-妹妹 弟【おとうと】-弟弟 息子【むす・こ】-儿子 娘【むすめ】-女儿 家族图标||自家人|别家人||:-:|:-:|:-:||父母|両親|ご両親||母亲|⺟|お⺟さん||父亲|⽗|お⽗さん||妻子|妻|奥さん||丈夫|夫|ご主⼈||姐姐|姉|お姉さん||哥哥|兄|お兄さん||妹妹|妹|妹さん||弟弟|弟|弟さん||儿子|息⼦|息⼦さん||女儿|娘|娘さん|","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 4.1 敬语和动词词根","slug":"日语语法 4.1 敬语和动词词根","date":"2020-03-21T11:09:48.000Z","updated":"2021-11-26T17:56:18.931Z","comments":true,"path":"2020/03/21/日语语法 4.1 敬语和动词词根/","link":"","permalink":"/2020/03/21/日语语法 4.1 敬语和动词词根/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 4 语法精要4.1 敬语和动词词根4.1.1 礼貌用语前面我们用于表示状态和动作时所使用的都是动词的辞书形。这种形式可以用于同龄或者年下的说话对象。而在实际生活中，面对年长、领导、师长等需要使用的是丁寧語。 后面（也许很后面）我们会学习一种更为礼貌的语言，叫做尊敬语（尊敬語）和谦逊语（謙譲語）。它可能比你想象的更有用，因为售货员、收银员等人都会用这种语言。但现在让我们还是先学会丁寧語，这是尊敬語和謙譲語的基础。 4.1.2 动词词根用法公式： （る动词）⻝べる→⻝べ对于る动词将「る」去掉获得词根。 （う动词）泳ぐ→泳ぎ对于う动词将最后的假名从う段换成同行的い段假名获得词根。 （例外）する→し （例外）くる→き 4.1.3 用「〜ます」来将动词变为丁宁语用法公式：示例词根「遊び」的丁宁语活用 ます活用形 词根+ます 字典形 ます 遊びます 否定式 ません 遊びません 过去形 ました 遊びました 过去否定式 ませんでした 遊びませんでした 示例： 明日、大学に行きます。明天，去大学。 先週、ボブに会いましたよ。知道不，上周⻅了Bob。 晩ご飯を食べませんでしたね。没吃晚饭，是吧？ 面白くない映画は見ません。对于没意思的电影，不看。 4.1.4 其他情况用「です」用法公式：い形容词 口语 丁宁语 字典形 かわいい かわいいです 否定式 かわいくない かわいくないです 过去形 かわいかった かわいかったです 过去否定式 かわいくなかった かわいくなかったです 用法公式：な形容词、名词 口语 丁宁语 字典形 静か（だ） 静かです 否定式 静かじゃない 静かじゃないです 过去形 静かだった 静かでした[1] 过去否定式 静かじゃなかった 静かじゃなかったです [1] 注意只有对于名词、な形容词的过去形，要用「でした」。一个常犯的错误就是把这规则也套用到い 形容词去了。记住「かわいいでした」是错的！ 示例： 子犬はとても好きです。关于小狗，非常喜欢。（最自然的翻译是某人很喜欢小狗，不过没有足够的上下文，也许本意是小狗很喜欢什么东西。） 昨日、時間がなかったんです。昨天没有时间。 その部屋はあまり静かじゃないです。房间并不是很安静。 先週に見た映画は、とても面白かったです。上周看过的电影很有意思。 用法公式：一种更加正式的否定式活用 口语 丁宁语 否定式 かわいくない かわいくありません 过去否定式 かわいくなかった かわいくありませんでした 否定式 静かじゃない 静かじゃありません 过去否定式 静かじゃなかった 静かじゃありませんでした 示例： その部屋はあまり静かじゃないですよ。知道不，那房间不大安静。 その部屋はあまり静かじゃありませんよ。知道不，那房间不大安静。 4.1.5 「です」跟「だ」的区别说「没错」的不同方式： そうだ。 そうです。 「だ」表示陈述语气，「です」表示礼貌语气，「です」不是「だ」的丁宁语形式。 除了上述细微差别之外，「だ」和「です」的另一个关键差异是「だ」在各种不同的语法里面可以用来标识关系从句，而「です」却只能用在句尾，用以礼貌地表达状态。比如下面俩个句子（其中的语法后面会介绍）： そうだと思います我想是那样的。 そうですと思います（语法错误）","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"旧文搬运 关于python jieba库的应用","slug":"旧文搬运 关于python jieba库的应用","date":"2020-03-19T11:47:06.000Z","updated":"2021-11-25T09:16:44.000Z","comments":true,"path":"2020/03/19/旧文搬运 关于python jieba库的应用/","link":"","permalink":"/2020/03/19/旧文搬运 关于python jieba库的应用/","excerpt":"","text":"关于python jieba库的应用作者：Yuk1n0链接：https://www.lofter.com/lpost/1e94581f_d566a55来源：LOFTER时间：2016-12-16 近期在做python的一些题目，其中有一道是用jieba库对《三国演义》进行分词，并统计其中人名的词频。 题目：《三国演义》是一本鸿篇巨著，里面出现了几百个各具特色的人物。每次读这本经典作品都会想一个问题，全书这些人物谁出场最多呢？一起来用Python回答这个问题吧。人物出场统计涉及对词汇的统计。中文文章需要分词才能进行词频统计，请使用jieba库。《三国演义》文本保存为：三国演义.txt。 输入：三国演义.txt，请使用input()函数方式获得文件名输入。 输出：打印输出全书出现次数最多的5个人物名字，名字采用逗号（,）分隔，每个名字后面用中括号标注改名字出现次数。例如：曹操[123], 刘备[110]。 1.1 打开文件的问题上来第一步，打开文件就出了问题，由于题目要求打开文件必须为中文名“三国演义.txt”，在编码上不符合python的默认编码模式。 之前就遇到过这样的问题，中文的“gbk”编码出了问题，用codecs库转码成utf-8就行了。 1.2 分词根据官方文档的说法，用jieba中的posseg函数就能实现将文本分词并给每个词智能识别词性。分词后的每个词x = x.word + / + x.flag，会有x.word = 原单词 以及 x.flag = 词性 两个属性 words = jieba.posseg.cut(t) #分词并识别词性 num = {} #建立一个字典用来对应key（词语）和value（出现次数） for word in words: #开始遍历所有的分词结果 if word.flag == &quot;nr&quot;: #判断词性是否为“nr”（人名） num[word.word] = num.get(word.word,0) + 1 #num.get(word.word,0)表示如果字典num中word的对应value值为空，则返回一个默认值0 1.3 排序 打印分词后我们已经有了“人名：次数”的一个字典，接下来我们需要根据key值降序排列这个字典 order = list(num.items()) #list(num.items)返回一个每个元素为（key，value）元组的列表 order.sort(key = lambda x:x[1], reverse = True) #以列表中的第一列即value的一列为key进行reverse排序 for i in range(5): print(str(order[i][0])+&quot;[%d]&quot;%int(order[i][1]),end = &quot;&quot;) if i == 4: break print(&quot;,&quot;,end = &quot;&quot;) 1.4 打印后的结果 蜜汁尴尬，jieba库直接把孔明曰当成了人名，当然还有很多的曹操曰，玄德曰也会被当成人名处理，因此我们需要改进一下算法或者利用jieba库自身的功能来避免这个现象。 我先看了一下讨论群里大家的反应，有些人说可以通过增加“孔明”“曹操”等词的词频来让系统只识别孔明和曹操，不识别曰，有人说可以直接把孔明曰加到孔明里，是系统自己识别不够智能，我们也没办法。可是我总觉得这两种方法都太low了，我想要从根本上解决这个问题。 1.5 试图解决“孔明曰”出现这种情况的根本原因是系统本身不够智能，将“孔明曰”识别成了人名。讨论群里有个人说可以用jieba.cut(t, cut_all = True)的方式来避免这个bug。我为此查看了官方文档对这个函数的解释。 import jieba seg_list = jieba.cut(&quot;我来到北京清华大学&quot;, cut_all=True) print(&quot;Full Mode:&quot;, &quot;/ &quot;.join(seg_list)) # 全模式 seg_list = jieba.cut(&quot;我来到北京清华大学&quot;, cut_all=False) print(&quot;Default Mode:&quot;, &quot;/ &quot;.join(seg_list)) # 精确模式 seg_list = jieba.cut(&quot;他来到了网易杭研大厦&quot;) # 默认是精确模式 print(&quot;, &quot;.join(seg_list)) #output 【全模式】: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学 【精确模式】: 我/ 来到/ 北京/ 清华大学 【新词识别】：他, 来到, 了, 网易, 杭研, 大厦 (此处，“杭研”并没有在词典中，但是也被Viterbi算法识别出来了) 也就是说用jieba.cut(t, cut_all = True)的时候，系统会把可以识别的所有词汇都列出来，那么“孔明曰”就不会直接被识别为“孔明曰”而会被识别为“孔明”和“孔明曰”，这样一来就给了我们统计所有孔明出现次数的机会。 但是，与此同时，另一个问题困扰了我，上文中用到的posseg函数能够实现词性识别，而这个jieba.cut函数却不能识别词性，那么我们怎么统计人名呢？我首先想到的设计一个cut，posseg函数并行的算法，然后通过将cut后的结果遍历posseg的结果来判断词性，但是显然的，这样做需要双重循环，十分复杂。 import codecs import jieba import jieba.posseg as pseg t = input() t = codecs.open(t,&quot;r&quot;,&quot;utf-8&quot;).read() words = list(jieba.cut(t, cut_all = True)) ws = pseg.cut(t) num = {} for word in words: for w in ws: if word == w.word and w.flag == &quot;nr&quot;: num[word] = num.get(word,0) + 1 order = list(num.items()) order.sort(key = lambda x:x[1], reverse = True) for i in range(10): print(str(order[i][0])+&quot;[%d]&quot;%int(order[i][1]),end = &quot;&quot;) if i == 9: break print(&quot;,&quot;,end = &quot;&quot;) 十分尴尬的是，我运行了20分钟也没出结果，看来这双重循环确实不实用。 那就换个方法呗，之后我想的方法是先跑一遍cut，然后再把cut后的每个词跑一遍posseg判断一下词性，再排序。于是就有了： import codecs import jieba import jieba.posseg as pseg t = input() t = codecs.open(t,&quot;r&quot;,&quot;utf-8&quot;).read() words = jieba.cut(t, cut_all = True) num = {} for word in words: if len(word) &lt; 2: #发现系统会把单字识别为任命我就加了长度限制 continue else: num[word] = num.get(word,0) + 1 x = list(num.keys()) for i in x: ws = pseg.cut(str(i)) for w in ws: if w.flag != &quot;nr&quot;: num[w] = 0 order = list(num.items()) order.sort(key = lambda x:x[1], reverse = True) for i in range(10): print(str(order[i][0])+&quot;[%d]&quot;%int(order[i][1]),end = &quot;&quot;) if i == 9: break print(&quot;,&quot;,end = &quot;&quot;) 结果让我快崩溃了,因为不用全文posseg所以跑得很快，几秒出结果。 真的快哭了好吗。。。 1.6 关于试图解决“孔明曰”的反思在两次失败后我仔细思考了一下从根本上解决这一问题的可行性，发现这似乎不能够从根本上解决。如果我们开启全分词，系统不但会把”孔明曰“分为“孔明”和”孔明曰“，同时还会智障的把“司马懿”分为“司马”和”司马懿“，然后再名字判断的时候还会将这两个都判断为“nr”，即名字。但一旦我们不开启全分词，那么系统就会智障地把“孔明曰”当成一个名字，也不行。所以怎么着都不能直接从根本上解决这一问题。最好的方式就是建立一个包含三国各个人名的字典，然后让系统在文章中找这些词的出现次数，这样才能准确无误地统计。但是由于我们的题目的输入项只是一个”三国演义.txt“,所以想要这样解决显然是不行的。 1.7 向AI的智障低头在打了一晚上无用码之后，身心俱疲，为了更好的输出效果，我最终采取了讨论组中那个增加”孔明“、”曹操“等词的词频的方法，虽然我当时并不像采用这个很low的方法，但是迫于现实的残酷性，我只有向AI低头了。 import codecs import jieba.posseg as pseg import jieba t = input() t = codecs.open(t,&quot;r&quot;,&quot;utf-8&quot;).read() jieba.add_word(&#39;孔明&#39;,100000,&quot;nr&quot;) jieba.add_word(&#39;玄德&#39;,100000,&quot;nr&quot;) jieba.add_word(&#39;曹操&#39;,100000,&quot;nr&quot;) jieba.add_word(&#39;刘备&#39;,100000,&quot;nr&quot;) jieba.add_word(&#39;关羽&#39;,100000,&quot;nr&quot;) jieba.add_word(&#39;关公&#39;,100000,&quot;nr&quot;) jieba.add_word(&#39;张飞&#39;,100000,&quot;nr&quot;) words = pseg.cut(t) num = {} for word in words: if word.flag == &quot;nr&quot;: num[word] = num.get(word,0) + 1 order = list(num.items()) order.sort(key = lambda x:x[1], reverse = True) for i in range(5): print(str(order[i][0])[:-3]+&quot;[%d]&quot;%int(order[i][1]),end = &quot;&quot;) if i == 4: break print(&quot;,&quot;,end = &quot;&quot;) 这种暴力增加词频的方法让系统在识别”孔明曰“的时候直接将其识别为”孔明“和”曰“。虽然我依然觉得很low，但在解决这一问题时，这确实不失为一种实用的方法。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"python","slug":"python","permalink":"/tags/python/"},{"name":"旧文搬运","slug":"旧文搬运","permalink":"/tags/旧文搬运/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"语音和语言处理第三版 2.3 语料库","slug":"语音和语言处理第三版 2.3 语料库","date":"2020-03-16T09:26:48.000Z","updated":"2020-03-16T09:26:48.000Z","comments":true,"path":"2020/03/16/语音和语言处理第三版 2.3 语料库/","link":"","permalink":"/2020/03/16/语音和语言处理第三版 2.3 语料库/","excerpt":"","text":"本文翻译自Dan Jurafsky和James H. Martin合著的Speech and LanguageProcessing第三版 原文链接：https://web.stanford.edu/~jurafsky/slp3/ Chapter 2 正则表达式，文本标准化，编辑距离2.3 语料库单词不会无中生有。任何我们研究的特定文本片段都是一个或多个特定的说话者或是作者用特定语言的特殊方言，在特定的时间，特定的地点，为了一种特定的功能产出的。 或许差异最大的维度就是语言了。NLP算法在涉及跨语种应用时最有用。截至本文撰文时，根据在线 Ethnologue catalog (Simons and Fennig, 2018)，这个世界共有7079种语言。大多数NLP工具包倾向于为大型工业国家（中文，英文，西班牙语，阿拉伯语等）的官方语言开发。但我们不想将工具仅限于这几种语言的应用。而且，大多数语言还有多种变种，比如在不同地区或者不同社会群体使用的方言。因此，举个例子，如果我们正在处理African American Vernacular English(AAVE)，一种美国百万人使用的方言。制作与该方言功能匹配的NLP工具是很重要的。在用AAVE写出的推特推文中，经常使用类似 iont 的结构(I don’t in Standard American English (SAE))，或者 talmbout 对应SAE的 talking about 。这两种例子都影响分词 (Blodgett et al. 2016, Jones 2015)。 在一个单一交流行为中，说话者或者作者使用多种语言也是十分普遍的，这个现象称为编码切换。编码转换是在世界范围都十分常见。这里有一些西班牙语和（音译）印地语跟英语编码切换的例子 (Solorio et al. 2014, Jurgens et al. 2017)： (2.2) Por primera vez veo a @username actually being hateful! it was beautiful:)[For the ﬁrst time I get to see @username actually being hateful! it was beautiful:) ] (2.3) dost tha or ra- hega … dont wory … but dherya rakhe[“he was and will remain a friend … don’t worry … but have faith”] 差异的另一个维度就是体裁。我们的算法必须处理的文本可能会来自新闻，科幻，非科幻，科技文章，维基百科，或者宗教文本。他们也可能来自语音体裁，比如电话会话，商业会议，警用携带相机，医疗采访，或者电视综艺、电影的片段。他们也可能来自工作情况，比如医生的字条，法律条文，议会或国会程序。 文本也会反映作者（或者讲话者）的人口统计学特征：他们的年龄，性别，种族，社会经济阶层都可以影响我们正在处理的文本的语言学属性。 并且最后，实践也会影响。语言会随着时间变化，并且对于一些语言我们拥有不同历史时期的良好的语料库。 因为语言是需要因地制宜的，当开发语言处理的计算模型时，考虑到谁创造了这个语言，在什么上下文环境下，为了什么目的，并确保模型适合数据是重要的。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"/tags/NLP/"},{"name":"翻译","slug":"翻译","permalink":"/tags/翻译/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"语音和语言处理第三版 2.2 单词","slug":"语音和语言处理第三版 2.2 单词","date":"2020-03-15T11:08:02.000Z","updated":"2020-03-15T11:08:02.000Z","comments":true,"path":"2020/03/15/语音和语言处理第三版 2.2 单词/","link":"","permalink":"/2020/03/15/语音和语言处理第三版 2.2 单词/","excerpt":"","text":"本文翻译自Dan Jurafsky和James H. Martin合著的Speech and LanguageProcessing第三版 原文链接：https://web.stanford.edu/~jurafsky/slp3/ Chapter 2 正则表达式，文本标准化，编辑距离2.2 单词在我们讨论处理单词之前，我们需要决定什么算作一个单词。让我们通过观察一个语料库，一个计算机可读的文本或者语音集合，开始。举个例子，Brown语料库是一个百万单词的集合。其中包含不同体裁的（新闻，科幻，非科幻，学术等等）500个手写英文文本样本。这些样本被Brown大学在1963-1964年间（KuˇceraandFrancis,1967）收集。在下面的Brown语料库的句子中有多少个单词呢？ He stepped out into the hall, was delighted to encounter a water brother. 如果我们不将标点看作单词的话，这句话有13个单词。如果我们考虑标点的话，则有15个单词。我们是否考虑句号，逗号等等取决于任务的要求。标点对于寻找事物的边界（逗号，句号，冒号）以及识别一些意思的角度（问号，叹号，引号）十分关键。对于一些任务，比如语音片段的标签化，解析或是语音合成，我们有时将标点符号视为独立的单词。 Switchboard美式英语陌生人电话会话语料库在1990年代早期被收集。它包括2430个平均每个6分钟的会话，总计240小时的语音和约3百万个单词（Godfrey et al., 1992）。这样的口语语料库没有标点但是引入了其他的针对单词定义的难题。让我们看一个Switchboard语料库中的 utterance ； utterance 是一句话的口语关联 [1] ：[1] utterance原意为“a spoken word, statement, or vocal sound.”后文我准备将utterance译为语音。 I do uh main- mainly business data processing 这段语音有两种不连贯之处。破碎的单词 main- 被称为碎片。像 uh 和 um 的单词被称作填充词或者填充停顿。我们应该将这些视为单词吗？再一次说明，这取决于应用背景。如果我们正在搭建一个语音转录系统，我们可能想要最后剔除这些不连贯。 但是我们有时也保留这些不连贯之处。像 uh 或者 um 这样的不连贯实际上对于语音识别预测即将出现的单词是有用的，因为他们可能标志着说话人正在重新开始一个从句或者一个想法，并且因此也对被视为常规单词的语音识别有用。因为人们使用不同的停顿词 [2] ，这也可以成为识别说话人的一个线索。实际上， Clark and Fox Tree (2002) 展示了 uh 和 um 的不同意思。你认为他们分别表达什么意思呢？ [2] 本段之前我将disfluency译作了不连贯之处，即指口语中的停顿。本段起我将译作停顿词。 大写的token [3] 比如 They 和小写的token比如 they 是一个单词吗？这些单词在某些任务中（语音识别）会被搅在一起，然而对于部分语音或者命名实体标签，大写是一个有用的特征那个被保留。 [3] token指的是语料库中的单个单词。前文中被我直译为标签，但这不太符合原意。之后我将直接使用token，后文有对于token的详细说明。 对于一些被改变的形态比如 cats 和 cat 呢？这两个单词有相同的词源 cat 单具有不同的词态。lemma [4] 是具有相同词根，相同的主要词性，以及相同的词义的词汇形态集合。词态是完全变形或者派生的词汇形态。对于像阿拉伯语这种形态复杂的语言，我们经常需要处理词形化。对于许多英文处理任务，词态就足够了。 [4] lemma就像是日语中的辞书形，指词汇出现在词典的基础形态。后文为将其与前后缀的词根相区分，将直接使用lemma。 英语中有多少单词呢？想要回答这个问题我们需要区分两种对于单词的讨论方式。type是指语料库中不同单词的数量，如果词汇中的单词集合是V，type的数量就是词汇的规模|V|。token是所有单词的总数。如果忽略标点，下面这个Brown语料库的句子有16个token和14个type： They picnicked by the pool, then lay back on the grass and looked at the stars. 当提及语言中的单词数量的时候，我们通常指的是单词的type数量。 Corpus Tokens= N Types= V Shakespeare 884 thousand 31 thousand Brown corpus 1 million 38 thousand Switchboard telephone conversations 2.4 million 20 thousand COCA 440 million 2 million Google N-grams 1 trillion 13 million 图 2.10 一些英语语料库的大致type和token数量。最大的谷歌N-gram语料库包含1300万个type，但这只计算了出现超过40次及以上的type，实际数字将更大。 图2.10展示了不同主流英语语料库中计算得到的大致type和token数量。我们观察的语料库越大，找到的单词type就越多。并且实际上，这个type数量|V|和token数量N的关系被称为Herdan’sLaw (Herdan, 1960) 或者 Heaps’Law (Heaps, 1978) 。这些都以发现者命名（分别在语言学和信息检索领域）。该公式在式2.1中展示，其中k和β为正常数，且0&lt;β&lt;1。 \\begin{equation}|V| = kN^β \\tag{2.1}\\end{equation} β的值取决于语料库规模和体裁，但是至少对于图2.10中的大型语料库，β在 .67 到 .75 之间。大致上我们可以说文本的词汇规模增长速度远快于单词长度的平方根。 另一个确定语言中单词数量的方法是用lemma数量代替词形type。词典可以帮助给出lemma的数量。词典的条目和加粗字体的数量是lemma数量的一个大致上限（因为一些lemma拥有多重加粗形式）。牛津英语词典1989年版拥有615000个词条。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"/tags/NLP/"},{"name":"翻译","slug":"翻译","permalink":"/tags/翻译/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"语音和语言处理第三版 2.1 正则表达式","slug":"语音和语言处理第三版 2.1 正则表达式","date":"2020-03-13T10:18:16.000Z","updated":"2020-03-13T10:18:16.000Z","comments":true,"path":"2020/03/13/语音和语言处理第三版 2.1 正则表达式/","link":"","permalink":"/2020/03/13/语音和语言处理第三版 2.1 正则表达式/","excerpt":"","text":"本文翻译自Dan Jurafsky和James H. Martin合著的Speech and LanguageProcessing第三版 原文链接：https://web.stanford.edu/~jurafsky/slp3/ Chapter 2 正则表达式，文本标准化，编辑距离 User: I am unhappy.ELIZA: DO YOU THINK COMING HERE WILL HELP YOU NOT TO BE UNHAPPYUser: I need some help, that much seems certain.ELIZA: WHAT WOULD IT MEAN TO YOU IF YOU GOT SOME HELPUser: Perhaps I could learn to get along with my mother.ELIZA: TELL ME MORE ABOUT YOUR FAMILYUser: My mother takes care of me.ELIZA: WHO ELSE IN YOU FAMILY TAKES CARE OF YOUUser: My father.ELIZA: YOUR FATHERUser: You are like my father in some ways.Weizenbaum (1966) 上述对话出自ELIZA，一个早期自然语言处理系统。他可以通过模仿罗氏心理治疗师 [1] 和用户执行有限的对话。ELIZA是一个惊人简单的程序。他用模式匹配去识别像“I need X”这样的短句，并将他们翻译成像“What would it mean to you if you got X?”这样合适的输出。这个简单的技巧在这个领域得到成功，因为要模仿一个罗氏心理治疗师，实际上ELIZA不需要知道任何东西。就像Weizenbaum记录的那样，这是一种少有的对话形式，倾听者可以表现得就像他们对这个世界一无所知一样。ELIZA对于人类对话的模仿行为是非常成功的：很多和ELIZA互动过的人都开始相信，ELIZA确实可以理解他们和他们的问题。很多人甚至在被解释了程序的操作后，依然相信ELIZA的能力。甚至现在，这样的对话机器人还是一种有趣的消遣。 当然，现代的对话代理远不止是一种消遣。他们可以回答问题，预定航班，或者找餐厅。他们依靠这些功能对用户的意图有更复杂的了解。我们将在26章对此进行进一步说明。尽管如此，这个简单的基于模式的方法驱动了在自然语言处理领域扮演重要角色的ELIZA和其他对话机器人。 我们将从最重要的描述文本模式的工具正则表达式开始。正则表达式可以被用于指定字符串。这些我们可能想将从一个文件中，从ELIZA在上文的转义“I need X”中提取的字符串转变为定义的字符串，比如199美元或24.99美元，以提取文件中的标价表格。 我们之后会转向一系列任务，统称为文本标准化，在这之中正则化扮演了重要的角色。标准化的文本意味着将其转化为一个更加便捷标准的形式。举个例子，我们要对语言进行的大多数处理都依赖于文本中第一次的分词或者标记化单词，即标记化任务。英文单词往往被空格分离，但有时空格总是不充分。 New York 和 rock ’n’ roll 总是被认为是长单词，尽管他们包含空格。与此同时，我们需要把 I’m 分成 I 和 am 两个单词。为了处理推特文本，我们将会需要标记化表情符号比如 :) 或者标签比如 #nlproc 。有些语言，比如日语，没有单词见的空格，因此单词标签化变得更加困难。 文本标准化的另一部分就是词的形态变化，即确定两个单词是否拥有相同的词源，尽管他们表面看上去不一样。举个例子，单词 sang ， sung 和 sings 都是动词 sing 的变形。形态变化对于处理语言形态复杂的语言，比如阿拉伯语，是非常必要的。提取词干指的是一种简单版本的形态变化，在这种变化中我们通常只是去掉词尾的后缀。文本标准化也包括文本分割：将一段文本分为独立的句子，使用句号或者惊叹号作为提示。 最后，我们将需要比较单词和其它字符串。我们将介绍一个度量标准称作编辑距离，它将基于把一个字符串改为另一个所需要的编辑次数（插入，删除、替换）测定两个字符串的相似程度。编辑距离是一个广泛用于语言处理的算法，从拼写纠正，语音识别到同义词解析。 [1] https://en.wikipedia.org/wiki/Carl_Rogers 2.1 正则表达式一个没有被承认的在计算机科学标准化中的成功就是正则表达式，这是一个指定文本搜索字符串的语言。这个实用的语言用于每个计算机语言，单词处理器，以及文本处理工具比如Unix工具grep或者Emacs。正式而言，正则表达式是一种描述字符串集的代数标记。正则表达式在文本搜索中尤其有用，比如当我们有一个模式需要搜索匹配，或者有一个文本语料库需要遍历的时候。一个正则表达式搜索功能将会搜索遍历整个语料库，返回所有匹配模式的文本。这个语料库可以是单一文件或者一个集合。举个例子，Unix命令行工具grep使用正则表达式并返回匹配表达式的每一行输出文件。 如果匹配数超过一个，一次搜索可以被设计成在一行中返回所有匹配文本，也可以被设计成只返回第一个。之后的例子我们通常会强调匹配正则表达式的模式部分，并且只显示第一个匹配结果。我们将展示被斜杠限制的正则表达式，但请注意斜杠并不是正则表达式的一部分。 正则表达式有很多变种。我们将描述延伸正则表达式，不同的正则表达式解析器可能只能识别上述表达式的子集，或者处理一些表达式时会有些许区别。用一个线上正则表达式测试器是一个测试你的正则表达式以及探索这些变化的便捷方法。 2.1.1 基本正则表达式的一些模式最简单的一种正则表达式就是简单字符的序列。为了检索 woodchunk 我们键入 /woodchuck/ 。这个表达式 /Buttercup/ 会匹配任何包含子字符串 Buttercap 的字符串。在grep中使用那个表达式将会返回 I’m called little Buttercup 。这个检索字符串可以包含单个字符（比如 /!/ ）或者一个字符序列（比如 /urgl/）。 正则表达式 示例匹配pattern /woodchucks/ “interesting links to woodchucks and lemurs“ /a/ “Mary Ann stopped by Mona’s“ /!/ “You’ve left the burglar behind again!“ said Nori 图 2.1 一些简单的正则表达式搜索 正则表达式是大小写敏感的，小写的 /s/ 和大写的 /S/ 是不一样的（ /s/ 会匹配小写的 s 而不是大写的 S ）。这意味着pattern woodchunks 将不会匹配字符串 Woodchunks 。我们可以用方括号 [and] 解决这个问题。在括号中的字符串指定了其中字符多选一的匹配。举个例子，图2.2展示了pattern /[wW]/ 匹配了包含 w 或者 W 的pattern。[1] [1] 一直把pattern译为模式真是太烦了，这段之后我就直接用pattern了。 正则表达式 匹配 示例pattern /[wW]oodchuck/ Woodchuck or woodchuck “Woodchuck” /[abc]/ ‘a’, ‘b’, or ‘c’ “In uomini, in soldati” /[1234567890]/ any digit “plenty of 7 to 5” 图 2.2 方括号 [] 用于指定字符的或 正则表达式 [1234567890] 指定了任意单个十进制数。尽管像十进制数或者字母这样类型的字符在表达中是重要的构建基块，但他们会变得笨拙。（比如，指定 [ABCDEFGHIJKLMNOPQRSTUVWXYZ] 就十分不方便）。如果有一个定义正确的序列和字符集关联，那么方括号就可以与横杠（-）连用去指定范围内任何一个字符。pattern [2-5] 指定了2，3，4 或 5中的任意一个字符。pattern [b-g] 指定了b，c，d，e，f 或 g 中的任意一个字符。一些其他的示例如图2.3中所示。 RE Match Example Patterns Matched /[A-Z]/ an upper case letter “we should call it ‘Drenched Blossoms’ ” /[a-z]/ a lower case letter “my beans were impatient to be hoed!” /[0-9]/ a single digit “Chapter 1: Down the Rabbit Hole” 图 2.3 方括号 [] 和横杠 - 连用指定范围 方括号也可以和插入符号 ^ 连用来指定一个不满足某些条件的字符。如果插入符号 ^ 是左方括号后的第一个符号，那么结果pattern就需要被取反。举个例子，pattern /[ˆa]/ 匹配任意一个除了a以外的单字符（包括特殊字符）。这只有在插入符号是左方括号后的第一个符号时成立。如果他出现在了其他地方，它通常只是代表一个插入符。图2.4展示了一些例子。 RE Match(single characters) Example Patterns Matched /[ˆA-Z]/ not an upper case letter “Oyfn pripetchik” /[ˆSs]/ neither ‘S’ nor ‘s’ “I have no exquisite reason for’t” /[ˆ.]/ not a period “our resident Djinn” /[eˆ]/ either ‘e’ or ‘ˆ’ “look up ˆ now” /aˆb/ the pattern ‘aˆb’ “look up aˆ b now” 图 2.4 插入符号用于表示取反或者指示代表插入符号本身 *请看后面的正则表达式，用反斜杠跳出这个状态。 我们如何描述可选元素，比如在 woodchunk 和 woodchunks 中的可选元素 s 。我们不能用方括号，因为当他们允许我们描述”s or S“的时候，他们不会允许我们描述”s or nothing“。为此我们用问号/?/来描述”the preceding character or nothing“，如图2.5所示。 RE Match Example Patterns Matched /woodchucks?/ woodchuck or woodchucks “woodchuck” /colou?r/ color or colour “color” 图 2.5 问号 ? 标志了前面表达式 [2] 的可选性 [2] 此处实际上是标志了问号前一个字符的可选性。 我们可以把问号看作是“zero or one instances of the previous character”的意思。也就是说，这是一个指定我们想要多少某个东西，这在正则表达中是一个非常重要的事情。举个例子，考虑某些绵羊的语言包括一些和下面类似的字符串： baa! baaa! baaaa! baaaaa! … 这个语言包含字符串 a b ，后面跟着至少两个 a ，最后还有一个惊叹号。允许我们描述一些类似“some number of a s”的运算符集合基于星号或者说是 * ，通常被称为 Kleene * (generally pronounced“cleanystar”)。星号 * 意味着“zero or more occurrences of the immediately previous character or regular expression”。因此， /a*/ 表示“any string of zero or more a s”。这会匹配 a 或者aaa ，但是着也会匹配 Off Minor 因为 字符串Off Minor 有0个a。因此，正则表达式为了匹配一个以及以上的 a 用 /aa*/ 表示，表示a后面可以跟着0个及以上的 a 。更复杂的表达也可以被重复。因此， /[ab]*/ 表示“zero or more a’s or b’s”（不是“zero or more right square braces”）。这会匹配像 aaaa 或 ababab 或 bbbb 这样的字符串。 为了指定多个十进制数（对寻找价钱很有用），我们俩可以将正则表达式 /[0-9]/ 拓展一位。因此，整数（十进制数的字符串）就是 /[0-9][0-9]*/ 。（为什么不是 /[0-9]*/ 呢） 有些时候，不得不为了十进制数而写两次正则表达式，这十分让人讨厌。因此，有一种指定某个字符出现至少一次的更简单方法。Kleene + 表示“one or more occurrences of the immediately precedingKleene+ character or regular expression”。因此，表达式 /[0-9]+/ 是指定一个十进制数序列最一般的方法。综上，指定绵羊语言有两种方法： /baaa*！/ 或者 /baa+!/ 。 一个非常重要的特殊符号是句号 /./ ，一个可以匹配任何单个字符的任意替代表达（除去回车），如图2.6所示。 RE Match Example Matches /beg.n/ any character between beg and n begin, beg’n, begun 图 2.6 使用句号 . 指定任意字符 任意替代符号经常和星号合用表示“any string of characters”。举个例子，假设我们想要找到含有特定单词的一行，比如 aardvark 出现两次。我们可以通过正则表达式 /aardvark.*aardvark/ 指定上述情况。 锚符号是将正则表达式抛锚在字符串特定位置的一个特殊符号。最常见的锚符号就是插入符号 ^ 以及金钱符号 $ 。插入符号 ^ 匹配每句话的开始。pattern /ˆThe/ 只匹配位于每句话开始的单词 The 。因此，插入符号有三个用法：匹配每句话的开始，在方括号中表示取反，或者仅仅表示插入符号本身。（是什么上下文环境让grep或者Python知道一个给定的插入符号应该具有的功能呢？）金钱符号 $ 匹配每句话的结尾。因此，pattern / $/ 是一个匹配结尾空格得有用pattern，并且 /ˆThe dog.$/ 匹配一个只含有词组 The dog 的句子。（我们不得不在此处使用反斜杠，因为我们想要 . 表示句号而不是任意替代符号） 有两种其他的锚符号： \\b 匹配一个单词的边缘， \\B 匹配一个非边缘。因此， /\\bthe\\b/ 匹配单词 the 而不是单词 other 。更技术性地讲， 一个用于正则表达式的单词被定义为任意十进制数、下划线或者字母的序列。这基于单词在编程语言中的定义。举个例子， /\\b99\\b/ 将会在 There are 99 bottles of beer on the wall 中匹配字符串 99 （因为 99 跟在一个空格后面），但是不会在 There are 299 bottles of beer on the wall 中匹配 99 （因为 99 跟在一个数字后面）。但是，他会匹配 $99 中的 99 （因为99跟在一个金钱符号之后，而金钱符号不是一个十进制数、下划线或者字母）。 2.1.2 取或，分类，优先假设我们需要搜索关于宠物的文本，也许我们特别对猫和狗感兴趣。在这种情况下，我们可能想要搜索字符串 cat 或者字符串 dog 。由于我们不能用方括号搜索“cat or dog”（我们为什么不能直接使用 /[catdog]/ ？），我们需要一个新的运算符，取或运算符，也可以称为管道符号 | 。pattern /cat|dog/ 可以匹配字符串 cat 或者字符串 dog 。 有时候我们需要用这个取或运算符在一个更大序列的中间。举个例子，假设我想要为我的侄子Daivid搜索关于宠物鱼的信息。我怎么可以同时指定 guppy 和 guppies ？我们不能简单的用 /guppy|ies/ ，因为这只会匹配字符串 guppy 和 ies 。这是因为像 guppy 这样的序列优先于取或运算符 | 。为了使取或运算符支队特定pattern生效，我们需要使用括号运算符( and )。为了相邻运算符比如管道符号 | 以及星号 * 的使用，我们可以将一个pattern封闭在括号内使其看起来像一个单个字符。因此，pattern /gupp(y|ies)/ 可以根据我们的想法指定取或符号只对后缀 y 和 ies 起作用。 括号运算符 ( 在我们使用像星号 * 的计数器时也有用。不像 | 运算符，星号 * 运算符默认情况下只应用于单字符，而不是整个序列。假设我们想要匹配字符串的一段重复实例。也许我们有一段文字包含表格中的列标签 Column 1 Column 2 Column 3 。这个表达式 /Column [0-9]+ */ 不会匹配任何列的数值，而会匹配一个后面跟着任意个空格的单列。星号只作用于它前面的空格而非整个序列。我们可以用括号写出表达式 /(Column [0-9]+ *)*/ 去匹配单词 Column 后面可以跟0及以上个空格，并且整个pattern可以重复0及以上次。 这个一个运算符可以比其他运算符优先级高的想法要求我们用括号去指定我们想表达的含义。这个想法被正则表达式的运算符优先级制度进行格式化。下列表格给出了正则表达式的运算符优先级，从最高到最低的优先级。 名称 符号 Parenthesis () Counters * + ? {} Sequences and anchors the ˆmy end$ Disjunction \\ 因此，因为计数器比文本序列的优先级更高， /the*/ 将匹配 theeeee 而不是 thethe 。因为文本序列比取或富豪的优先级高， /the|any/ 将匹配 the 或者 any 而不是 thany 或者 theny 。 pattern在另一种角度可能会模棱两可。考虑当匹配文本 once upon a time 时，表达式 /[a-z]*/ 的结果。因为 /[a-z]*/ 匹配0及以上各字母，这个表达式可以匹配空字符，或者指示首字母 o ， on ， onc 或者 once 。在这些情况正则表达式总是匹配可以匹配到的最大字符串，我们称pattern式贪婪的，拓展去尽可能覆盖更多的字符串。 然而，有很多办法去强迫非贪婪匹配，使用 ? 筛选器的另一种用法。运算符 *？是一个星号匹配尽可能短的文本。运算符 +? 是一个加号匹配尽可能短的文本。 2.1.3 一个简单的例子假设我们想要写一个正则表达式去寻找英语的冠词 the 。一个简单的（但是不正确的）pattern可能是 /the/ 一个问题就是这个pattern将错过 the 位于句首因此首字母大写的情况（比如 The ）。这可能将我们引导至下面的pattern： /[tT]he/ 但是我们仍然将错误的返回嵌入其他单词的文本（比如 other 或者 theology ）。因此我们需要指定两边具有边界的单词实例： /\\b[tT]he\\b/ 假设我们想要不用 /\\b/ 实现这个。我们可能因为 /\\b/ 将不处理下划线和数字作为单词边界而弃用这个方法，但是我们可能想要在一些与下划线和数字相邻的上下文中找到 the 。我们需要指定我们想要 the 两边都没有英文字母的实例： /[ˆa-zA-Z][tT]he[ˆa-zA-Z]/ 但是这个pattern仍然有一个问题：他将不会找到位于句首的 the 。这是因为正则表达式 [ˆa-zA-Z] 被我们用来避免 the 的嵌入实例，意味着 the 的前面必须有一些单字符（尽管非字母）。我们可以通过指定 the 为句首或者 the 前面是一个非字母字符避免这个问题。在句尾也是同理。 /(ˆ|[ˆa-zA-Z])[tT]he([ˆa-zA-Z]|$)/ 我们刚刚进行的操作是基于两种固定错误：错误肯定，我们错误地匹配了像 other 或者 there 的字符串；以及错误否定，我们错误地错过了像 The 的字符串。处理这两种问题在实现语音和语言处理系统中频繁出现。因此，应用中降低总错误率主要有两个努力方向： 提高准确率（降低错误肯定） 提高召回率（降低错误否定） 2.1.4 一个更复杂的例子让我们使用正则表达式的力量尝试一个更重要的例子。假设我们想要开发一个帮助用户网上买电脑的应用。用户可能想要“any machine with at least 6 GHz and 500 GB of disk space for less than $1000”。想要实现这样的检索，我们首先需要有能力找到像 6 GHz 或 500 GB 或 Mac 或 $999.99。在这部分的最后，我们将找到这个任务的一些简单的正则表达式。 首先，让我们完成关于价格的正则表达式。这是一个用金钱符号后面跟十进制数字符串的正则表达式： /$[0-9]+/ 注意， $ 符号在这里有一个相比我们之前讨论的句尾功能不一样的功能。大多数正则表达式解析器足够聪明，可以意识到 $ 在这里不是表示句尾。（作为一个思维实验，试思考正则表达式解析器如何从上下文弄清楚 $ 的功能。） 现在我们需要处理价钱的小数部分。我们需要加一个十进制的小数点和十进制的小数点后两位： /$[0-9]+.[0-9][0-9]/ 这个pattern只允许 $199.99 但不允许 $199 。我们需要让钱的分位可选并确定他不在单词边界。 /(ˆ|\\W)$[0-9]+(.[0-9][0-9])?\\b/ 最后一点，这个pattern允许像 $199999.99 这样非常贵的价格。我们需要限制价格的高低： /(ˆ|\\W)$[0-9]{0,3}(.[0-9][0-9])?\\b/ 硬盘空间怎么样呢？我们需要再一次允许可选的小数位（5.5 GB）。注意 ? 的使用可以使得最后的 s 是可选的，并且 / */ 表示“zero or more spaces”因为看可能总是有额外的空格在附近： /\\b[0-9]+(.[0-9]+)? *(GB|[Gg]igabytes?)\\b/ 改善正则表达式使得他只匹配高于 500 GB 是留给读者的练习。 2.1.5 更多的运算符图2.7展示了一些普遍范围的别名。这些别名主要被用于节省打字时间。租好了星号 * 和加号 + ，我们还可以通过括在圆括号里使用明确的数字，比如计数器。正则表达式 /{3}/ 表示“exactly 3 occurrences of the previous character or expression”。因此，/a.{24}z/ 将会匹配 a 后面跟着24个点再跟一个 z （而不是 a 后面跟23或者25个点再跟一个 z ） RE Expansion Match First Matches \\d [0-9] any digit Party of 5 \\D [ˆ0-9] any non-digit Blue moon \\w [a-zA-Z0-9_] any alphanumeric/underscore Daiyu \\W [ˆ\\w] a non-alphanumeric !!!! \\s [ \\r\\t\\n\\f] whitespace (space, tab) \\S [ˆ\\s] Non-whitespace in Concord 图 2.7 一些常见字符集的别名 一个区间的数字也可以被指定。因此， /{n,m}/ 指定前面字符或者表达式 n 到 m 次的出现次数。并且， /{n,}/ 表示前面表达式至少出现 n 次。正则表达式中的计数用法被总结在图2.8中。 RE Match * zero or more occurrences of the previous char or expression + one or more occurrences of the previous char or expression ? exactly zero or one occurrence of the previous char or expression {n} n occurrences of the previous char or expression {n,m} from n to m occurrences of the previous char or expression {n,} at least n occurrences of the previous char or expression {,m} up to m occurrences of the previous char or expression 图 2.8 计数的一些正则表达式运算符 最后，特定特殊字符可以被特殊的基于反斜杠的记号进行指定（见图2.9）。最常见的就是转行符号 \\n 以及缩进符号 \\t 。想要指定这些特殊的字符本身（像 ., *, [, 和 \\），在他们前面加上反斜杠（比如/./, /*/, /[/, 和 /\\/）。 RE Match First Patterns Matched * an asterisk “*” “K*A*P*L*A*N” . a period “.” “Dr. Livingston, I presume” \\? a question mark “Why don’t they come and lend a hand?” \\n a newline \\t a tab 图 2.9 一些需要被反斜杠转义的字符 2.1.6 替换，群组捕获，ELIZA一个正则表达式的一个重要用法就是替换。举个例子，替换运算符 s/regexp1/pattern/ 被用在Python中，并且Unix命令行中，像vim或者sed允许被正则表达式特定的字符串被另一个字符串替换： s/colour/color/ 有能力指定一个匹配首个pattern的字符串的子部分往往很有用。举个例子，假设我们想要用尖括号括住文本中的所有整数，比如将 the 35 boxes 改为 the boxes。我们喜欢拥有一种可以直接指向我们发现的整数的方法，这样我们就可以简单的添加括号。为了实现这个，我们将括号运算符( and )括在第一个pattern的周围并用第二个pattern的数字运算符 \\1 指向后面。这是他的具体实现： s/([0-9]+)/&lt;\\1&gt;/ 括号运算符和数字运算符也可以指定一个特定的在文本中必须出现两次的字符串或者表达式。举个例子，假设我们正在寻找pattern “the Xer they were, the Xer they will be“，这里我们想要限制两个X为相同的字符串。我们可以通过在第一个X周围加上括号运算符，并用数字运算符 \\1 替换第二个X的方法实现，如下所示： /the (.*)er they were, the \\1er they will be/ 这里 \\1 将会被在括号运算符中第一个物品匹配的任何字符串替代。因此这将会匹配 the bigger they were, the bigger they will be 但是不会匹配 the bigger they were, the faster they will be 。 这个将一个pattern存在内存中的括号运算符的用法被称为群组捕获。每当一个群组捕获被使用时（比如，用一个括号运算符括住一个pattern），匹配的结果将会被存在数字标记的寄存器中。如果你匹配了两个不同的括号运算符集合， \\2 表示你第二个捕获群组匹配的任何东西。因此： /the (.*)er they (.*), the \\1er we \\2/ 将会匹配 the faster they ran,the faster we ran 而不会匹配 the faster they ran,the faster we ate 。相似地，第三个捕获群组被存在 \\3 中，第四个被存在 \\4 中，依此类推。 括号运算符因此在正则表达式中拥有双重功能：我们被用于集合terms，以指定运算符应该采用的顺序，并且他们被用以捕获寄存器中的一些东西。偶尔我们可能想要用括号集合一些东西，而不像将匹配结果讯在寄存器中。这种情况，我们通过在左括号后面添加 ?: 指令，以这种形式 (?: pattern) 使用非捕获群组。 /(?:some|a few) (people|cats) like some \\1/ 将会匹配 some cats like some cats but not some cats like some a few 。 替换和捕获群组在实现简单对话机器人中，比如 ELIZA (Weizenbaum, 1966)，非常有用。回想 ELIZA 通过像如下对话一样模拟一个罗氏心理学家： User1: Men are all alike.ELIZA1: IN WHAT WAYUser2: They’re always bugging us about something or other.ELIZA2: CAN YOU THINK OF A SPECIFIC EXAMPLEUser3: Well, my boyfriend made me come here.ELIZA3: YOUR BOYFRIEND MADE YOU COME HEREUser4: He says I’m depressed much of the time.ELIZA4: I AM SORRY TO HEAR YOU ARE DEPRESSED ELIZA 通过一系列或者一连串正则表达式的替换进行工作，每个替换都会匹配或者替换输入文本的一些部分。输入文本是首字母大写的。第一个替换就是将所有 MY 的实例改变为 YOUR ，以及将 I’M 变为YOU ARE ，依此类推。下一个替换集匹配并替换其他输入中的pattern。这里有一些例子： s/.* I’M (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE \\1/s/.* I AM (depressed|sad) .*/WHY DO YOU THINK YOU ARE \\1/s/.* all .*/IN WHAT WAY/s/.* always .*/CAN YOU THINK OF A SPECIFIC EXAMPLE/ 因为多重替换可以应用于给出的输入，替换被分配了一个排名，并根据顺序被执行。创造pattern是练习2.3的主题，并且我们将在2.6章回到 ELIZA 架构的细节。 2.1.7 前瞻判别最终，我们将会多次遇到需要预判未来的情况：在文本中提前看是否有pattern匹配，但是不提前匹配指针，这样我们可以在pattern出现时处理他们。 这些前瞻判别使用我们在前面部分，非捕获群组见到的 (? 语法。如果pattern出现，则运算符 (?= pattern) 为真，单宽度为0，举个例子，匹配指针并不提前。如果一个pattern不匹配，运算符 (?! pattern) 只返回真，但再一次说明，这是宽度为0，且不提前指针的。否定前瞻在我们解析一些复杂pattern但想要排除特殊情况时十分常用。举个例子，假设我们想要匹配句首的任何非 ”Volcano“起始的任意单个单词。我们可以这样用否定前瞻： /ˆ(?!Volcano)[A-Za-z]+/","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"/tags/NLP/"},{"name":"翻译","slug":"翻译","permalink":"/tags/翻译/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 3.12 副词和用于结束句子的助词","slug":"日语语法 3.12 副词和用于结束句子的助词","date":"2020-03-07T08:16:14.000Z","updated":"2020-03-07T08:16:14.000Z","comments":true,"path":"2020/03/07/日语语法 3.12 副词和用于结束句子的助词/","link":"","permalink":"/2020/03/07/日语语法 3.12 副词和用于结束句子的助词/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 3 基本语法3.12 副词和用于结束句子的助词3.12.1 副词的属性用法公式： （い形容词）早いく对于い形容词，把「い」换成「く」完成い形容词的副词化。 （な形容词）な形容词 + に对于な形容词，直接在な形容词后加上「に」完成な形容词的副词化。 示例： ボブは朝ご飯を早く⻝べた。Bob很快的吃了早饭。 アリスは自分の部屋をきれいにした。Alice向着干净整了她的房间。 注意：不是所有副词都是从形容词变化而来的，比如「全然」和「たくさん」本身就是副词，非活用而来。这些词可以像其他副词一样不带助词使用。 示例： 映画をたくさん見た。看了很多电影。 最近、全然⻝べない。最近什么都没吃。 示例:以下是更多的副词使用示例。 ボブの声は、結構大きい。Bob的声音相当大。 この町は、最近大きく変わった。这个镇子最近变化很大。 図書館の中では、静かにする。在图书馆里面，（我们）安静的做事情。 3.12.2 用来结束句子的助词用法公式： 句子 + ね人们希望别人赞同自己的时候，经常会在句尾加一个「ね」，意思类似「对吧？」、「不是吗？」[1]。[1] 我认为「ね」作为一个我理解的语气助词在不同场景的用法不同。比如下文示例1中 アリス：そうね。 此处的「ね」如果读成降调就不应该被理解为希望别人赞同自己，而应该被理解为表示语气的肯定。 示例： ボブ：いい天気だね。Bob：好天气，是吧?アリス：そうね。Alice：是那样，不是吗? アリス：おもしろい映画だったね。Alice：电影挺好看的，对吧?ボブ：え？全然おもしろくなかった。Bob：啊？不好看，一点也不好看。 用法公式： 句子 + よ当把「よ」加到句子结尾的时候，说话的人是想让对方知道新的信息。英语里面我们会说You know…，比如You know, I’m actually a genius。中文里，与其类似的是在句子的结尾加入「哦」。 示例： アリス：時間がないよ。Alice：知道不，没时间了哦。ボブ：大丈夫だよ。Bob：没事的哦。 アリス：今日はいい天気だね。Alice：今天好天气，是吧？ボブ：うん。でも、明日雨が降るよ。Bob：嗯，不过明天会下雨哦。 用法公式： 句子 + よね当你需要告诉别人新的信息，同时又期望对方同意的时候，可以这么用[1]。[1] 我认为「よね」连用不是两者用法的结合。从感觉上「よね」连用的使用场景与「ね」类似，但期待同意的语气更强。 示例： アリス：ボブは、魚が好きなんだよね。Alice：知道不，你喜欢吃⻥，不是吗?ボブ：そうだね。Bob：是这样啊。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 3.11 名词相关助词（と、や、とか、の）","slug":"日语语法 3.11 名词相关助词（と、や、とか、の）","date":"2020-02-26T13:33:54.000Z","updated":"2020-02-26T13:33:54.000Z","comments":true,"path":"2020/02/26/日语语法 3.11 名词相关助词（と、や、とか、の）/","link":"","permalink":"/2020/02/26/日语语法 3.11 名词相关助词（と、や、とか、の）/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 3 基本语法3.11 名词相关助词（と、や、とか、の）3.11.1 包含助词「と」用法公式： 名词 + と + 名词「と」用来连接两个或者两个以上的名词，表示确定「和」。 示例： ナイフとフォークでステーキを⻝べた。通过刀和叉吃了牛排。 本と雑誌と葉書を買った。买了书、杂志和明信片。 用法公式： 名词 + と + 动词と用来连接名词和动词，表示说话人与人或物一起完成了动作。 示例： 友達と話した。和朋友聊天了。 先生と会った。和老师⻅面了。 3.11.2 表示模糊罗列的助词「や」和「とか」用法公式： 名词 + や/とか + 名词「や」和「とか」用来连接两个或者两个以的名词，表示模糊的「和」，与此同时暗示还有一些名词没有被列举出来。其中，「とか」比「や」更加口语化。 示例： 飲み物 や カップ や ナプキンは、いらない？你不需要饮料、杯子或餐巾什么的（这些东西）？ 靴 とか シャツを買う。买鞋和衬衫什么的（这类东西）。 3.11.3 助词「の」用法公式： 名词 + の + 名词「の」连接两个名词时，前面的名词表示对后面名词的修饰或者后面名词是前面名词的附属。 示例： ボブは、アメリカの大学の学生だ。Bob 是美国的大学的学生。 如果上下文可以推断出来的话，被修饰的名词可以被省略掉。 示例： A：そのシャツは誰のシャツ？那件衬衫是谁的？B：ボブのシャツだ。它是Bob 的衬衫。 A：そのシャツは誰の？那件衬衫是谁的？B：ボブのだ。它是Bob 的。*「その」是「それ+ の」的缩写，因为已经包含了助词「の」，所以它可以直接修饰名词。从「これの」缩写为的「この」以及从「あれの」缩写为的「あの」也一样。 用法公式： い形容词/动词 + のい形容词/动词后接「の」表示形容词/动词的名词化。 示例： 白いのは、かわいい。白色的东西是可爱的。*此处的「の」与「物」等价，泛指广义的物。 授業に行くのを忘れた。忘记了去上课这件事。*此处的「の」与「こと」等价，泛指广义的事。 用法公式： な形容词 + な + のな形容词后接「の」表示形容词的名词化时，要注意在形容词和「の」之间加上「な」。 示例： 静かな部屋が、アリスの部屋だ。安静的房间是Alice 的房间。 静かなのが、アリスの部屋だ。安静的那个是Alice 的房间。 3.11.4 助词「の」用作解释助词「の」加在句子里最后一个从句末尾的时候还可以表达一种解释的口吻。比如有人问你是否有空，你也许会回答「问题是我现在有点忙」，这个抽象的广义名词「问题是…」可以用助词「の」来表达。这种句子隐含了解释的意味。 用法公式： 完整句子 + の在完整的一句话后加「の」表示一种解释性口吻。 示例： 今は忙しいの。问题是（我）现在有点忙。 不过这样听起来有些女性化。实际生活中成年男性都会再加上一个表陈述的「だ」，除非他们想卖萌。 示例： 今は忙しいのだ。问题是（我）现在有点忙。 不过因为「だ」不能被用在问句里，提问的时候只用「の」是没问题的，这是一种中性用法。 示例： 今は忙しいの？（你）是不是现在很忙？（中性） 如果要表达状态，且又要用「の」来体现解释的口吻，我们就要添加「な」来和仅表示「的」的「の」区分开。 示例：（用法的特例） ジムのだ。这是Jim 的。 ジムなのだ。这是Jim（解释的口吻）。 实际生活里面，虽然这种解释口吻很常用，但通常却都是用「んだ」来替代「のだ」。这也许是因为「んだ」比「のだ」更容易发音。这种语法可以表达很多意思，因为它不仅能和所有形态的形容词、名词和动词合用，它自己也可以像状态表示语那样活用。请参⻅下面的活用图表。 直接粘黏到各种活用形后面的「んだ」（把「の」或者「のだ」换成了「んだ」） 名词、な形容词 动词、い形容词 字典形 学生 なんだ 飲む んだ 否定式 学生じゃない んだ 飲まない んだ 过去形 学生だった んだ 飲んだ んだ 过去否定式 学生じゃなかった んだ 飲まなかった んだ 活用「んだ」（把「の」替换成「ん」，「の」或「のだ」替换成「んだ」） 名词、な形容词 动词、い形容词 字典形 学生 なんだ 飲む んだ 否定式 学生 なんじゃない 飲む んじゃない 过去形 学生 なんだった 飲む んだった 过去否定式 学生 なんじゃなかった 飲む んじゃなかった 示例： アリス：どこに⾏くの？Alice: Where is it that (you) are going?ボブ：授業に⾏くんだ。Bob: It is that (I) go to class.*Alice: Where are you going? (Seeking explanation)Bob: I’m going to class. (Explanatory) アリス：今、授業があるんじゃない？Alice: Isn’t it that there is class now?ボブ：今は、ないんだ。Bob: Now it is that there is no class.*Alice: Don’t you have class now? (Expecting that there is class)Bob: No, there is no class now. (Explanatory) アリス：今、授業がないんじゃない？Alice: Isn’t it that there isn’t class now?ボブ：ううん、ある。Bob: No, there is.*Alice: Don’t you not have class now? (Expecting that there is no class)Bob: No, I do have class. アリス：その⼈が買うんじゃなかったの？Alice: Wasn’t it that that person was the one to buy?ボブ：ううん、先⽣が買うんだ。Bob: No, it is that teacher is the one to buy.*Alice: Wasn’t that person going to buy? (Expecting that the person would buy)Bob: No, the teacher is going to. (Explanatory) アリス：朝ご飯を⾷べるんじゃなかった。Alice: It is that breakfast wasn’t to eat.ボブ：どうして？Bob: Why?*Alice: Should not have eaten breakfast, you know. (Explaining that breakfastwasn’t to be eaten)Bob: How come?","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 3.10 关系从句和语序","slug":"日语语法 3.10 关系从句和语序","date":"2020-02-26T08:42:58.000Z","updated":"2020-02-26T08:42:58.000Z","comments":true,"path":"2020/02/26/日语语法 3.10 关系从句和语序/","link":"","permalink":"/2020/02/26/日语语法 3.10 关系从句和语序/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 3 基本语法3.10 关系从句和语序3.10.1 把动词和状态表示语当作形容词对待你有没有注意到，很多形式的动词和状态表示语可以像い形容词那样来用？这是因为，他们某种意义上来说可以算是形容词。比如这个句子，「那个还没吃饭的人去了银行」，这里「还没吃饭」修饰了人，在日语里面你可以直接把「还没吃饭」当作普通形容词一样来修饰名词「人」。这样我们可以用任意的动词短语来修饰名词了！ 3.10.2 把状态表示从句当作形容词用用法公式： 名词 + だった/じゃない/じゃなかった + 名词状态表示从句当作形容词修饰后面的名词。*下划线的部分为状态表示从句。但该从句的状态不能为现在及将来，即「だ」；只能为过去「だった」，否定「じゃない」，过去否定「じゃなかった」。 示例： 学生じゃない人は、学校に行かない。不是学生的人不去学校。 子供だったアリスが立派な大人になった。曾经是小孩的Alice 成了一个优雅的大人。 友達じゃなかったアリスは、いい友達になった。曾经不是朋友的Alice 成了好朋友。 先週医者だったボブは、仕事を辞めた。上周还是医生的Bob 辞去了他的工作。 3.10.3 把动词从句当作形容词用用法公式： 名词 + を + 动作 + 名词动词从句当作形容词修饰后面的名词。*下划线的部分为动词从句。 示例： 先週に映画を見た人は誰？谁上周看过电影了？ ボブは、いつも勉強する人だ。Bob 是那个一直学习的人。 赤いズボンを買う友達はボブだ。买了红色裤子的朋友是Bob。 ご飯を食べなかった人は、映画で見た銀行に行った。没吃过饭的人去了她在电影里看到过的那个银行。 3.10.4 语句的语序日语中没有特定的语法顺序，但是一个完整的句子所必需的是动词。 示例：（以下均为语法完整、语序正确的句子） 私は 公園で お弁当を 食べた。 公園で 私は お弁当を 食べた。 お弁当を 私は 公園で 食べた。 弁当を 食べた。 食べた。 用法规则： 一个完整的句子需要在末尾有动词（这里包括隐式的状态表达）。示例：1.⻝べた2.学生（だ） 完整的句子（关系从句）可以用来修饰名词，形成嵌套关系从句的句子。但「だ」除外。示例：お弁当を食べた学生が公園に行った。吃过午饭的学生去了公园。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 3.9 他动词和自动词","slug":"日语语法 3.9 他动词和自动词","date":"2020-02-26T08:42:22.000Z","updated":"2021-11-26T17:53:45.060Z","comments":true,"path":"2020/02/26/日语语法 3.9 他动词和自动词/","link":"","permalink":"/2020/02/26/日语语法 3.9 他动词和自动词/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 3 基本语法3.9 他动词和自动词日语里面，有两种相同动作的动词通常被称为他动词和自动词[1]。两者的区别在于前者是由一个活动主体完成的动作，而后者则是在没有直接主体的情况下发生的动作。英语里面这两种情况一般用相同的动词来表示，比如”The ball dropped” 和”I dropped the ball”，但在日语里面就要写成「ボールが落ちた」和「ボールを落とした」。 [1] 此处译者译为他动词和自动词，这可能是日语语法中的称呼。原文中使用的是transitive和intransitive，即及物动词和不及物动词。 他动词和自动词 他动词 自动词 落とす 扔掉 落ちる 掉下 出す 拿出来 出る 出来 入れる 放入 入る 进入 開ける 打开 開く 被打开 閉める 关上 閉まる 被关上 つける 附加上 つく 被附上 消す 擦除 消える 消失 抜く 解开 抜ける 被解开 示例： 私が電気をつけた。把灯打开的人是我。 電気がついた。灯亮了。 電気を消す。关掉灯。 電気が消える。灯灭了。 誰が窓を開けた？谁开的窗？ 窓がどうして開いた？窗户为什么开了？","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 3.8 与动词一起使用的助词（を、に、へ、で）","slug":"日语语法 3.8 与动词一起使用的助词（を、に、へ、で）","date":"2020-02-22T07:04:48.000Z","updated":"2021-11-26T17:53:16.979Z","comments":true,"path":"2020/02/22/日语语法 3.8 与动词一起使用的助词（を、に、へ、で）/","link":"","permalink":"/2020/02/22/日语语法 3.8 与动词一起使用的助词（を、に、へ、で）/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 3 基本语法3.8 与动词一起使用的助词（を、に、へ、で）3.8.1 表示直接对象的助词「を」用法公式： 动作直接对象 + を + 动作「を」用来指示「を」前面的动作直接对象。 我们要学的第一个助词是对象助词，这个最直白。单词后面加上假名「を」就表示它是动作的直接对象。我们在别的地方是看不到这个假名的，所以它对应的片假名「ヲ」几乎不会被用到，因为助词基本都是用平假名写的。假名「を」虽然应该念作/wo/，但在真实生活中一般被念作/o/。以下是一些例子。 示例： 魚を⻝べる。吃⻥。 ジュースを飲んだ。喝了果汁。 与英语中我们熟悉的直接对象不同[1]，地点也可以作为「歩く」和「走る」这种动作动词的直接对象。因为动作动词是对地点做的，所以这在日语里是跟直接对象的概念一致的。然而，如下例所示，翻译到英语里面却有变化，因为两种语言里面直接对象的概念是有点区别的。 [1] Kim的原文中提到，日语的地点也可以歩く」和「走る」这种动作动词的直接对象。这个特点与英语不同：比如英文中，没有 run expressway 这种说法。但这个特点与中文有些情况相同：比如中文中，可以将其表述为 跑高速 。 示例： 街をぶらぶら歩く。无目标的在街上闲逛。（字面：无目标的走街）Aimlessly walk through town. (Lit: Aimlessly walk town) 高速道路を走る。跑在高速路上。（字面：跑高速路。）Run through expressway. (Lit: Run expressway) 当你合用名词和「する」时，助词「を」可以省略，你可以把整个[ 名词+ する] 当作一个动词[1]。 [1] 我通常将[名词+ する]这个组合成为名词的动词化。 示例： 毎日、日本語を勉強する。每天学习日语。 メールアドレスを登録した。注册了电邮地址。 3.8.2 目标助词「に」用法公式： 地点 + に + 动词「に」用来指示前面的地点是动词的目标。 地点 + に + 存在动词「に」用来指示前面的地点是名词存在的位置。 名词 + に + 动词「に」用来指示前面的名词是后面动词的目标。 时间 + に「に」用来强调后面的动作在前面的时间发生。 示例： ボブは日本に行った。Bob 去过日本。 猫は部屋にいる。猫在房间里。 いい友達に会った。⻅过了好朋友。 ジムは医者になる。Jim 将成为医生。 先週に図書館に行った。上周去过了图书馆。 3.8.3 方向助词「へ」虽然「へ」的读音应该是/he/，但作为助词时要读成/e/ （え）。「に」和「へ」的主要区别在于「に」表示动作向着一个最终的、期望中的目标（具体或抽象）发生，而助词「へ」则用来表达朝着目标的方向进发，因此，它只和动作动词合用，同时也不能保证目标是最终的、所期望的，只能说朝那个方向前进[1]。 [1] 换句话说，「に」强调结果（最终目标），「へ」强调过程（朝方向前进）。 换句话说，助词「に」强调终点，而助词「へ」对此则比较模糊。比如我们把前面例子里的「に」换成「へ」的话，句子意思就有了微妙的变化。 示例： ボブは日本へ行った。Bob往日本去了。 注意动词如果没有具体方向的话是不能用「へ」的。例如下面这句是不对的。 示例： 医者へなる。「医者になる」的语法错误版本。 但这并不意味着「へ」不能描述抽象概念。实际上因为表示的方向是模糊的，助词「へ」也可以用于谈论向着特定的未来目标或期望努力。 示例： 勝ちへ向かう。向着胜利进发。 3.8.4 上下文助词「で」用法公式： 地点 + で + 动词「で」用来指示动词发生的地点。 名词 + で + 动词「で」用来指示动词借助的手段或者方式。 示例： 映画館で見た。在电影院看了。 バスで帰る。坐巴士回家。 用法公式：配合「何」使用「で」 何 + で + 动词用于询问动作借助的名词，即方式。 「什么」这个词（何）很讨厌，因为它虽然通常读作「なに」，但有时候又要读成「なん」，这取决于它的用法。但它又总是用汉字写，所以你还分不出来。我建议你就一直使用「なに」，直到有些时候别人纠正你应该读成「なん」。跟助词「で」一起用的时候，它也读作「なに」。 示例： 何（なに）できた？通过什么来的？ バスできた。通过巴士来的。 接下来就容易让人混乱了。单词「为什么」有一个口语版本，它比更正式一点的「どうして」或者更有力一点的「なぜ」用的都多，写作「何で」，但却读作「なんで」。这是一个完全独立的单词，跟助词「で」没有半点关系。 示例： 何で（なんで）きた？你为什么来？ 暇だから。因为我有空。 3.8.5 当主题是地点时有时候动作发生的地点也是句子的主题，这种情况下你可以把主题助词（「は」和「も」）加在三个可以指代地点的助词（「に」、「へ」、「で」）后面，如下例所示。 示例： ボブ：学校に行った？Bob：（你）去学校了吗？アリス：行かなかった。Alice：没去。ボブ：図書館 には ？Bob：那图书馆呢？アリス：図書館 にも 行かなかった。Alice：也没去图书馆。 ボブ：どこで⻝べる？Bob：哪里吃？アリス：イタリアレストランではどう？Alice：意大利餐馆如何？ 3.8.6 当主题是直接对象时直接对象助词跟指代地点的助词不同，你不能同时使用其他助词。例如回到前面的例子，你也许会想用「をは」来表示直接对象同时也是主题，但事实并非如此。直接对象可以不使用「を」就成为主题。 实际上，把「を」加进去反而错了。 示例： 日本語を習う。学日语。 日本語は、習う。关于日语，（会去）学习。 示例： 日本語をは、習う。（这样不对。）","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 3.7 动词的过去式","slug":"日语语法 3.7 动词的过去式","date":"2020-02-19T08:52:59.000Z","updated":"2021-11-26T17:52:32.486Z","comments":true,"path":"2020/02/19/日语语法 3.7 动词的过去式/","link":"","permalink":"/2020/02/19/日语语法 3.7 动词的过去式/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 3 基本语法3.7 动词的过去式动词的否定式在大家的日语中用 た形 表示。 3.7.1 る动词的过去形用法公式： 出る + た = 出た把「る」去掉换成「た」。 示例： ご飯は、食べた。对于饭来说，吃过了。 映画は、全部見た。对于电影来说，全看过了。 3.7.2 う动词的过去形用法公式：う动词的过去形活用 结尾 非过去形 改为…… 过去形 例外 す 話す す→ した 話した する→した く 書く く→ いた 書いた くる→きた ぐ 泳ぐ ぐ→ いだ 泳いだ 行く行った* む 飲む む→ んだ 飲んだ ぬ 死ぬ ぬ→ んだ 死んだ ぶ 遊ぶ ぶ→ んだ 遊んだ る 切る る→った 切った つ 持つ つ→った 持った う 買う う→った 買った 示例： 今日は、走った。至于今天，跑了。 友達が来た。来过的那个是朋友。 私も遊んだ。我也玩过了。 勉強は、した。关于学习，已经做了。 3.7.3 所有动词的过去否定式用法公式： 行かない + かった = 行かなかった在动词否定式基础上，把「い」去掉换成「かった」表示过去否定。 示例： アリスは⻝べなかった。对Alice 来说，没有吃过。 ジムがしなかった。Jim 是没有做过的那个。 ボブも行かなかった。Bob 也没有去过。 お金がなかった。没有过钱。（字面：对于钱来说，不曾存在过。） 私は買わなかった。对于我来说，没有去买。 猫はいなかった。没有过猫。（对于猫来说，不曾存在过。）","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 3.6 动词的否定式","slug":"日语语法 3.6 动词的否定式","date":"2020-02-19T07:51:09.000Z","updated":"2021-11-26T17:51:48.677Z","comments":true,"path":"2020/02/19/日语语法 3.6 动词的否定式/","link":"","permalink":"/2020/02/19/日语语法 3.6 动词的否定式/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 3 基本语法3.6 动词的否定式动词的否定式在大家的日语中用 ない形 表示。 3.6.1 将动词活用为否定式用法公式： （对る动词）⻝べる + ない = ⻝べない把「る」去掉换成「ない」。 （对以「う」结尾的う动词） 買う + わ + ない= 買わない把「う」换成「わ」，再加上「ない」 （其他う动词）待つ + た = 待たない把末假名换成同行的あ段假名，再加上「ない」。 （例外）する→ しない （例外）くる→ こない （例外）ある→ ない *Tae Kim’s Guide中文版中，译者注：加上「ない」之前的活用形在语法上称为「未然形」。 动词否定式示例 る动词 う动词 例外 見る→ 見ない 話す→ 話さない する→ しない ⻝べる→ ⻝べない 聞く→ 聞かない くる→ こない 寝る→ 寝ない 泳ぐ→ 泳がない *ある→ない 起きる→ 起きない 遊ぶ→ 遊ばない 考える→ 考えない 待つ→ 待たない 教える→ 教えない 飲む→ 飲まない 出る→ 出ない *買う→ 買わない 着る→ 着ない 帰る→ 帰らない いる→ いない 死ぬ→ 死なない 示例： アリスは食べない。对Alice 来说，不吃。 ジムが遊ばない。Jim 是不玩的那个人。 ボブもしない。Bob 也不做。 お金がない。没有钱。(字面：钱是不存在的那样东西。） 私は買わない。对我来说，不买。 猫はいない。没有猫。（字面：对于猫来说，不存在。）","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 3.5 动词基础","slug":"日语语法 3.5 动词基础","date":"2020-02-18T15:23:50.000Z","updated":"2021-11-26T17:51:10.609Z","comments":true,"path":"2020/02/18/日语语法 3.5 动词基础/","link":"","permalink":"/2020/02/18/日语语法 3.5 动词基础/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/以及《大家的日语》中文版（外研社出版）进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 3 基本语法3.5 动词基础词汇学习： ⻝べる【た・べる】（る动词）- 吃 分かる【わ・かる】（う动词）- 了解 見る【み・る】（る动词）- 看 寝る【ね・る】（る动词）- 睡 起きる【お・きる】（る动词）- 叫醒；发生 考える【かんが・える】（る动词）- 考虑 教える【おし・える】（る动词）- 教；告知 出る【で・る】（る动词）- 出来 いる（る动词）- 存在（有生命） 着る【き・る】（る动词）- 穿 話す【はな・す】（う动词）- 说 聞く【き・く】（う动词）- 问；听 泳ぐ【およ・ぐ】（う动词）- 游泳 遊ぶ【あそ・ぶ】（う动词）- 玩 待つ【ま・つ】（う动词）- 等待 飲む【の・む】（う动词）- 喝 買う【か・う】（う动词）- 买 ある（う动词）- 存在（无生命） 死ぬ【し・ぬ】（う动词）- 死 する（例外）- 做 来る【く・る】（例外）- 来 お金【お・かね】- 钱 私【わたし】- 我，我自己 猫【ねこ】- 猫 3.5.1 动词概述日语里面的动词总是放在句尾。因为我们还没有学习如何使用一个以上的从句，所以目前所有含动词的句子都必须以动词结尾。为了介绍活用规则，我们要先学习三种主要的动词种类。但在这之前，有一件事要切记： 仅需 一个动词就可以构成一个语法上完整的句子（包括表示状态）。 示例： ⻝べる。吃。（可能对应的翻译包括我吃、她吃、他们吃） 3.5.2 将动词归类为る动词和う动词在学习活用动词前，我们先来学习动词是如何分类的。除了两个特例之外，所有动词可分类为る动词（一段动词）或者う动词（五段动词）。 所有る动词以「る」结尾，而う动词则以包括「る」在内的多种う元音结尾。所以，一个不是以「る」结尾的动词肯定是う动词。对于以「る」结尾的动词，如果在「る」音之前的假名包含/a/、/u/ 或者/o/元音，那它就是う动词，而前面是/i/ 或者/e/ 元音的，那它大多数情况下就是る动词。一些常⻅的例外列在本节末尾。 示例： ⻝べる- 「べ」含有え元音，所以是る动词 分かる- 「か」含有あ元音，所以是う动词 不同类型动词的示例 る动词 う动词 例外 見る 話す する ⻝べる 聞く 来る 寝る 泳ぐ 起きる 遊ぶ 考える 待つ 教える 飲む 出る 買う いる ある 着る 死ぬ 示例： アリスは食べる。对Alice 来说，吃。 ジムが来る。来的那个人是Jim。Jim is the one who comes. ボブもする。Bob 也做了。（Bob also do.） お金がある。有钱。（字面：钱是存在的那个东西。） 私は買う。对于我来说，买。 猫はいる。有猫。（字面：对于猫来说，它存在于那里）*对于示例4和6，我认为「は」和「が」不能单纯地用指示主语和指示识别对象进行区别，实际情况比这个要复杂。在大家的日语初级1L14文法中，提到 名词+が+动词 用作客观地将五官所感传达出来，可以用于解释此处的用法。我认为示例6的用法较为少见，个人认为示例6的用法倾向于强调猫的存在，即活着。即表达 猫は生きている。 3.5.3 动词的分类附在大家的日语中，动词首先都是学的ます形，后学的辞書形。在活用过程中，书将动词分为了1类动词、2类动词、3类动词三类。1类动词对应上文中的う动词，2类动词对应上文的る动词，3类动词对应上文的例外。 示例： 食べます→⻝べる 話します→話す します→する 而针对ます型，我总结的分类方法是：（不保证正确，但能应对大多数情况） 1型动词（う动词）均为以 i 为辅音结尾的动词（充分不必要）。其中，以 り 结尾的1型动词即为上文中特殊的，辞書形以 る 结尾的う动词。 2型动词大多数以 e 辅音结尾（必要不充分），还有少部分以 i 为辅音结尾。*对于2型动词需要进行针对性记忆，因为2型动词在活用时，以动词整体为基础进行变形。 3型动词即为特殊性，除了上文中的例外之外，还包含一些名词动词化的动词，例如 勉強します（辞書形为 勉強する）","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 3.4 形容词","slug":"日语语法 3.4 形容词","date":"2020-02-17T11:27:07.000Z","updated":"2021-11-26T17:50:43.910Z","comments":true,"path":"2020/02/17/日语语法 3.4 形容词/","link":"","permalink":"/2020/02/17/日语语法 3.4 形容词/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar&amp;译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 3 基本语法3.4 形容词3.4.1 形容词概述形容词用来修饰形容词后的名词，主要分为な形容词和い形容词两类。 3.4.2 な形容词用法公式： な形容词 ＋ な + 名词将な形容词和名词用「な」连接起来，这就是な形容词这个名字的由来。 示例： 静かな人。安静的人。 きれいな景色。漂亮的风景。 用法公式： 主题 + は + な/い形容词使用「は」将主题和形容词连接起来表示形容词对主题的修饰。 示例： 友達は親切。朋友是和善的。 友達は親切な人だ。朋友是和善的人。 用法公式： な形容词的活用规则跟名词是一样的。详见 3.2 状态表示 名词 + が + 好き/嫌い/上手/下手部分形容词的宾语用「が」和形容词相连，表示对什么东西（名词）怎么样（喜欢/讨厌/擅长/不擅长） 示例： ボブは魚が好きだ。Bob 喜欢⻥。 ボブは魚が好きじゃない。Bob 不喜欢⻥。 ボブは魚が好きだった。Bob 以前喜欢⻥。 ボブは魚が好きじゃなかった。Bob 以前不喜欢⻥。 示例 魚が好きじゃない人は、肉が好きだ。不喜欢⻥的人喜欢肉。 魚が好きな人は、野菜も好きだ。喜欢⻥的人也喜欢蔬菜。*此处的「も」不用来指示主题，而用来代替「が」指示宾语，同时表示逻辑与上文的一致性，即 也 喜欢。 3.4.3 い形容词所有い形容词都是以平假名「い」结尾的。但你也许注意到了，有些な形容词也是以「い」结尾，例如「きれい（な）」。那应该怎么分辨呢？实际上，不是用汉字写的但以「い」结尾的な形容词非常少⻅。两个最常⻅的例子是「きれい」和「嫌い」。所有其他以「い」结尾的な形容词基本都是用汉字写的，所以你可以很容易分辨出来它们不是い形容词。比如，「きれい」写成汉字就是「綺麗」或者「奇麗」。因为「麗」里面包含了「い」，你就能知道它肯定不是い形容词。因为い形容词以「い」结尾的全部意义就在于活用的时候改变「い」而汉字不动。实际上，「嫌い」是极少几个以「い」而非汉字结尾的な形容词之一，这是因为「嫌い」实际上是从动词「嫌う」派生而来的。与な形容词不同，你无需在用い形容词修饰名词的时候添加一个「な」。 用法公式： い形容词（包含い）+ 名词い形容词 与 名词直接连接表示对名词的修饰。 示例： 嫌いな⻝べ物。不喜欢的⻝物。 おいしい⻝べ物。可口的⻝物 用法公式：い形容词活用 （否定）高いくない去掉 い ，在结尾加上 くない 表示否定。 （过去）高いかった去掉 い ，在结尾加上 かった 表示过去。 （过去否定）高くないかった在否定基础上，去掉 い ，在结尾加上 かった 表示过去否定。 3.4.4 形容词活用规则中的例外只有一个意思是「好」的い形容词跟其他い形容词不一样。「好」这个词原本是「よい（良い）」，但后来逐渐演变成「いい」，而写成汉字时，又通常读成「よい」，所以「いい」基本上总是用平假名写。到此为止一切都没问题。不幸的是，这个词的活用形却又都是从「よい」而非「いい」派生出来。 用法公式：「いい」活用 肯定 否定 非过去 いい よくない 过去 よかった よくなかった 用法公式：「かっこいい」活用 肯定 否定 非过去 かっこいい かっこよくない 过去 かっこよかった かっこよくなかった 「かっこいい」是两个单词合并缩写而成：「格好」和「いい」，与「いい」活用类似。 3.4.5 形容词总结な形容词 肯定 否定 非过去 好き（だ） 好きじゃない 过去 好きだった 好きじゃなかった い形容词 肯定 否定 非过去 高い 高くない 过去 高かった 高くなかった","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 3.3 助词介绍（は、も、が）","slug":"日语语法 3.3 助词介绍（は、も、が）","date":"2020-02-16T16:53:09.000Z","updated":"2021-11-26T17:49:55.972Z","comments":true,"path":"2020/02/17/日语语法 3.3 助词介绍（は、も、が）/","link":"","permalink":"/2020/02/17/日语语法 3.3 助词介绍（は、も、が）/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar&amp;译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 3 基本语法3.3 助词介绍（は、も、が）3.3.1 用助词来定义语法功能助词是用来定义语法功能的，这些助词可以帮助表达正确且唯一的语意。 3.3.2 「は」主题助词用法公式： 主题 ＋ はは 用来指示前面的部分是这句话谈论的主题。 示例： ボブ：アリスは学生？Bob：（你）Alice 是学生吗？アリス：うん、学生。Alice：对，（我）是。 アリス：今日は試験だ。Alice：今天是考试。ボブ：ジョンは？Bob：那John 呢？アリス：ジョンは明日。Alice：John 是明天。（对John 来说，考试是明天。） 3.3.3 「も」包含主题助词用法公式： 主题 ＋ もも 用来指示前面的部分是这句话谈论的主题，且与上文的逻辑相同，表示“也”。*加粗部分为与「は」主题助词的区别 示例： 肯定情况：ボブ：アリスは学生？Bob：（你）Alice 是学生吗？アリス：うん、トムも学生。Alice：是，Tom也是学生。*使用「も」要注意一致性，比如说「我是学生，Tom 也不是学生。」就很奇怪了。这种情况下，可以用「は」新开一个主题，如下所示。 转折情况ボブ：アリスは学生？Bob：（你）Alice 是学生吗？アリス：うん、でもトムは学生じゃない。Alice：是，但Tom 不是学生。*此处，でも 表示语意转折。 否定情况：ボブ：アリスは学生？Bob：（你）Alice 是学生吗？アリス：ううん、トムも学生じゃない。Alice：不是，Tom 也不是学生。 3.3.4 「が」识别助词用法公式： 识别对象 ＋ がが 用来指示前面的部分时说话者想要进行识别的特定对象。 示例： ボブ：誰が学生？Bob：是学生的那个人是谁？Bob: Who is the one that is student?アリス：ジョンが学生。Alice：John 是那个学生。Alice: John is the one who is student.*「が」前面的对象是说话者需要识别的对象 「が」与「は」辨析：誰が学生？是学生的那个人是谁？Who is the one that is student?学生は誰？（那个）学生是谁？As for that student, who is he/her?*使用「が」时，说话者不能将「が」前的对象和现实建立联系，还需要识别，尚处于抽象概念阶段；使用「は」时，说话者已经将「は」前的对象和现实建立了联系，不需要识别，已经成为了有意义的主题。 私は学⽣。我是学生As for me, I am a student.私が学⽣。我是那个（是学生的）人I am the one who is a student. 3.3.5 助词总结 助词 用法 「は」 用来指示前面的部分是这句话谈论的主题 「も」 用来指示前面的部分是这句话谈论的主题，且与上文的逻辑相同，表示“也” 「が」 用来指示前面的部分时说话者想要进行识别的特定对象","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"日语语法 3.2 状态表示","slug":"日语语法 3.2 状态表示","date":"2020-02-16T15:31:32.000Z","updated":"2021-11-26T17:48:45.817Z","comments":true,"path":"2020/02/16/日语语法 3.2 状态表示/","link":"","permalink":"/2020/02/16/日语语法 3.2 状态表示/","excerpt":"","text":"本文参考Tae Kim’s Guide to Learning Japanese http://www.guidetojapanese.org/learn/grammar&amp;译者pizzamx翻译的中文译本 https://res.wokanxing.info/jpgramma/进行日语语法学习。部分示例及理解摘自上述原文中，部分示例加入了我自己的理解。 Chapter 3 基本语法3.2 状态表示3.2.1 「だ」表示某物的现在状态用法公式： 名词/な形容词 ＋ だ表示 现在状态。 示例： 人だ。是人。 綺麗だ。是美丽的。 注意： 在表示现在状态的时候，だ有时候也会被省略。 相比省略だ的句式，不省略だ的句式语气更重。 示例： Ａ：元気？甲：你好吗？Ｂ：元気。乙：我很好。 3.2.2 否定状态用法公式： 名词/な形容词 ＋ じゃない表示 现在状态的否定。 示例： 学生じゃない。不是学生。 静かじゃない。不是静止的。 3.2.3 过去状态&amp;过去状态的否定用法公式： 名词/な形容词 ＋ だった表示 过去状态 名词/な形容词 ＋ じゃなかった表示 过去状态的否定 示例： 友達だった。以前是朋友。 元気じゃなかった。曾经不好。 3.2.4 状态表示总结 肯定 否定 非过去 学生（だ） 是学生 学生じゃない 不是学生 过去 学生だった 以前是学生 学生じゃなかった 以前不是学生","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"日语","slug":"日语","permalink":"/tags/日语/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"github.io+Hexo实现博客托管","slug":"github.io+Hexo实现博客托管","date":"2020-02-14T07:45:00.000Z","updated":"2021-11-26T07:48:29.092Z","comments":true,"path":"2020/02/14/github.io+Hexo实现博客托管/","link":"","permalink":"/2020/02/14/github.io+Hexo实现博客托管/","excerpt":"","text":"github.io+Hexo实现博客托管1 准备工作1.1 安装Node.js Node.js安装地址过程：全程无脑下一步*不要check让你安装额外组件的框框 目的：是为了进一步使用npm进行Hexo的安装。验证：命令行输入npm后回显正常（如下图所示） 1.2 安装Hexo博客框架过程：命令行输入 npm install hexo -g 目的：安装Hexo框架验证：命令行输入hexo之后回显正常（如下图所示） 1.3 Hexo的本地初始化过程：命令行进入你想要存放Hexo的位置，输入hexo init在指定位置初始化，输入hexo s启动本地服务，默认位置是 http://localhost:4000。 hexo init hexo s 目的：为Hexo初始化，创建本地地址。验证：访问 http://localhost:4000。 1.4 GitHub账号注册过程：自行注册免费版的GitHub账号，需要注意的是免费版的个人空间只有500MB。 1.5 Gitpage的初始化1.5.1 域名初始化过程：新建一个 Github用户名.github.io 的repository。目的：这样你可以直接通过 Github用户名.github.io 访问你的博客。 1.5.2 SSH配置过程：下载Git for Windows https://gitforwindows.org/。在Git Bash中输入 cd ~/.ssh 检查是否使用过ssh，如果是第一次使用则会返回 No such file or directory。 在命令行中输入 ssh-keygen -t rsa -C &quot;Github注册使用的邮箱&quot; 连续回车三次，在 C:\\Users\\你的电脑用户名.ssh 中找到 id_rsa.pub，用记事本复制其中内容。打开Github主页，进入Settings-SSH and GPG keys-New SSH key，写好Title，并将复制的内容粘贴到Key中。 验证是否连接成功 ssh -T git@github.com 目的：为了方便将本地的页面推送到云端，github提供了ssh服务以节省输入用户名密码的时间。 1.5.3 Hexo配置首先，命令行输入 npm install hexo-deployer-git 安装Hexo的github部署器。 在Hexo根目录下的_config.yml文件中修改 deploy: type: git repo: github: git@github.com:Github用户名/Github用户名.github.io branch: master 之后就可以直接上传到GitHub的指定位置了。 2 Gitpage的文件上传在进行完上述准备工作后，命令行进入Hexo根目录，运行 Hexo g Hexo d 生成html文件并部署到云端。之后我们就可以直接访问 Github用户名.github.io 查看我们的博客了。 3 主题切换将网上开源的主题包打包下载后放置到Hexo/themes的位置，并在根目录下的_config.yml文件中修改 theme: 你下载的主题包名称 主题中很多部分可能用了各种开源插件，想要在原作者基础上自定义需要细心查找相关插件的文档说明，理解作者写的layout代码，并对照主题原作者的代码进行自定义修改。","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"/tags/Hexo/"},{"name":"博客","slug":"博客","permalink":"/tags/博客/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"opencv边缘检测 C++实现","slug":"opencv边缘检测 C++实现","date":"2020-02-12T06:37:28.000Z","updated":"2021-11-26T17:48:07.908Z","comments":true,"path":"2020/02/12/opencv边缘检测 C++实现/","link":"","permalink":"/2020/02/12/opencv边缘检测 C++实现/","excerpt":"","text":"1 opencv C++ 环境配置1.1 C++环境安装visual studio 2019 community version 下载地址： https://visualstudio.microsoft.com/zh-hans/vs/community/ 双击安装包进入安装界面，选择C++开发包之后点击安装，等待程序安装完成。 1.2 opencv环境配置1.2.1 opencv 下载安装包 下载地址：https://sourceforge.net/projects/opencvlibrary/files/4.2.0/opencv-4.2.0-vc14_vc15.exe/download 双击安装包进入安装界面，并将其安装到一个方便你找到的文件夹中。比如：C:\\opencv，之后的opencv目录将以该目录作为安装根目录为例。 1.2.2 环境变量配置1 打开控制面板 - 所有控制面板项 - 系统。 2 点击左侧的高级系统设置。 3 选择右下的环境变量。 4 双击下方系统变量的Path。 5 在Path中新建一个环境变量，地址是C:\\opencv\\build\\x64\\vc15\\bin。 6 连点应用确认完成配置 1.2.3 VS2019环境配置1 打开VS2019，create new project - 选择C++ - 选择empty project。 2 进入主界面之后，点击左上角view - other windows - property manager，调出属性管理器。 3 在属性管理器的Debug | 64选项右键选择Properties。 4 在Debug Property Pages界面左侧选择VC++ Directories，将右侧的Include Directories添加C:\\opencv\\build\\include，将Library Directories添加C:\\opencv\\build\\x64\\vc15\\lib。 5 在Debug Property Pages界面左侧选择Linker，将右侧的Additional Dependencies添加opencv_world420.lib。 *若你装的版本不是4.2.0，则将420换为你安装的版本的版本号。 6 之后关闭Debug Property Pages界面，确认VS主界面上方的Debug使用的是x64版本，若不是则选择x64。 至此，整个C++ opencv 的VS环境配置完毕。顺便，该配置只针对当前创建的Project有效，若创建新的Project后，需重新进行以上配置。 2 opencv的基本输入输出以及指针调用2.1 头文件#include &lt;opencv2/opencv.hpp&gt; //opencv头文件 #include &lt;iostream&gt; //C++基本输入输出 //名空间声明 using namespace cv; using namespace std; 2.2 图像IO2.2.1 图像输入Mat In = imread(&quot;C:\\\\test.png&quot;, IMREAD_GRAYSCALE); imread函数是opencv库中的基本输入函数，可以将.png.jpg.bmp等格式的图片读入，并转化为opencv的图片矩阵类型Mat。imread的第一个输入变量为文件的具体位置，第二个变量IMREAD_GRAYSCALE表示读入灰度图，若不加这条则会读入RGB图。 2.2.2 图像复制在opencv中，如果使用赋值手法进行图像的复制，如下所示： Mat Out = In; 则会将In的每个像素位置的指针赋给Out，不论对In还是Out进行修改时，另外一个变量也会跟着进行变化。因此，想要进行图像复制要用Mat类型的clone()方法： Mat Out = In.clone; 2.2.3 图像显示输出imshow(&quot;Output&quot;, Out); imshow函数是opencv库中的基本输出函数，可以将Mat类型的图像显示在屏幕上。imshow的第一个变量是图像显示在屏幕上的文本内容，第二个变量是选择输出的Mat类型变量。 *注意：如果两个输出图像的文本相同则只会输出先输出的一个图像，因此不要将输出图像的文本定义为一样的文本。 2.2.4 图像保存输出imwrite(&quot;C:\\\\Out.png&quot;, Output); imwrite函数是opencv库的基本输出函数，可以进行指定目录的图片写入功能。imwrite的第一个变量是保存输出图片的具体位置，第二个变量是选择输出的Mat类型的变量。 2.3 图片的特定像素提取下面以一个阈值法进行边缘检测为例说明像素提取的方法： Mat Output = Input.clone(); for (int x = 1; x &lt; Input.rows - 1; x++) { uchar* out = Output.ptr&lt;uchar&gt;(x); uchar* current = Input.ptr&lt;uchar&gt;(x); uchar* last = Input.ptr&lt;uchar&gt;(x - 1); for (int y = 1; y &lt; Input.cols - 1; y++) { if (abs(current[y] - current[y - 1]) &gt; K || abs(current[y] - last[y]) &gt; K) //Threshold K { out[y] = 255; continue; } else { out[y] = 0; } } } 其中， uchar* out = Output.ptr&lt;uchar&gt;(x); 表示将第 x 行的第一个像素指针赋给out。 out[y] = 255; 表示将第 x 行的第 y 个像素值赋值255。 另外，上述像素操作建立在灰度图的基础上。若对于RGB彩色图，官方的Mat类型将以下图形式存储像素值数据 [1]： [1]https://docs.opencv.org/master/db/da5/tutorial_how_to_scan_images.html 3 边缘检测算法3.1 阈值法流程图: 代码: void Threshold_Method(Mat Input, int K) { Mat Output = Input.clone(); for (int x = 1; x &lt; Input.rows - 1; x++) { uchar* sum = Output.ptr&lt;uchar&gt;(x); uchar* current = Input.ptr&lt;uchar&gt;(x); uchar* last = Input.ptr&lt;uchar&gt;(x - 1); for (int y = 1; y &lt; Input.cols - 1; y++) { if (abs(current[y] - current[y - 1]) &gt; K || abs(current[y] - last[y]) &gt; K) //Threshold K { //sum[y] = 255; continue; } else { sum[y] = 0; } } } imshow(&quot;Threshold_Method&quot;, Output); } 结果： 3.2 差分法流程图： 代码： void Difference_Method(Mat Input) { Mat Output = Input.clone(); for (int x = 1; x &lt; Input.rows - 1; x++) { uchar* sum = Output.ptr&lt;uchar&gt;(x); uchar* current = Input.ptr&lt;uchar&gt;(x); uchar* last = Input.ptr&lt;uchar&gt;(x - 1); for (int y = 1; y &lt; Input.cols - 1; y++) { float horizontal = pow(current[y] - current[y - 1], 2); float vertical = pow(current[y] - last[y], 2); sum[y] = saturate_cast&lt;uchar&gt;(sqrt(horizontal + vertical)); //output = sqrt(delta_x ^ 2 + delta_y ^ 2) } } imshow(&quot;Difference_Method&quot;, Output); } 结果： 3.3 Sobel方法流程图： 代码： void Sobel_Method(Mat Input, const char* Str) { Mat Output = Input.clone(); for (int x = 1; x &lt; Input.rows - 1; x++) { uchar* sum = Output.ptr&lt;uchar&gt;(x); uchar* current = Input.ptr&lt;uchar&gt;(x); uchar* next = Input.ptr&lt;uchar&gt;(x + 1); uchar* last = Input.ptr&lt;uchar&gt;(x - 1); for (int y = 1; y &lt; Input.cols - 1; y++) { /* horizontal [ -1 0 1 -2 0 2 -1 0 1 ] */ float horizontal = pow(-last[y - 1] + last[y + 1] - 2 * current[y - 1] + 2 * current[y + 1] - next[y - 1] + next[y + 1] , 2); /* vertical [ -1 -2 -1 0 0 0 1 2 1 ] */ float vertical = pow(-last[y - 1] - 2 * last[y] - last[y + 1] + next[y - 1] + 2 * next[y] + next[y + 1] , 2); sum[y] = saturate_cast&lt;uchar&gt;(sqrt(horizontal + vertical)); } } imshow(Str, Output); } 结果： 3.4 拉普拉斯方法流程图： 代码: void Laplacian_Method(Mat Input, const char* Str) { Mat Output = Input.clone(); for (int x = 1; x &lt; Input.rows - 1; x++) { uchar* sum = Output.ptr&lt;uchar&gt;(x); uchar* current = Input.ptr&lt;uchar&gt;(x); uchar* next = Input.ptr&lt;uchar&gt;(x + 1); uchar* last = Input.ptr&lt;uchar&gt;(x - 1); for (int y = 1; y &lt; Input.cols - 1; y++) { /* [ 0 1 0 1 -4 1 0 1 1 ] */ sum[y] = saturate_cast&lt;uchar&gt;(last[y] + current[y - 1] - 4 * current[y] + current[y + 1] + next[y]) * 20; } } imshow(Str, Output); } 结果： 3.5 高斯滤波流程图： 代码： Mat Gaussian_Filter_5(Mat Input, int K) { Mat Output = Input.clone(); for (int k = 1; k &lt; K + 1; k++) { Mat Output_buff = Output.clone(); for (int x = 2; x &lt; Input.rows - 2; x++) { uchar* sum = Output.ptr&lt;uchar&gt;(x); uchar* current = Output_buff.ptr&lt;uchar&gt;(x); uchar* next = Output_buff.ptr&lt;uchar&gt;(x + 1); uchar* next_ = Output_buff.ptr&lt;uchar&gt;(x + 2); uchar* last = Output_buff.ptr&lt;uchar&gt;(x - 1); uchar* last_ = Output_buff.ptr&lt;uchar&gt;(x - 2); for (int y = 2; y &lt; Input.cols - 2; y++) { /* [ 1 4 6 4 1 4 16 24 16 4 6 24 36 24 6 4 16 24 16 4 1 4 6 4 1 ] */ sum[y] = saturate_cast&lt;uchar&gt;((+1 * last_[y - 2] + 4 * last_[y - 1] + 6 * last_[y] + 4 * last_[y + 1] + 1 * last_[y + 2] + 4 * last[y - 2] + 16 * last[y - 1] + 24 * last[y] + 16 * last[y + 1] + 4 * last[y + 2] + 6 * current[y - 2] + 24 * current[y - 1] + 36 * current[y] + 24 * current[y + 1] + 6 * current[y + 2] + 4 * next[y - 2] + 16 * next[y - 1] + 24 * next[y] + 16 * next[y + 1] + 4 * next[y + 2] + 1 * next_[y - 2] + 4 * next_[y - 1] + 6 * next_[y] + 4 * next_[y + 1] + 1 * next_[y + 2]) * 0.00390625); } } } return Output; } 结果： 3.6 高斯滤波后的Sobel流程图： 代码： Sobel_Method(Gaussian_Filter_5(In, 2), &quot;Sobel_Method_with_Gaussian_Filter&quot;); 结果： 3.7 高斯滤波后的拉普拉斯方法流程图： 代码： Laplacian_Method(Gaussian_Filter_5(In, 2), &quot;Laplacian_Method_with_Gaussian_Filter&quot;); 结果： 3.8 基于拉普拉斯方法的锐化算法流程图： 代码： void Sharpening_based_on_Laplacian(Mat Input, int K) { Mat Output = Input.clone(); for (int x = 1; x &lt; Input.rows - 1; x++) { uchar* sum = Output.ptr&lt;uchar&gt;(x); uchar* current = Input.ptr&lt;uchar&gt;(x); uchar* next = Input.ptr&lt;uchar&gt;(x + 1); uchar* last = Input.ptr&lt;uchar&gt;(x - 1); for (int y = 1; y &lt; Input.cols - 1; y++) { /* [ 0 - K 0 - K (1 + 4 * K) - K 0 - K 0 ] */ sum[y] = saturate_cast&lt;uchar&gt;( -K * last[y] - K * current[y - 1] + (1 + 4 * K) * current[y] - K * current[y + 1] - K * next[y]); } } imshow(&quot;Sharpening_based_on_Laplacian&quot;, Output); } 结果： 4 边缘检测方法的性能对比（主观）","categories":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}],"tags":[{"name":"opencv","slug":"opencv","permalink":"/tags/opencv/"},{"name":"C++","slug":"C","permalink":"/tags/C/"}],"keywords":[{"name":"学习","slug":"学习","permalink":"/categories/学习/"}]},{"title":"如何选购一台笔记本 测试用文本","slug":"如何选购一台笔记本 测试用文本","date":"2019-12-11T15:12:15.000Z","updated":"2021-11-26T17:59:23.134Z","comments":true,"path":"2019/12/11/如何选购一台笔记本 测试用文本/","link":"","permalink":"/2019/12/11/如何选购一台笔记本 测试用文本/","excerpt":"","text":"如何选购一台笔记本 测试用文本1 确定自身需求不以自身需求出发的选购都是耍流氓！在选购笔记本之前，我们首先要确定我们这台笔记本是用来 干嘛的。 学习？ 剪辑？ 游戏？ 其次我们要通过购买目的确定 笔记本的类型。 轻薄商务？ 全能影音？ 性能游戏？ 最后我们就要通过 预算 以及 主观客观评价 来选购笔记本了。 2 影响笔记本价格的因素笔记本的价格与性能和外观这两个方面成正相关。即 性能越强劲，价格越高；外观越漂亮，价格越高 。性能主要有以下影响因素： CPU GPU 内存 硬盘 外观主要有以下影响因素： 厚度 重量 屏幕显示效果 模具制造工艺以及工业设计 除此之外还有一些因素会成为卖点影响价格： 生物识别 散热效果 扬声器素质 键盘及触控板素质 *其中，打✔的为主要影响因素 3 主要因素的具体说明3.1 CPUCPU是决定笔记本运行速度的主要因素之一。主要有英特尔和AMD两大厂商 基本只有英特尔一个选择。英特尔移动端主要有3种CPU： 酷睿™ M3系列 M3-8100Y 极致轻薄 酷睿™ i系列低电压 i5-8250U i7-8550U 轻薄与性能并存 酷睿™ i系列标准电压 i5-8300H i7-8750H 性能至上 M3系列 大多用于二合一平板或者厚度 1cm 以内的笔记本。i系列低压 大多数用于常见的商务本，厚度在 1.5cm 左右。i系列标压 大多数用于常见的高性能笔记本，厚度在 2cm 左右。 3.2 GPUGPU是决定笔记本影音处理、游戏性能的主要因素之一。主要有英伟达和AMD两大厂商 基本只有英伟达一个选择。英伟达移动端主要有2种GPU： 低性能独立显卡 MX150 MX250 高性能独立显卡 GTX1650/1060 RTX2060/2070 低性能独立显卡 能够胜任简单的 视频剪辑 ，流畅运行 普通网游 。高性能独立显卡 能够胜任复杂的 模型渲染 ，流畅运行 3A大作 。除此之外 ，还有一些文本工作者 不需要 独立显卡，那么英特尔CPU自带的 核心显卡 就可以胜任日常的影音娱乐。 3.3 内存内存是决定笔记本多任务运行速度的主要因素之一。主要有 内存品牌基本没得选择，决定权在笔记本厂商手里。但我们可以选择内存容量： 4G 没法用，win10不是win98 8G 一般程度的多任务处理 16G 基本可以为所欲为 现在各大笔记本厂商都在普及8G内存，4G内存只存在于少数轻薄本。 3.4 厚度与重量我们以1.5cm厚，1.5kg重的中等配置的一款笔记本为例。 相同配置 ，厚度每 增加 0.1cm，价格 降低 500-1000元左右。相同配置 ，厚度每 减少 0.1cm，价格 增加 500-1000元左右。*经验公式，仅供参考 3.5 补充说明具体的 性能比较 可以百度CPU天梯图/显卡天梯图进行查询。具体的 外观偏好 还是以实体店亲身体验感受为佳。 4 笔记本厂商选择笔记本厂商主要有三种。第一种是传统笔记本电脑厂商： 联想 戴尔 惠普 华硕 第二种是主打游戏本的厂商： 神舟 机械革命 微星 第三种是互联网厂商： 小米 华为 优缺点如下方表格所示： 传统笔记本厂商 游戏本厂商 互联网厂商 优势 服务过硬 性价比极高 互联网思想 缺点 价格偏高 售后少 用户群有待积累 传统笔记本推荐 联想 ，原因：近些年产品品质以及性价比的提升。游戏本推荐 神舟 ，原因：我买神舟的时候，还没有人开始称之为上船。互联网 小米华为皆可 。小米自始至终 追求性价比 ，华为自始至终 追求好产品 。","categories":[{"name":"生活","slug":"生活","permalink":"/categories/生活/"}],"tags":[{"name":"笔记本","slug":"笔记本","permalink":"/tags/笔记本/"},{"name":"性价比","slug":"性价比","permalink":"/tags/性价比/"}],"keywords":[{"name":"生活","slug":"生活","permalink":"/categories/生活/"}]}]}