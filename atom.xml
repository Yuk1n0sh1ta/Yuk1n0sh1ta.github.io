<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>彩音のBlog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="/"/>
  <updated>2023-01-16T17:21:17.770Z</updated>
  <id>/</id>
  
  <author>
    <name>彩音</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>transformer位置编码 Python实现</title>
    <link href="/2023/01/17/transformer%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%20Python%E5%AE%9E%E7%8E%B0/"/>
    <id>/2023/01/17/transformer位置编码 Python实现/</id>
    <published>2023-01-16T17:13:00.000Z</published>
    <updated>2023-01-16T17:21:17.770Z</updated>
    
    <content type="html"><![CDATA[<h1 id="transformer位置编码-Python实现"><a href="#transformer位置编码-Python实现" class="headerlink" title="transformer位置编码 Python实现"></a>transformer位置编码 Python实现</h1><pre><code class="Python"># coding: utf-8import torch.nn as nnimport torchimport mathdevice = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)class PositionalEncoding(nn.Module):    def __init__(self, d_model: int, dropout: float = 0.1, max_length: int = 20, mode: str = &#39;no_position&#39;):        &quot;&quot;&quot;        Args:            d_model: int, dimension of hidden size            dropout: float, dropout probability            max_length: int, max sequence length of input text            mode: str                &#39;add&#39;, add a trainable positional embedding to word embedding                &#39;add_fixed&#39;, add a fixed positional embedding to word embedding                &#39;add_sinusoid&#39;, add a sinusoid positional embedding to word embedding                &#39;multiply&#39;, do element-wise multiplication of word embedding and a trainable positional embedding                &#39;multiply_fixed&#39;, do element-wise multiplication of word embedding and a fixed positional embedding                &#39;multiply_sinusoid&#39;, do element-wise multiplication of word embedding and a sinusoid positional embedding                &#39;no_position&#39;, do not combine word embedding with positional information        &quot;&quot;&quot;        super(PositionalEncoding, self).__init__()        self.dropout_layer = nn.Dropout(p=dropout)        self.position_embedding = torch.rand(1, max_length, d_model).to(device)        self.mode = mode        fixed_position_embedding = torch.arange(1 / max_length, 1 + 1 / max_length, 1 / max_length, requires_grad=False)        self.fixed_position_embedding = fixed_position_embedding.unsqueeze(0).T.expand(max_length, d_model).to(device)        position = torch.arange(max_length).unsqueeze(1)        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(20.0) / d_model))        sinusoid_position_embedding = torch.zeros(max_length, 1, d_model)        sinusoid_position_embedding[:, 0, 0::2] = torch.sin(div_term * position)        sinusoid_position_embedding[:, 0, 1::2] = torch.cos(div_term * position)        sinusoid_position_embedding.requires_grad = False        sinusoid_position_embedding = sinusoid_position_embedding.permute([1, 0, 2])        sinusoid_position_embedding = sinusoid_position_embedding.to(device)        self.sinusoid_position_embedding = sinusoid_position_embedding    def forward(self, word_embedding: torch.Tensor):        &quot;&quot;&quot;        Args:            word_embedding: torch.Tensor, shape [batch_size, seq_length, embedding_dim]        Returns:            output: torch.Tensor, shape [batch_size, seq_length, embedding_dim]        &quot;&quot;&quot;        if self.mode == &#39;no_position&#39;:            return word_embedding        if self.mode == &#39;add&#39;:            input_embeddings = word_embedding + self.position_embedding        elif self.mode == &#39;multiply&#39;:            input_embeddings = word_embedding * self.position_embedding        elif self.mode == &#39;add_fixed&#39;:            input_embeddings = word_embedding + self.fixed_position_embedding        elif self.mode == &#39;multiply_fixed&#39;:            input_embeddings = word_embedding * self.fixed_position_embedding        elif self.mode == &#39;add_sinusoid&#39;:            input_embeddings = word_embedding + self.sinusoid_position_embedding        elif self.mode == &#39;multiply_sinusoid&#39;:            input_embeddings = word_embedding * self.sinusoid_position_embedding        return self.dropout_layer(input_embeddings)def mean_pooling(model_output, attention_mask):    token_embeddings = model_output    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float().to(device)    return torch.sum(token_embeddings * input_mask_expanded, 1).to(device) / torch.clamp(input_mask_expanded.sum(1),                                                                                         min=1e-9).to(device)# Transformer Modelclass TransformerModel(nn.Module):    def __init__(self, vocab_size: int, d_model: int, nhead: int, num_layers: int, dropout: float = 0.1,                 output_mode: str = &#39;mean&#39;) -&gt; None:        &quot;&quot;&quot;        Args:        &quot;&quot;&quot;        super(TransformerModel, self).__init__()        self.encoder = nn.Embedding(vocab_size, d_model)        # self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=embedding_freeze)        self.positional_encoder = PositionalEncoding(d_model)        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=d_model * 4, dropout=dropout,                                                   batch_first=True)        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)        self.output_mode = output_mode    def forward(self, input: torch.Tensor, attention_mask: torch.Tensor) -&gt; torch.Tensor:        word_embeddings = self.encoder(input)        input_embeddings = self.positional_encoder(word_embeddings)        output_embeddings = self.transformer_encoder(input_embeddings)        if self.output_mode == &#39;cls&#39;:            output_embeddings = output_embeddings[:, 0, :]        elif self.output_mode == &#39;mean&#39;:            output_embeddings = mean_pooling(output_embeddings, attention_mask)        # output_embeddings = torch.mean(output_embeddings, 1)        return output_embeddings</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;transformer位置编码-Python实现&quot;&gt;&lt;a href=&quot;#transformer位置编码-Python实现&quot; class=&quot;headerlink&quot; title=&quot;transformer位置编码 Python实现&quot;&gt;&lt;/a&gt;transformer位置编
      
    
    </summary>
    
      <category term="学习" scheme="/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="NLP" scheme="/tags/NLP/"/>
    
      <category term="Python" scheme="/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>文献阅读 Divide and Conquer Text Semantic Matching with Disentangled Keywords and Intents</title>
    <link href="/2023/01/15/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%20Divide%20and%20Conquer%20Text%20Semantic%20Matching%20with%20Disentangled%20Keywords%20and%20Intents/"/>
    <id>/2023/01/15/文献阅读 Divide and Conquer Text Semantic Matching with Disentangled Keywords and Intents/</id>
    <published>2023-01-15T12:17:00.000Z</published>
    <updated>2023-01-16T17:14:37.459Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文献阅读-Divide-and-Conquer-Text-Semantic-Matching-with-Disentangled-Keywords-and-Intents"><a href="#文献阅读-Divide-and-Conquer-Text-Semantic-Matching-with-Disentangled-Keywords-and-Intents" class="headerlink" title="文献阅读 Divide and Conquer Text Semantic Matching with Disentangled Keywords and Intents"></a>文献阅读 Divide and Conquer Text Semantic Matching with Disentangled Keywords and Intents</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p><strong>链接：</strong><a href="https://arxiv.org/pdf/2203.02898.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2203.02898.pdf</a></p><p><strong>题目：</strong>Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents</p><p><img src="https://gitlab.com/yuk1n0sh1ta/Source/-/raw/master/pictures/2023/01/17_0_59_50_20230117005949.png" alt="image.png"></p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><strong>训练数据示例：</strong></p><ol><li><strong>Sentence A：</strong>电磁阀怎么工作？</li></ol><p><strong>Key A:</strong> 电磁阀</p><p><strong>Intent A:</strong> [MASK]怎么工作？</p><ol start="2"><li><strong>Sentence B：</strong>电磁阀是什么？</li></ol><p><strong>Key B:</strong> 电磁阀</p><p><strong>Intent B:</strong> [MASK]是什么？</p><ol start="3"><li><strong>相似度标签 = [0, 1] 不匹配</strong></li></ol><h3 id="一个假设"><a href="#一个假设" class="headerlink" title="一个假设"></a><strong>一个假设</strong></h3><ol><li>每个用户query都拥有一个关键词及一个意图，关键词匹配与意图匹配是独立的。</li></ol><h3 id="三个目标"><a href="#三个目标" class="headerlink" title="三个目标"></a><strong>三个目标</strong></h3><p><strong>Loss = Loss_sm + Loss_ds + Loss_dc</strong></p><ol><li><strong>Loss_sm (semantic matching)</strong>：为了使 <strong>句子匹配的结果</strong> 逼近 <strong>匹配相似度标签</strong>，句子匹配的概率-&gt;相似度标签</li></ol><p>BERT句子匹配概率 = [0.2, 0.8]<br>相似度标签 = [0, 1]<br>Loss_sm = cross_entropy(句子匹配概率分布, 相似度标签)</p><ol start="2"><li><strong>Loss_ds (distant supervision)</strong>：为了在超平面使 <strong>关键词的Representation</strong> 和 <strong>意图的Representation 彼此远离</strong>，在 BERT 的 Representation 中提取 关键词Representation 之后的平均表示（mean pooling）经过一个分类器之后-&gt;1，提取 意图Representation 之后的平均表示经过同样分类器之后-&gt;0。</li></ol><p>Loss_ds = cross_entropy(关键词分类概率，[0, 1]) + cross_entropy(意图分类概率，[1, 0])</p><ol start="3"><li><strong>Loss_dc (divide and conquer)</strong>：为了让模型同时学习到关键词和意图是否匹配，关键词匹配概率与意图匹配的概率的联合概率分布-&gt;句子匹配的概率分布</li></ol><p>匹配 概率分布 = [匹配概率, 不匹配概率]</p><p><strong>key匹配概率分布</strong>（”电磁阀”, “电磁阀”） = <strong>[0.9, 0.1]</strong><br><strong>intent匹配概率分布</strong>（”[MASK]怎么工作？”, “[MASK]是什么？”） = <strong>[0.1, 0.9]</strong></p><p><strong>key-intent联合匹配概率</strong>（同时匹配） = key匹配intent匹配概率 = <strong>0.9 * 0.1</strong> = <strong>0.09</strong><br><strong>key-intent联合不匹配概率</strong> （至少有一个不匹配）<br>= key匹配intent不匹配概率 + key不匹配intent匹配概率 + key不匹配intent不匹配概率<br>= <strong>0.9 * 0.9 + 0.1 * 0.1 + 0.9 * 0.1</strong> = <strong>0.91</strong></p><p><strong>key-intent联合匹配概率分布</strong> = <strong>[0.09, 0.91]</strong><br><strong>句子匹配概率分布</strong>（”电磁阀怎么工作？”, “电磁阀是什么？”）= <strong>[0.2, 0.8]</strong></p><p>Loss_dc = KL(<strong>key-intent联合匹配概率分布</strong>, <strong>句子匹配概率分布</strong>)</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><strong>结果：</strong><br><img src="https://gitlab.com/yuk1n0sh1ta/Source/-/raw/master/pictures/2023/01/17_1_1_1_20230117010101.png" alt="image.png"></p><p><strong>优势：</strong></p><ol><li>将句子匹配的两个识别特征分开看（分治），降低了模型用一个目标函数同时识别两个特征点的难度。</li></ol><p><strong>限制：</strong></p><ol><li>这种方法受限于关键词提取的方式和精度。</li><li>原文用了Sogou knowledge graph（Knowledge graph construction and applications for web search and beyond）</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;文献阅读-Divide-and-Conquer-Text-Semantic-Matching-with-Disentangled-Keywords-and-Intents&quot;&gt;&lt;a href=&quot;#文献阅读-Divide-and-Conquer-Text-Semant
      
    
    </summary>
    
      <category term="学习" scheme="/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="NLP" scheme="/tags/NLP/"/>
    
      <category term="文献阅读" scheme="/tags/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>Onnx 模型转换</title>
    <link href="/2023/01/15/Onnx%20%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2/"/>
    <id>/2023/01/15/Onnx 模型转换/</id>
    <published>2023-01-15T12:05:00.000Z</published>
    <updated>2023-01-16T17:11:57.011Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Onnx-模型转换代码示例"><a href="#Onnx-模型转换代码示例" class="headerlink" title="Onnx 模型转换代码示例"></a>Onnx 模型转换代码示例</h1><h2 id="BERT类模型转换"><a href="#BERT类模型转换" class="headerlink" title="BERT类模型转换"></a>BERT类模型转换</h2><pre><code class="Python">from transformers import BertTokenizerFastimport torch.nn as nnimport onnxruntime as ortimport numpy as npdevice = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)class BertForClassification(nn.Module):    def __init__(self, bert, label_num):        super(BertForClassification, self).__init__()        self.bert = bert        self.decoder = nn.Linear(768, label_num)    def forward(self, input_ids, attention_mask=None, token_type_ids=None):        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids).pooler_output        logits = self.decoder(outputs)        return logitsdef pt2onnx(pt_dir, onnx_dir, pseudo_input):    model = torch.load(pt_dir, map_location=&#39;cpu&#39;)    torch.onnx.export(model,                       pseudo_input,                       onnx_dir,                       opset_version=12,                      do_constant_folding=True,                      input_names=[&#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;token_type_ids&#39;],                      output_names=[&#39;output&#39;],                      dynamic_axes={                          &#39;input_ids&#39;: {0: &#39;N&#39;, 1: &#39;L&#39;},                          &#39;attention_mask&#39;: {0: &#39;N&#39;, 1: &#39;L&#39;},                          &#39;token_type_ids&#39;: {0: &#39;N&#39;, 1: &#39;L&#39;},                          &#39;output&#39;: {0: &#39;N&#39;}                          }                    )    print(&#39;### pt2onnx finished ###\n&#39;)if __name__ == &quot;__main__&quot;:    # onnx模型转换    batch_size = 50    data = torch.zeros(batch_size, 96, device=device).long()    input_ids = attention_mask = token_type_ids = data    pseudo_input = (input_ids, attention_mask, token_type_ids)    pt2onnx(&#39;model.bin&#39;, &#39;model.onnx&#39;, pseudo_input)    # 检查onnx模型输出精度差距    input_text_list = [&#39;你好&#39;, &#39;hello&#39;, &#39;天气不错！&#39;, &#39;good day!&#39;, &#39;怎么老是你？&#39;, &#39;How old are you?&#39;]    tokenizer = BertTokenizerFast.from_pretrained(&#39;./bert-base&#39;)  # 加载分词器    sess = ort.InferenceSession(&#39;model.onnx&#39;, None)  # 加载onnx模型    model = torch.load(&#39;model.bin&#39;, map_location=device)  # 加载torch模型    input_text_encoding = tokenizer(candidate_query_list, padding=&#39;max_length&#39;, truncation=True, max_length=32, return_tensors=&#39;pt&#39;)    input_ids = encoding.data[&#39;input_ids&#39;].numpy()    attention_mask = encoding.data[&#39;attention_mask&#39;].numpy()    token_type_ids = encoding.data[&#39;token_type_ids&#39;].numpy()    # onnx、torch分别进行推理    onnx_output = sess.run([&#39;output&#39;],{&#39;input_ids&#39;: input_ids, &#39;attention_mask&#39;: attention_mask, &#39;token_type_ids&#39;: token_type_ids})    torch_output = model(encoding.data[&#39;input_ids&#39;], encoding.data[&#39;attention_mask&#39;], encoding.data[&#39;token_type_ids&#39;])</code></pre><h2 id="T5类模型转换"><a href="#T5类模型转换" class="headerlink" title="T5类模型转换"></a>T5类模型转换</h2><p>链接：<a href="https://github.com/Ki6an/fastT5/tree/8dda859086af631a10ad210a5f1afdec64d49616" target="_blank" rel="noopener">fastT5</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Onnx-模型转换代码示例&quot;&gt;&lt;a href=&quot;#Onnx-模型转换代码示例&quot; class=&quot;headerlink&quot; title=&quot;Onnx 模型转换代码示例&quot;&gt;&lt;/a&gt;Onnx 模型转换代码示例&lt;/h1&gt;&lt;h2 id=&quot;BERT类模型转换&quot;&gt;&lt;a href=&quot;#
      
    
    </summary>
    
      <category term="学习" scheme="/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="/tags/python/"/>
    
      <category term="onnx" scheme="/tags/onnx/"/>
    
  </entry>
  
  <entry>
    <title>Python的地址引用</title>
    <link href="/2023/01/15/Python%E7%9A%84%E5%9C%B0%E5%9D%80%E5%BC%95%E7%94%A8/"/>
    <id>/2023/01/15/Python的地址引用/</id>
    <published>2023-01-15T11:38:00.000Z</published>
    <updated>2023-01-16T17:11:43.347Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python-地址引用"><a href="#Python-地址引用" class="headerlink" title="Python 地址引用"></a>Python 地址引用</h1><h2 id="数值、列表地址"><a href="#数值、列表地址" class="headerlink" title="数值、列表地址"></a>数值、列表地址</h2><p><strong>被赋值对象不涉及索引的：</strong>对被赋值对象来说是直接引用</p><pre><code class="python"># 数值&gt;&gt;&gt; x = 1&gt;&gt;&gt; y = x&gt;&gt;&gt; x = 2&gt;&gt;&gt; y1# 字符串&gt;&gt;&gt; str_a = &#39;hello&#39;&gt;&gt;&gt; str_b = str_a&gt;&gt;&gt; str_a = str_a[:2]&gt;&gt;&gt; str_a&#39;he&#39;&gt;&gt;&gt; str_b&#39;hello&#39;# 列表&gt;&gt;&gt; list_a = [[1, 2], [3, 4]]&gt;&gt;&gt; list_b = list_a&gt;&gt;&gt; list_a = [[1, 2], [3, 5]]&gt;&gt;&gt; list_b[[1, 2], [3, 4]]</code></pre><p><strong>被赋值对象涉及索引的：</strong>对被赋值对象来说是间接引用</p><pre><code class="python">&gt;&gt;&gt; list_a = [[1, 2], [3, 4]]&gt;&gt;&gt; list_b = list_a&gt;&gt;&gt; list_a[0] = [1]&gt;&gt;&gt; list_b[[1], [3, 4]]</code></pre><h2 id="函数地址"><a href="#函数地址" class="headerlink" title="函数地址"></a>函数地址</h2><pre><code class="python">&gt;&gt;&gt; def func():...     return 0...&gt;&gt;&gt; func&lt;function func at 0x00000254E026B158&gt;&gt;&gt;&gt; func()0&gt;&gt;&gt; id(func)2563561140568&gt;&gt;&gt; id(func())1760324608&gt;&gt;&gt; id(0)1760324608&gt;&gt;&gt; func_x = func&gt;&gt;&gt; func_y = func&gt;&gt;&gt; id(func_x)2563561140568&gt;&gt;&gt; id(func_y)2563561140568&gt;&gt;&gt; def func_():...     return 0...&gt;&gt;&gt; id(func_)2563561119528&gt;&gt;&gt; id(func_())1760324608&gt;&gt;&gt; func = func_&gt;&gt;&gt; id(func)2563561119528&gt;&gt;&gt; id(func_x)2563561140568&gt;&gt;&gt; id(func_y)2563561140568</code></pre><ol><li><code>func</code> 会在程序中返回函数存储的内存地址，位置固定。</li><li><code>func_x = func</code> 会将 <code>func</code> 的内存地址直接赋给 <code>func_x</code>，此时 <code>func</code> 和 <code>func_x</code> 只有命名的不同，指向的位置完全一样。</li><li>对于逻辑完全相同但定义两次的两个函数，程序会为他们赋予不同的内存地址。</li><li>由于 <code>func_x = func</code> 过程是直接引用，运行 <code>func = func_</code> 后，<code>func_x</code> 指向不变。</li></ol><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ol><li>不要使用 <code>def func(a = [])</code> ，多次调用时会出现 a 不被初始化的现象，尚未找出原因。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Python-地址引用&quot;&gt;&lt;a href=&quot;#Python-地址引用&quot; class=&quot;headerlink&quot; title=&quot;Python 地址引用&quot;&gt;&lt;/a&gt;Python 地址引用&lt;/h1&gt;&lt;h2 id=&quot;数值、列表地址&quot;&gt;&lt;a href=&quot;#数值、列表地址&quot; c
      
    
    </summary>
    
      <category term="学习" scheme="/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>文本生成方法（待补充）</title>
    <link href="/2023/01/15/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E6%96%B9%E6%B3%95/"/>
    <id>/2023/01/15/文本生成方法/</id>
    <published>2023-01-15T11:38:00.000Z</published>
    <updated>2023-01-16T17:16:26.660Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Text-Generation"><a href="#Text-Generation" class="headerlink" title="Text Generation"></a>Text Generation</h1><ul><li>参考：</li></ul><ol><li><a href="https://arxiv.org/pdf/2201.05273.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2201.05273.pdf</a></li></ol><h2 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h2><p>对于给定的输入信息xxx，生成一系列离散的Token序列 。其中xxx是词典。</p><p>文本生成任务也可以被描述为：<br>xxx</p><h2 id="输入信息方式"><a href="#输入信息方式" class="headerlink" title="输入信息方式"></a>输入信息方式</h2><p>根据输入信息的种类，可以将文本生成的应用分为如下5类：</p><ol><li>没有提供或者是一个随机噪声向量</li><li>是一个离散特征的集合（如主题关键词、情感标签）</li><li>是结构化数据（如知识图谱、表格）</li><li>是多媒体输入（如图像、语音）</li><li>是文本序列（如应用于机器翻译、文本摘要、对话系统）</li></ol><h2 id="预训练模型结构"><a href="#预训练模型结构" class="headerlink" title="预训练模型结构"></a>预训练模型结构</h2><p>预训练模型结构主要可以分为 类：</p><ol><li>Encoder-decoder Transformer</li><li>Decoder-only Transformer</li><li>（待补充）</li></ol><h2 id="代表模型"><a href="#代表模型" class="headerlink" title="代表模型"></a>代表模型</h2><h3 id="GPT-Family"><a href="#GPT-Family" class="headerlink" title="GPT Family"></a>GPT Family</h3><p>论文链接：<br>主要评价方式：<br>模型Overview：<br>训练技巧：<br>结果：</p><h3 id="BART"><a href="#BART" class="headerlink" title="BART"></a>BART</h3><h3 id="T5"><a href="#T5" class="headerlink" title="T5"></a>T5</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Text-Generation&quot;&gt;&lt;a href=&quot;#Text-Generation&quot; class=&quot;headerlink&quot; title=&quot;Text Generation&quot;&gt;&lt;/a&gt;Text Generation&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;参考：&lt;/li&gt;
&lt;/u
      
    
    </summary>
    
      <category term="学习" scheme="/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="NLP" scheme="/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Pandas DataFrame 速查</title>
    <link href="/2023/01/15/Pandas%20DataFrame%20%E9%80%9F%E6%9F%A5/"/>
    <id>/2023/01/15/Pandas DataFrame 速查/</id>
    <published>2023-01-15T10:45:00.000Z</published>
    <updated>2023-01-16T17:18:05.647Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pandas-DataFrame-速查"><a href="#Pandas-DataFrame-速查" class="headerlink" title="Pandas DataFrame 速查"></a>Pandas DataFrame 速查</h1><h2 id="1-DataFrame-初始化"><a href="#1-DataFrame-初始化" class="headerlink" title="1 DataFrame 初始化"></a>1 DataFrame 初始化</h2><h3 id="1-1-从列表初始化"><a href="#1-1-从列表初始化" class="headerlink" title="1.1 从列表初始化"></a>1.1 从列表初始化</h3><pre><code class="python">data = [[&#39;apple&#39;, &#39;fruit&#39;, 5], [&#39;bike&#39;, &#39;vehicle&#39;, 10], [&#39;computer&#39;, &#39;device&#39;, 2]]dataset = pd.DataFrame(data, columns=[&#39;name&#39;, &#39;category&#39;, &#39;number&#39;], dtype=float)&gt;&gt;&gt; dataset       name category  number0     apple    fruit     5.01      bike  vehicle    10.02  computer   device     2.0</code></pre><h3 id="1-2-从字典初始化"><a href="#1-2-从字典初始化" class="headerlink" title="1.2 从字典初始化"></a>1.2 从字典初始化</h3><pre><code class="python">&gt;&gt;&gt; data = {    &#39;name&#39;: [&#39;apple&#39;, &#39;bike&#39;, &#39;computer&#39;],     &#39;category&#39;: [&#39;fruit&#39;, &#39;vehicle&#39;, &#39;device&#39;],     &#39;number&#39;: [5, 10, 2]}&gt;&gt;&gt; dataset = pd.DataFrame(data)&gt;&gt;&gt; dataset       name category  number0     apple    fruit       51      bike  vehicle      102  computer   device       2</code></pre><h3 id="1-3-从文件读取"><a href="#1-3-从文件读取" class="headerlink" title="1.3 从文件读取"></a>1.3 从文件读取</h3><p><code>pandas.read_csv(file_dir, sep: str, usecols: list, na_values: str)</code></p><p><code>pandas.read_excel(file_dir, sheet_name: int / str / list, usecols: list, na_values: str)</code></p><h2 id="2-DataFrame-操作"><a href="#2-DataFrame-操作" class="headerlink" title="2 DataFrame 操作"></a>2 DataFrame 操作</h2><h3 id="2-1-遍历操作"><a href="#2-1-遍历操作" class="headerlink" title="2.1 遍历操作"></a>2.1 遍历操作</h3><pre><code class="python">import pandas as pddata = {    &#39;name&#39;: [&#39;apple&#39;, &#39;bike&#39;, &#39;computer&#39;],     &#39;category&#39;: [&#39;fruit&#39;, &#39;vehicle&#39;, &#39;device&#39;],     &#39;number&#39;: [5, 10, 2]}dataset = pd.DataFrame(data)&gt;&gt;&gt; dataset       name category  number0     apple    fruit       51      bike  vehicle      102  computer   device       2for i in range(len(dataset)):    for j in range(len(dataset.columns)):        dataset[dataset.columns[j]][i] = i * len(dataset.columns) + jprint(dataset)&gt;&gt;&gt; dataset  name category  number0    0        1       21    3        4       52    6        7       8</code></pre><h3 id="2-2-获取指定数据"><a href="#2-2-获取指定数据" class="headerlink" title="2.2 获取指定数据"></a>2.2 获取指定数据</h3><h4 id="2-2-1-获取指定元素"><a href="#2-2-1-获取指定元素" class="headerlink" title="2.2.1 获取指定元素"></a>2.2.1 获取指定元素</h4><p>获取 “device” 。</p><pre><code class="python">&gt;&gt;&gt; dataset       name category  number0     apple    fruit       51      bike  vehicle      102  computer   device       2&gt;&gt;&gt; dataset[&#39;category&#39;][2]&#39;device&#39;# .loc[ , ]&gt;&gt;&gt; dataset.loc[2, &#39;category&#39;]&#39;device&#39;# .iloc[ , ]&gt;&gt;&gt; dataset.iloc[2, 1]&#39;device&#39;</code></pre><h4 id="2-2-2-获取指定行"><a href="#2-2-2-获取指定行" class="headerlink" title="2.2.2 获取指定行"></a>2.2.2 获取指定行</h4><p>获取第2行。</p><pre><code class="python"># .loc[ , ]&gt;&gt;&gt; line_2 = dataset.loc[2, :]&gt;&gt;&gt; line_2name        computercategory      devicenumber             2Name: 2, dtype: object&gt;&gt;&gt; for item in line_2:...     print(item)...computerdevice2# .iloc[ , ]&gt;&gt;&gt; line_2 = dataset.iloc[2, :]&gt;&gt;&gt; line_2name        computercategory      devicenumber             2Name: 2, dtype: object&gt;&gt;&gt; for item in line_2:...     print(item)...computerdevice2</code></pre><h4 id="2-2-3-获取指定列"><a href="#2-2-3-获取指定列" class="headerlink" title="2.2.3 获取指定列"></a>2.2.3 获取指定列</h4><p>获取 “category” 列。</p><pre><code class="python"># .loc[ , ]&gt;&gt;&gt; row_category = dataset.loc[:, &#39;category&#39;]&gt;&gt;&gt; row_category0      fruit1    vehicle2     deviceName: category, dtype: object&gt;&gt;&gt; for item in row_category:...     print(item)...fruitvehicledevice# .iloc[ , ]&gt;&gt;&gt; row_category = dataset.iloc[:, 1]&gt;&gt;&gt; row_category0      fruit1    vehicle2     deviceName: category, dtype: object&gt;&gt;&gt; for item in row_category:...     print(item)...fruitvehicledevice</code></pre><h3 id="2-3-空值判断"><a href="#2-3-空值判断" class="headerlink" title="2.3 空值判断"></a>2.3 空值判断</h3><pre><code class="python">df = pd.DataFrame(dict(age=[5, 6, np.NaN],...               born=[pd.NaT, pd.Timestamp(&#39;1939-05-27&#39;),...               pd.Timestamp(&#39;1940-04-25&#39;)],...               name=[&#39;Alfred&#39;, &#39;Batman&#39;, &#39;&#39;],...               toy=[None, &#39;Batmobile&#39;, &#39;Joker&#39;]))&gt;&gt;&gt; df   age       born    name        toy0  5.0        NaT  Alfred       None1  6.0 1939-05-27  Batman  Batmobile2  NaN 1940-04-25              Joker&gt;&gt;&gt; df.isna()     age   born   name    toy0  False   True  False   True1  False  False  False  False2   True  False  False  False&gt;&gt;&gt; pd.isna(df)     age   born   name    toy0  False   True  False   True1  False  False  False  False2   True  False  False  False</code></pre><h3 id="2-4-转array处理"><a href="#2-4-转array处理" class="headerlink" title="2.4 转array处理"></a>2.4 转array处理</h3><pre><code class="python">&gt;&gt;&gt; df = pd.DataFrame({&#39;age&#39;:     [ 3,  29],...                    &#39;height&#39;:  [94, 170],...                    &#39;weight&#39;:  [31, 115]})&gt;&gt;&gt; df   age  height  weight0    3      94      311   29     170     115&gt;&gt;&gt; df.valuesarray([[  3,  94,  31],       [ 29, 170, 115]], dtype=int64)</code></pre><h3 id="2-5-去重"><a href="#2-5-去重" class="headerlink" title="2.5 去重"></a>2.5 去重</h3><pre><code class="python">&gt;&gt;&gt; df = pd.DataFrame({...     &#39;brand&#39;: [&#39;Yum Yum&#39;, &#39;Yum Yum&#39;, &#39;Indomie&#39;, &#39;Indomie&#39;, &#39;Indomie&#39;],...     &#39;style&#39;: [&#39;cup&#39;, &#39;cup&#39;, &#39;cup&#39;, &#39;pack&#39;, &#39;pack&#39;],...     &#39;rating&#39;: [4, 4, 3.5, 15, 5]... })&gt;&gt;&gt; df    brand style  rating0  Yum Yum   cup     4.01  Yum Yum   cup     4.02  Indomie   cup     3.53  Indomie  pack    15.04  Indomie  pack     5.0&gt;&gt;&gt; df.drop_duplicates()    brand style  rating0  Yum Yum   cup     4.02  Indomie   cup     3.53  Indomie  pack    15.04  Indomie  pack     5.0&gt;&gt;&gt; df.drop_duplicates(subset=[&#39;brand&#39;, &#39;style&#39;], keep=&#39;last&#39;)    brand style  rating1  Yum Yum   cup     4.02  Indomie   cup     3.54  Indomie  pack     5.0</code></pre><h2 id="3-DataFrame-导出"><a href="#3-DataFrame-导出" class="headerlink" title="3 DataFrame 导出"></a>3 DataFrame 导出</h2><h3 id="3-1-导出为Excel"><a href="#3-1-导出为Excel" class="headerlink" title="3.1 导出为Excel"></a>3.1 导出为Excel</h3><h3 id="3-2-导出为tsv"><a href="#3-2-导出为tsv" class="headerlink" title="3.2 导出为tsv"></a>3.2 导出为tsv</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Pandas-DataFrame-速查&quot;&gt;&lt;a href=&quot;#Pandas-DataFrame-速查&quot; class=&quot;headerlink&quot; title=&quot;Pandas DataFrame 速查&quot;&gt;&lt;/a&gt;Pandas DataFrame 速查&lt;/h1&gt;&lt;h2 i
      
    
    </summary>
    
      <category term="学习" scheme="/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="/tags/python/"/>
    
      <category term="pandas" scheme="/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>Python的多进程</title>
    <link href="/2023/01/15/Python%E7%9A%84%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
    <id>/2023/01/15/Python的多进程/</id>
    <published>2023-01-15T10:45:00.000Z</published>
    <updated>2023-01-16T17:11:38.549Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python-多进程"><a href="#Python-多进程" class="headerlink" title="Python 多进程"></a>Python 多进程</h1><p><strong>参考：</strong></p><ol><li><a href="https://www.shouxicto.com/article/1313.html" target="_blank" rel="noopener">https://www.shouxicto.com/article/1313.html</a></li><li><a href="https://www.maxlist.xyz/2020/03/20/multi-processing-pool/" target="_blank" rel="noopener">https://www.maxlist.xyz/2020/03/20/multi-processing-pool/</a></li><li><a href="https://zhuanlan.zhihu.com/p/104919288" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/104919288</a></li></ol><h1 id="多进程实验"><a href="#多进程实验" class="headerlink" title="多进程实验"></a>多进程实验</h1><h2 id="测试脚本"><a href="#测试脚本" class="headerlink" title="测试脚本"></a>测试脚本</h2><pre><code class="python">from multiprocessing import Poolimport timedef func():    sum(range(0, 50000000))if __name__ == &#39;__main__&#39;:    file_dir = &#39;test.txt&#39;    p = Pool(8)    start = time.time()    with open(file_dir, &#39;r&#39;, encoding=&#39;utf-8&#39;) as input_file:        for line in input_file:            p.apply_async(func, ())    p.close()    p.join()    print(int(time.time()-start), &#39;s&#39;)</code></pre><p><strong>说明：</strong></p><ol><li><code>apply(func[, args=()])</code>会阻塞主进程，无法实现主进程循环子进程。</li><li><code>apply_async(func[, args=()])</code>不会阻塞主进程，可以实现主进程循环子进程。</li><li><code>p.close()</code>会阻止进程池继续添加新的任务。</li><li><code>p.join()</code>会阻塞主进程等待进程池中的任务全部进行完，该方法必须在<code>p.close()</code>之后运行。</li><li>注意<code>(func[, args=()])</code>中的<code>func</code>传入的是函数名，参数在后面传入。</li></ol><h2 id="本地实验"><a href="#本地实验" class="headerlink" title="本地实验"></a>本地实验</h2><p><strong>实验环境：</strong>i7-1165G7(4 cores, 8 threads)</p><table><thead><tr><th>进程池大小</th><th>运行时间</th></tr></thead><tbody><tr><td>1</td><td>79s</td></tr><tr><td>2</td><td>77s</td></tr><tr><td>4</td><td>37s</td></tr><tr><td>8</td><td>28s</td></tr><tr><td>16</td><td>28s</td></tr></tbody></table><p><strong>进程池大小 = 1</strong><br><img src="https://gitlab.com/yuk1n0sh1ta/Source/-/raw/master/pictures/2023/01/17_1_2_35_20230117010235.png" alt="image.png"></p><p><strong>进程池大小 = 2</strong><br><img src="https://gitlab.com/yuk1n0sh1ta/Source/-/raw/master/pictures/2023/01/17_1_2_41_20230117010241.png" alt="image.png"></p><p><strong>进程池大小 = 4</strong><br><img src="https://gitlab.com/yuk1n0sh1ta/Source/-/raw/master/pictures/2023/01/17_1_6_42_20230117010642.png" alt="image.png"></p><p><strong>进程池大小 = 8</strong><br><img src="https://gitlab.com/yuk1n0sh1ta/Source/-/raw/master/pictures/2023/01/17_1_2_55_20230117010255.png" alt="image.png"></p><p><strong>进程池大小 = 16</strong><br><img src="https://gitlab.com/yuk1n0sh1ta/Source/-/raw/master/pictures/2023/01/17_1_7_25_20230117010725.png" alt="image.png"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ol><li>当线程池数量与CPU逻辑处理器数量 X（4核X线程）相同时，CPU可以满载运行。</li><li>当线程池数量 = kX 时，程序运行速度没有可观提升，将线程池数量设置为 X 即可。</li></ol><pre><code class="python">&gt;&gt;&gt; from multiprocessing import cpu_count&gt;&gt;&gt; cpu_count()8</code></pre><h1 id="多进程加锁"><a href="#多进程加锁" class="headerlink" title="多进程加锁"></a>多进程加锁</h1><p>锁对象：<a href="https://docs.python.org/zh-cn/3/library/threading.html#lock-objects" target="_blank" rel="noopener">https://docs.python.org/zh-cn/3/library/threading.html#lock-objects</a></p><ul><li><p>原始锁处于 “锁定” 或者 “非锁定” 两种状态之一。它被创建时为非锁定状态。它有两个基本方法， <a href="https://docs.python.org/zh-cn/3/library/threading.html#threading.Lock.acquire" title="threading.Lock.acquire" target="_blank" rel="noopener"><code>acquire()</code></a> 和 <a href="https://docs.python.org/zh-cn/3/library/threading.html#threading.Lock.release" title="threading.Lock.release" target="_blank" rel="noopener"><code>release()</code></a> 。当状态为非锁定时， <a href="https://docs.python.org/zh-cn/3/library/threading.html#threading.Lock.acquire" title="threading.Lock.acquire" target="_blank" rel="noopener"><code>acquire()</code></a> 将状态改为 锁定 并立即返回。当状态是锁定时， <a href="https://docs.python.org/zh-cn/3/library/threading.html#threading.Lock.acquire" title="threading.Lock.acquire" target="_blank" rel="noopener"><code>acquire()</code></a> 将阻塞至其他线程调用 <a href="https://docs.python.org/zh-cn/3/library/threading.html#threading.Lock.release" title="threading.Lock.release" target="_blank" rel="noopener"><code>release()</code></a> 将其改为非锁定状态，然后 <a href="https://docs.python.org/zh-cn/3/library/threading.html#threading.Lock.acquire" title="threading.Lock.acquire" target="_blank" rel="noopener"><code>acquire()</code></a> 调用重置其为锁定状态并返回。 <a href="https://docs.python.org/zh-cn/3/library/threading.html#threading.Lock.release" title="threading.Lock.release" target="_blank" rel="noopener"><code>release()</code></a> 只在锁定状态下调用； 它将状态改为非锁定并立即返回。如果尝试释放一个非锁定的锁，则会引发 <a href="https://docs.python.org/zh-cn/3/library/exceptions.html#RuntimeError" title="RuntimeError" target="_blank" rel="noopener"><code>RuntimeError</code></a>  异常。</p></li><li><p>锁同样支持 <a href="https://docs.python.org/zh-cn/3/library/threading.html#with-locks" target="_blank" rel="noopener">上下文管理协议</a>。</p></li><li><p>当多个线程在 <a href="https://docs.python.org/zh-cn/3/library/threading.html#threading.Lock.acquire" title="threading.Lock.acquire" target="_blank" rel="noopener"><code>acquire()</code></a> 等待状态转变为未锁定被阻塞，然后 <a href="https://docs.python.org/zh-cn/3/library/threading.html#threading.Lock.release" title="threading.Lock.release" target="_blank" rel="noopener"><code>release()</code></a> 重置状态为未锁定时，只有一个线程能继续执行；至于哪个等待线程继续执行没有定义，并且会根据实现而不同。</p></li><li><p>所有方法的执行都是原子性的。</p></li></ul><pre><code class="python">from multiprocessing import Pool, Lockimport timedef func(lock, count):    sum(range(0, 50000000))    lock.acquire()    with open():        file.write()    lock.release()    count.value = count.value + 1if __name__ == &#39;__main__&#39;:    file_dir = &#39;test.txt&#39;    p = Pool(8)    type_str = &#39;int&#39;    count = multiprocessing.Value(type_str, 0)    lock = Lock()    start = time.time()    with open(file_dir, &#39;r&#39;, encoding=&#39;utf-8&#39;) as input_file:        for line in input_file:            p.apply_async(func, args=(lock, count))    p.close()    p.join()    print(int(time.time()-start), &#39;s&#39;)</code></pre><p>不加锁会输出乱码，原因是并行执行时出现多个核心同时操作同一文件的情况发生。</p><p>加锁会和单进程一样慢，原因是加锁后只有一个进程可以执行（官方文档）。</p><p>想要不乱码且高速需要使用Queue。</p><h1 id="独立文件IO"><a href="#独立文件IO" class="headerlink" title="独立文件IO"></a>独立文件IO</h1><p>运行开始分割数据集，将不同数据集交给不同进程。</p><p>文件A in - 进程A - 文件A out</p><p>文件B in - 进程B - 文件B out</p><p>…</p><p>文件C in - 进程C - 文件A out</p><p>运行结束后合并文件</p><h2 id="本地实验-1"><a href="#本地实验-1" class="headerlink" title="本地实验"></a>本地实验</h2><p><strong>实验环境：</strong>i7-1165G7(4 cores, 8 threads)</p><table><thead><tr><th></th><th>运行时间</th><th>CPU占用率</th><th>输出文件大小 (反映运行速度)</th></tr></thead><tbody><tr><td>单进程</td><td>5min</td><td>30%-40%</td><td>136MB (* 1.00)</td></tr><tr><td>8进程</td><td>5min</td><td>80%</td><td>302MB (* 2.22)</td></tr></tbody></table><h2 id="服务器在线实验"><a href="#服务器在线实验" class="headerlink" title="服务器在线实验"></a>服务器在线实验</h2><p><strong>实验环境：</strong>48 cores, 96 threads</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Python-多进程&quot;&gt;&lt;a href=&quot;#Python-多进程&quot; class=&quot;headerlink&quot; title=&quot;Python 多进程&quot;&gt;&lt;/a&gt;Python 多进程&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;参考：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a h
      
    
    </summary>
    
      <category term="学习" scheme="/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>数据扩增方法</title>
    <link href="/2022/02/27/%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E%E6%96%B9%E6%B3%95/"/>
    <id>/2022/02/27/数据扩增方法/</id>
    <published>2022-02-27T08:05:00.000Z</published>
    <updated>2023-01-14T16:48:03.769Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h1><p>主要参考：</p><ol><li><a href="https://arxiv.org/pdf/2105.03075.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2105.03075.pdf</a></li><li><a href="https://amitness.com/2020/05/data-augmentation-for-nlp/" target="_blank" rel="noopener">A Visual Survey of Data Augmentation in NLP</a></li></ol><h2 id="Rule-Based-Techniques"><a href="#Rule-Based-Techniques" class="headerlink" title="Rule-Based Techniques"></a>Rule-Based Techniques</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p><strong>词汇替换：</strong></p><ol><li>基于同义词词典（WordNet）替换近义词</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bm49UJ.png" alt=" 164594982572829"></center><ol start="2"><li>基于 TF-IDF 替换低信息词汇</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bm4pE4.png" alt="bm4pE4.png"></center><ol start="3"><li>基于 Unigram 出现频率替换单词</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bmhzbF.png" alt="bmhzbF.png"></center><ol start="4"><li>缩写扩展收缩</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bmhvuT.png" alt="bmhvuT.png"></center><center><img src="https://s4.ax1x.com/2022/02/27/bmhxDU.png" alt="bmhxDU.png"></center><p><strong>随机引入噪声：</strong></p><ol><li>主动拼写错误</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bmhXvV.png" alt="bmhXvV.png"></center><ol start="2"><li>被动拼写错误</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bmhL3q.png" alt="bmhL3q.png"></center><ol start="3"><li>随机单词删除（符号替换 / 完全删除）</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bmh7Nj.png" alt="bmh7Nj.png"></center><center><img src="https://s4.ax1x.com/2022/02/27/bmhqCn.png" alt="bmhqCn.png"></center><ol start="4"><li>随机位置变换（句子 / 词汇）</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bmhTEQ.png" alt="bmhTEQ.png"></center><center><img src="https://s4.ax1x.com/2022/02/27/bmhH4s.png" alt="bmhH4s.png"></center><ol start="5"><li>随机单词的近义词插入</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bnw4ld.png" alt="bnw4ld.png"></center><p><strong>语法解析：</strong></p><p><a href="https://arxiv.org/abs/1812.04718" target="_blank" rel="noopener">https://arxiv.org/abs/1812.04718</a></p><center><img src="https://s4.ax1x.com/2022/02/27/bnwWfe.png" alt="bnwWfe.png"></center><h3 id="EDA-EMNLP2019-NLP"><a href="#EDA-EMNLP2019-NLP" class="headerlink" title="EDA (EMNLP2019, NLP)"></a>EDA (EMNLP2019, NLP)</h3><p><strong>论文链接：</strong><a href="https://aclanthology.org/D19-1670.pdf" target="_blank" rel="noopener">https://aclanthology.org/D19-1670.pdf</a></p><p><strong>评价方式 / 数据集</strong>：</p><ol><li>SST-2: Stanford Sentiment Treebank (Socher et al., 2013)</li><li>CR: customer reviews (Hu and Liu, 2004; Liu et al., 2015)</li><li>SUBJ: subjectivity/objectivity dataset (Pang and Lee, 2004)</li><li>TREC: question type dataset (Li and Roth, 2002)</li><li>PC: Pro-Con dataset (Ganapathibhotla and Liu, 2008)</li></ol><p><strong>扩增方法：</strong></p><ol><li>Synonym Replacement<br>Randomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random. </li><li>Random Insertion<br>Find a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random position in the sentence. Do this n times. </li><li>Random Swap<br>Randomly choose two words in the sentence and swap their positions. Do this n times. </li><li>Random Deletion<br>Randomly remove each word in the sentence with probability p. </li></ol><p>其中，n与文本长度正相关$n=\alpha \cdot l$，p为手动设置的概率。</p><p><strong>结果：</strong></p><ol><li>EDA能够帮助多个模型在文本分类数据集上得到可观的性能提升。</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bnwcTK.png" alt="bnwcTK.png"></center><ol start="2"><li>相比训练数据充足的情况，EDA能够在训练样本较少的时候给模型带来更多的性能提升。</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bm4C59.png" alt="bm4C59.png"></center><h3 id="UDA-NeurIPS2020-CV-amp-NLP"><a href="#UDA-NeurIPS2020-CV-amp-NLP" class="headerlink" title="UDA(NeurIPS2020, CV &amp; NLP)"></a>UDA(NeurIPS2020, CV &amp; NLP)</h3><p><strong>论文链接：</strong><a href="https://proceedings.neurips.cc/paper/2020/file/44feb0096faa8326192570788b38c1d1-Paper.pdf" target="_blank" rel="noopener">https://proceedings.neurips.cc/paper/2020/file/44feb0096faa8326192570788b38c1d1-Paper.pdf</a></p><p><strong>评价方式 / 数据集：</strong></p><ol><li>Image Classification：CIFAR-10、SVHN</li><li>Text Classification：IMDb、Yelp-2、Yelp-5、Amazon-2、Amazon-5、DBpedia</li></ol><p><strong>模型Overview：</strong></p><center><img src="https://s4.ax1x.com/2022/02/27/bm4iCR.png" alt="bm4iCR.png"></center><ol><li>有监督侧：<br>计算模型预测结果和实际结果的cross-entropy loss。</li><li>无监督侧：<br>计算原始数据输入时模型预测结果和扩增数据输入时模型预测结果的consistency loss。</li></ol><p><strong>扩增方法：</strong></p><ol><li>RandAugment for Image Classification (inspired by AutoAugment)<br>对图像做随机变换，如亮度、大小、角度调节。</li><li>Back-translation for Text Classification<br>将语言A文本$Text_A$翻译为语言B得到$Text_B$，再将语言B文本$Text_B$翻译回语言A得到$Text’_A$。</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bm4VKK.png" alt=" 164594985562235"></center><ol start="3"><li>Word replacing with TF-IDF for Text Classification<br>通过TF-IDF过滤低分的低信息单词，留下高分单词。</li></ol><p><strong>训练技巧：</strong></p><ol><li>Confidence-based masking<br>将预测结果中最高预测概率小于某个阈值的样本标注出来。</li><li>Sharpening Predictions<br>将预测结果锐化得到近似的 one-hot 结果。</li><li>Domain-relevance Data Filtering<br>收集到用于数据扩增的 out-of-domain unlabeled 数据后，用 in-domain 的 baseline 模型预测 out-of-domain unlabeled 数据是否 in-domain，并挑选 confidence 高的数据作为扩增候选。</li></ol><p><strong>结果：</strong></p><ol><li>CV：UDA能够帮助多个模型大幅度降低在图像分类上的错误率。</li></ol><center>[<img src="https://s4.ax1x.com/2022/02/27/bm4Av6.png" alt="bm4Av6.png">]</center><ol start="2"><li>NLP：半监督情况下，UDA能帮助BERT大幅度降低在文本分类上的错误率。</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bm4kgx.png" alt="bm4kgx.png"></center><h2 id="Example-Interpolation-Techniques"><a href="#Example-Interpolation-Techniques" class="headerlink" title="Example Interpolation Techniques"></a>Example Interpolation Techniques</h2><h3 id="Overview-1"><a href="#Overview-1" class="headerlink" title="Overview"></a>Overview</h3><ol><li>Image Mixup</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bm4F81.png" alt="bm4F81.png"></center><ol start="2"><li>Word Mixup</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bm4ebD.png" alt="bm4ebD.png"></center><ol start="3"><li>Sentence Mixup</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bm4nVe.png" alt="bm4nVe.png"></center><h3 id="MIXUP-ICLR2018-CV"><a href="#MIXUP-ICLR2018-CV" class="headerlink" title="MIXUP(ICLR2018, CV)"></a>MIXUP(ICLR2018, CV)</h3><p><strong>论文链接：</strong><a href="https://openreview.net/pdf?id=r1Ddp1-Rb" target="_blank" rel="noopener">https://openreview.net/pdf?id=r1Ddp1-Rb</a></p><p><strong>评价方式 / 数据集：</strong></p><ol><li>Image Classification：CIFAR-10、CIFAR-100</li></ol><p><strong>扩增方法：</strong></p><ol><li>将随机抽取的两个图像和对应图像类别分别加权求和得到新的一个样本。<br><center>$\widetilde x = \lambda x_i + (1-\lambda)x_j $<br>$\widetilde y = \lambda y_i + (1-\lambda)y_j$</center><br>其中，$\lambda \sim Beta(\alpha, \alpha) \in [0, 1] $。</li></ol><p><strong>结果：</strong></p><ol><li>MIXUP能够帮助多个模型有效降低在图像分类上的错误率。</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bm4uUH.png" alt="bm4uUH.png"></center><h3 id="MixText-ACL2020-NLP"><a href="#MixText-ACL2020-NLP" class="headerlink" title="MixText(ACL2020, NLP)"></a>MixText(ACL2020, NLP)</h3><p><strong>论文链接：</strong><a href="https://aclanthology.org/2020.acl-main.194.pdf" target="_blank" rel="noopener">https://aclanthology.org/2020.acl-main.194.pdf</a></p><p><strong>评价方式 / 数据集：</strong></p><ol><li>Text Classification：AG News(Zhang et al., 2015), BPpedia (Mendes et al., 2012), Yahoo! Answers (Chang et al., 2008) and IMDB (Maas et al., 2011). </li></ol><p><strong>模型Overview：</strong></p><center><img src="https://s4.ax1x.com/2022/02/27/bm4l8I.png" alt="bm4l8I.png"></center><ol><li>Data Augmentation<br>通过回译得到K个扩增样本作为无标签数据。</li><li>Label Guessing<br>将原始样本和扩增样本（K+1个）加权后得到猜测的 consistent 的分类概率向量作为标签。为避免结果过于平滑，使用一个锐化函数突出最高分类概率的值。</li><li>TMix on Labeled and Unlabeled Data<br>在训练阶段，在 $X = [X_l, X_u, X_a]$ 集合中随机挑选两个样本 $x, x’ \in X$。<br>使用 TMix 得到其混合预测输出：<br><center>$p(TMix(x, x’)) $</center><br>用 mix 函数得到其混合真值输出：<br><center>$\widetilde y = mix(y, y’) =λy + (1 − λ)y’$</center><br>计算两者的 KL-divergence 作为 Loss：<br><center>$L_{TMix} = KL(mix(y, y’)||p(TMix(x, x’); φ)$</center><br>当 $x \in X_l$ 时，我们称之为 Supervised loss；当 $x \in [X_u, X_a]$ 时，我们称之为 Consistency loss。</li></ol><p><strong>扩增方法：</strong></p><ol><li>通过 mix 两段文本中间状态输出的 Embedding 实现文本的混合，将两段文本真实标签的加权混合作为混合真值。</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bm4QPA.png" alt="bm4QPA.png"></center><p><strong>结果：</strong></p><center>[<img src="https://s4.ax1x.com/2022/02/27/bm43xP.png" alt="bm43xP.png">]</center><ol><li>MixText 在4个文本分类数据集上 10-shot 的效果远超BERT。</li><li>MixText 在4个文本分类数据集上明显优于其他数据扩增方法。</li></ol><h2 id="Model-Based-Techniques"><a href="#Model-Based-Techniques" class="headerlink" title="Model-Based Techniques"></a>Model-Based Techniques</h2><h3 id="Overview-2"><a href="#Overview-2" class="headerlink" title="Overview"></a>Overview</h3><p><strong>词汇替换：</strong></p><ol><li>基于 Word Embedding 替换近义词（word2vec）</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bm4K5d.png" alt="bm4K5d.png"></center><ol start="2"><li>基于上下文预测替换近义词（Language Model）</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bm4K5d.png" alt="bm4K5d.png"></center><p><strong>语义替换：</strong></p><ol><li>根据关键词替换并生成新的与关键词语义相近的句子成分（生成式模型）</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bm4Jr8.png" alt="bm4Jr8.png"></center><p><strong>句子生成：</strong></p><ol><li>回译（机器翻译模型）</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bm4GKf.png" alt="bm4GKf.png"></center><ol start="2"><li>训练生成式模型并根据标签生成句子（生成式模型）</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bm4YqS.png" alt="bm4YqS.png"></center><center><img src="https://s4.ax1x.com/2022/02/27/bm4NVg.png" alt="bm4NVg.png"></center><center><img src="https://s4.ax1x.com/2022/02/27/bm4UaQ.png" alt="bm4UaQ.png"></center><h3 id="Back-Translation-ACL2016-NLP"><a href="#Back-Translation-ACL2016-NLP" class="headerlink" title="Back Translation(ACL2016, NLP)"></a>Back Translation(ACL2016, NLP)</h3><p><strong>论文链接：</strong><a href="https://aclanthology.org/P16-1009.pdf" target="_blank" rel="noopener">https://aclanthology.org/P16-1009.pdf</a></p><p><strong>评价方式 / 数据集：</strong></p><ol><li>Machine Translation：WMT 15、IWSLT 15。</li></ol><p><strong>扩增方法：</strong></p><ol><li>Dummy Source Sentences (Language Modeling)</li><li>Synthetic Source Sentences (Back Translation)</li></ol><p><strong>结果：</strong></p><ol><li>两种扩增方法都能够帮助模型在机器翻译数据集上获得可观的性能提升。</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bnw56A.png" alt="bnw56A.png"></center><h3 id="Contextual-Augmentation-NAACL2018-NLP"><a href="#Contextual-Augmentation-NAACL2018-NLP" class="headerlink" title="Contextual Augmentation(NAACL2018, NLP)"></a>Contextual Augmentation(NAACL2018, NLP)</h3><p><strong>论文链接：</strong><a href="https://aclanthology.org/N18-2072.pdf" target="_blank" rel="noopener">https://aclanthology.org/N18-2072.pdf</a></p><p>评价方式 / 数据集：</p><ol><li>Text Classification：STT5、STT2、Subj、MPQA、RT、TREC。</li></ol><p><strong>扩增方法：</strong></p><center><img src="https://s4.ax1x.com/2022/02/27/bnwIOI.png" alt="bnwIOI.png"></center><ol><li>Word Prediction based on Context<br>通过搭建一个 Bi-LSTM 的 Language Model ，基于上下文预测 Mask 的单词。将预测出的单词</li><li>Conditional Constraint<br>为了避免模型预测出的单词改变原句含义导致与标签不符，在 Language Model 训练过程中为目标函数加入标签信息，即最大化：<center>$p(word_{center}|word_{context}, label)$</center></li></ol><p><strong>结果：</strong></p><center><img src="https://s4.ax1x.com/2022/02/27/bnwTmt.png" alt="bnwTmt.png"></center><ol><li>同义词替换并不能给模型带来可观提升。</li><li>基于上下文预测单词进行数据扩增能够为模型带来一定的提升，但并不可观，作者没有给出在训练数据不足的情况下模型的提升幅度。</li><li>基于上下文和标签预测单词进行数据扩增能够进一步提升模型效果，但结果也并非十分惊艳，仍不清楚该方法在训练数据不足情况下模型的提升幅度。</li></ol><h3 id="SMERTI-EMNLP2019-NLP"><a href="#SMERTI-EMNLP2019-NLP" class="headerlink" title="SMERTI(EMNLP2019, NLP)"></a>SMERTI(EMNLP2019, NLP)</h3><p><strong>论文链接：</strong><a href="https://aclanthology.org/D19-1272.pdf" target="_blank" rel="noopener">https://aclanthology.org/D19-1272.pdf</a></p><p><strong>评价方式 / 数据集：</strong></p><ol><li>Fluency：SLOR</li><li>Sentiment Preservation Accuracy：SPA</li><li>Content Similarity Score：CSS</li><li>Semantic Text Exchange Score：STES</li></ol><p><strong>扩增方法：</strong></p><ol><li>Entity Replacement Module (ERM)<br>使用 Stanford Parser 解析给定的 Replacement Entity 的词性及语法结构，并遍历输入文本$S$找到和 RE 具有相同或相似词性及语法结构的单词作为候选 Original Entity。使用 Universal Sentence Encoder 生成 Word Embedding 并计算 RE 及 每个候选 OE 的相似度，将相似度最高的作为选定的 OE。若 Stanford Parser 没有找到候选 OE，则使用 Universal Sentence Encoder 遍历计算输入文本中每个词和 RE 的相似度并选取相似度最高的作为 OE。最后，将 OE 替换为 RE 得到文本$S’$。</li><li>Similarity Masking Module (SMM)<br>遍历计算文本$S’$中所有单词与 OE 的相似度，并用 Masking Rate Threshold （MRT）将相似度较高的单词用 [mask] 替换（忽略包含 OE 的），得到$S’’$。其中，连续的 [mask] 将会合并为一个。</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bnwRYD.png" alt="bnwRYD.png"></center><ol start="3"><li>Text Infilling Module (TIM)<br>使用seq2seq模型预测 [mask] 的文本，文中使用了 Bi-GRU + Attention 以及 Transformer 模型生成文本。将每处 [mask] 的生成文本替换 [mask] ，得到$\hat S$作为新生成的文本。</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bnw70P.png" alt="bnw70P.png"></center><p><strong>结果：</strong></p><center><img src="https://s4.ax1x.com/2022/02/27/bnw2FO.png" alt="bnw2FO.png"></center><ol><li>SMERTI模型在 SLOR、CSS、STES 三个数据集上都可以超过基于 WordNet、word2vec 的模型。</li><li>基于 WordNet 的模型在 SPA 数据集上效果最好，这可能是因为 WordNet 模型只对动词和名词进行修改，这保留了大部分原文本。</li></ol><h3 id="LAMBADA-AAAI2020-NLP"><a href="#LAMBADA-AAAI2020-NLP" class="headerlink" title="LAMBADA(AAAI2020, NLP)"></a>LAMBADA(AAAI2020, NLP)</h3><p><strong>论文链接：</strong><a href="https://arxiv.org/pdf/1911.03118.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1911.03118.pdf</a></p><p><strong>评价方式 / 数据集：</strong></p><ol><li>Text Classification：</li></ol><center><img src="https://s4.ax1x.com/2022/02/27/bnw6w6.png" alt="bnw6w6.png"></center><p><strong>扩增方法：</strong></p><ol><li>Train baseline classifier<br>使用本地文本分类数据集$D_{train}$训练一个 baseline 分类器 $h$（论文中准备了三种分类器：SVM、LSTM、BERT）</li><li>Fine-tune language model<br>将预训练语言模型 GPT-2 在处理后的文本分类数据集上进行语言模型任务的微调，得到 $GPT-2_{Finetuned}$。数据集处理方式如下：<br>$$Label, [SEP], Text_1, [EOS]$$$$Label, [SEP], Text_2, [EOS]$$$$…$$<br>$$Label, [SEP], Text_n, [EOS]$$</li><li>Synthesize labeled data<br>对于给定标签输入$Label, [SEP]$，使用微调后的 $GPT2_{Finetuned}$ 继续预测直到预测出 $[EOS]$，将预测得到的句子及其标签整理成一个数据扩增候选数据集$D^*$。</li><li>Filter synthesized data<br>使用第一步训练得到的 baseline 分类器 $h$对数据扩增候选数据集$D^*$进行预测，保留每个类别中预测结果的置信度最高的 N 个样本，并将这些高置信度数据整理成一个数据扩增数据集$D_{synthesized}$。将原始数据集$D_{train}$和数据扩增数据集$D_{synthesized}$合成为一个新的数据集$D_{new}$。</li><li>Repeat<br>将$D_{new0}$作为新的${D_{train}}$，重复步骤1-4，得到新的$[{D_{new1}}, {D_{new2}}, …]$。</li></ol><p><strong>结果：</strong></p><p><center><a href="https://imgtu.com/i/bnwhSH" target="_blank" rel="noopener"><img src="https://s4.ax1x.com/2022/02/27/bnwhSH.png" alt="bnwhSH.png"></a><center></center></center></p><ol><li>LAMBADA扩增方法在多种分类器上都能取得相比其他扩增方法较为可观的提升。</li><li>生成的高质量样本帮助了分类模型进一步泛化。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Data-Augmentation&quot;&gt;&lt;a href=&quot;#Data-Augmentation&quot; class=&quot;headerlink&quot; title=&quot;Data Augmentation&quot;&gt;&lt;/a&gt;Data Augmentation&lt;/h1&gt;&lt;p&gt;主要参考：&lt;/p&gt;

      
    
    </summary>
    
      <category term="学习" scheme="/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="NLP" scheme="/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>文本匹配方法</title>
    <link href="/2022/02/12/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%96%B9%E6%B3%95/"/>
    <id>/2022/02/12/文本匹配方法/</id>
    <published>2022-02-12T03:51:00.000Z</published>
    <updated>2023-01-15T11:53:26.833Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Text-Matching"><a href="#Text-Matching" class="headerlink" title="Text Matching"></a>Text Matching</h1><p>主要参考：</p><ol><li><a href="https://tech.meituan.com/2021/06/03/acl-2021-consert-bert.html" target="_blank" rel="noopener">https://tech.meituan.com/2021/06/03/acl-2021-consert-bert.html</a></li><li><a href="https://www.zhihu.com/column/c_1370341763794710528" target="_blank" rel="noopener">https://www.zhihu.com/column/c_1370341763794710528</a></li></ol><h2 id="Definition-amp-Overview"><a href="#Definition-amp-Overview" class="headerlink" title="Definition &amp; Overview"></a>Definition &amp; Overview</h2><p>文本匹配任务可以用文本相似度量化。</p><p>对于给定文本 $ t_{target} $ ，找到文本集合 $ T=[t_1, …, t_n] $ 中  $ t_i $ 与  $ t_{target} $ 相似度最高的文本。</p><p>该任务可以帮助模型学到词汇、句子级别的语义。<strong>任务的本质 </strong>是学习到可靠的句子表征。</p><p>文本相似度可以表现为：</p><ol><li>词汇重叠</li><li>语义重叠</li></ol><p>词汇重叠的评价方式：</p><ol><li>基于词汇重合：Jaccard矩阵、Rouge（n-gram）</li><li>基于频率：tf-idf</li></ol><p>语义重叠的评价方式：</p><ol><li>句向量的相似度 / 距离（Representation based）</li><li>输入句子对的分类输出（Interaction based）</li></ol><p>句向量的获取：</p><ol><li>直接获取（[CLS] Token、Paragraph Vector）</li><li>词向量之和</li></ol><p>句子对分类输出：<br>[CLS] [word1] [word2] [word3] [SEP] [word1] [word2] [word3] [word4] [SEP]</p><h2 id="有监督方法学习句子表征"><a href="#有监督方法学习句子表征" class="headerlink" title="有监督方法学习句子表征"></a>有监督方法学习句子表征</h2><center><img src="https://s4.ax1x.com/2022/02/12/Hw12DI.jpg" alt="有监督方法学习句子表征"></center><h3 id="Siamese结构（Representation-based）"><a href="#Siamese结构（Representation-based）" class="headerlink" title="Siamese结构（Representation based）"></a>Siamese结构（Representation based）</h3><p>向量融合方法：</p><ol><li>拼接： $ (u; v) $</li><li>求差： $ |u - v| $</li><li>element-wise dot: $ u*v $</li></ol><h3 id="交互结构（Interaction-based）"><a href="#交互结构（Interaction-based）" class="headerlink" title="交互结构（Interaction based）"></a>交互结构（Interaction based）</h3><p>[CLS] [sentence 1] [SEP] [sentence 2] [SEP]</p><h2 id="无监督方法学习句子表征（预训练方法-迁移学习）"><a href="#无监督方法学习句子表征（预训练方法-迁移学习）" class="headerlink" title="无监督方法学习句子表征（预训练方法 + 迁移学习）"></a>无监督方法学习句子表征（预训练方法 + 迁移学习）</h2><h3 id="预训练方法"><a href="#预训练方法" class="headerlink" title="预训练方法"></a>预训练方法</h3><p>句向量预训练：</p><ol><li>句向量Token（Paragrapgh Vector / [CLS]）</li><li>预测上下句（Skip Thought / Next Sentence Prediction）</li></ol><h3 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h3><p>借助语义推理数据集 如SNLI（<a href="https://nlp.stanford.edu/projects/snli/" target="_blank" rel="noopener">Stanford Natural Language Inference</a>）预训练。</p><h2 id="句向量模型"><a href="#句向量模型" class="headerlink" title="句向量模型"></a>句向量模型</h2><h3 id="TF-IDF（doc-doc匹配）"><a href="#TF-IDF（doc-doc匹配）" class="headerlink" title="TF-IDF（doc-doc匹配）"></a>TF-IDF（doc-doc匹配）</h3><p>$ corpus = [Sentence_1, Sentence_2] $<br>$ Sentence_1 = [I, like, apple, devices, very, much] $<br>$ Sentence_2 = [Apple, is, a, company, who, is, selling, high-tech, devices] $</p><p>$ Term Freq = \frac{词汇在当前文本中出现次数}{当前文本总词数} $<br>$ Inverse Document Freq = log\frac{语料库中文本总个数}{包含该词汇的文本个数DF} $<br>$ TF-IDF = TF * IDF $ </p><p>$ Sentence_1 $</p><table><thead><tr><th style="text-align:center">Term</th><th style="text-align:center">$WordCount$</th><th style="text-align:center">$ TF_1 $</th><th style="text-align:center">$ IDF_1 $</th><th style="text-align:center">$ TF-IDF_1 $</th></tr></thead><tbody><tr><td style="text-align:center">i</td><td style="text-align:center">1</td><td style="text-align:center">$ \frac{1}{6} $</td><td style="text-align:center">$ log\frac{2}{1} $</td><td style="text-align:center">0.05</td></tr><tr><td style="text-align:center">like</td><td style="text-align:center">1</td><td style="text-align:center">$ \frac{1}{6} $</td><td style="text-align:center">$ log\frac{2}{1} $</td><td style="text-align:center">0.05</td></tr><tr><td style="text-align:center">apple</td><td style="text-align:center">1</td><td style="text-align:center">$ \frac{1}{6} $</td><td style="text-align:center">$ log\frac{2}{2} $</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">devices</td><td style="text-align:center">1</td><td style="text-align:center">$ \frac{1}{6} $</td><td style="text-align:center">$ log\frac{2}{2} $</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">very</td><td style="text-align:center">1</td><td style="text-align:center">$ \frac{1}{6} $</td><td style="text-align:center">$ log\frac{2}{1} $</td><td style="text-align:center">0.05</td></tr><tr><td style="text-align:center">much</td><td style="text-align:center">1</td><td style="text-align:center">$ \frac{1}{6} $</td><td style="text-align:center">$ log\frac{2}{1} $</td><td style="text-align:center">0.05</td></tr><tr><td style="text-align:center">is</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">$ log\frac{2}{1} $</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">a</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">$ log\frac{2}{1} $</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">company</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">$ log\frac{2}{1} $</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">who</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">$ log\frac{2}{1} $</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">selling</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">$ log\frac{2}{1} $</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">high-tech</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">$ log\frac{2}{1} $</td><td style="text-align:center">0</td></tr></tbody></table><p>$ Sentence_2 $</p><table><thead><tr><th style="text-align:center">Term</th><th style="text-align:center">$WordCount$</th><th style="text-align:center">$ TF_1 $</th><th style="text-align:center">$ IDF_1 $</th><th style="text-align:center">$ TF-IDF_1 $</th></tr></thead><tbody><tr><td style="text-align:center">i</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">$ log\frac{2}{1} $</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">like</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">$ log\frac{2}{1} $</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">apple</td><td style="text-align:center">1</td><td style="text-align:center">$ \frac{1}{8} $</td><td style="text-align:center">$ log\frac{2}{2} $</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">devices</td><td style="text-align:center">1</td><td style="text-align:center">$ \frac{1}{8} $</td><td style="text-align:center">$ log\frac{2}{2} $</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">very</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">$ log\frac{2}{1} $</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">much</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">$ log\frac{2}{1} $</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">is</td><td style="text-align:center">2</td><td style="text-align:center">$ \frac{2}{8} $</td><td style="text-align:center">$ log\frac{2}{1} $</td><td style="text-align:center">0.075</td></tr><tr><td style="text-align:center">a</td><td style="text-align:center">1</td><td style="text-align:center">$ \frac{1}{8} $</td><td style="text-align:center">$ log\frac{2}{1} $</td><td style="text-align:center">0.038</td></tr><tr><td style="text-align:center">company</td><td style="text-align:center">1</td><td style="text-align:center">$ \frac{1}{8} $</td><td style="text-align:center">$ log\frac{2}{1} $</td><td style="text-align:center">0.038</td></tr><tr><td style="text-align:center">who</td><td style="text-align:center">1</td><td style="text-align:center">$ \frac{1}{8} $</td><td style="text-align:center">$ log\frac{2}{1} $</td><td style="text-align:center">0.038</td></tr><tr><td style="text-align:center">selling</td><td style="text-align:center">1</td><td style="text-align:center">$ \frac{1}{8} $</td><td style="text-align:center">$ log\frac{2}{1} $</td><td style="text-align:center">0.038</td></tr><tr><td style="text-align:center">high-tech</td><td style="text-align:center">1</td><td style="text-align:center">$ \frac{1}{8} $</td><td style="text-align:center">$ log\frac{2}{1} $</td><td style="text-align:center">0.038</td></tr></tbody></table><p>$ Sentence_1 = [0.05, 0.05, 0, 0, 0.05, 0.05, 0, 0, 0, 0, 0, 0] $<br>$ Sentence_2 = [0, 0, 0, 0, 0, 0, 0.75, 0.38, 0.38, 0.38, 0.38, 0.38] $</p><p>$ Similarity = cos(Sentence1, Sentence2) $ </p><p>TF对词频加权（频率越高权重越高），IDF对特殊性加权（单词越特殊权重越高）。</p><h3 id="BM25（1980-1990，query-doc匹配）"><a href="#BM25（1980-1990，query-doc匹配）" class="headerlink" title="BM25（1980-1990，query-doc匹配）"></a>BM25（1980-1990，query-doc匹配）</h3><p>$ score(D, Q) = \sum_{i=1}^{n}IDF(q_i)\cdot R(D, q_i) $ </p><p>$ IDF(q_i) = log \frac{N - DF_i + 0.5}{DF_i + 0.5}$</p><p>$ R(D, q_i)=\frac{f(q_i, D) \cdot(k_1+1)}{f(q_i, D)+k_1\cdot(1+b\cdot(\frac{Length_D}{Length_{avg}} - 1))} $</p><h3 id="Paragraph-Vector（2014，无监督）"><a href="#Paragraph-Vector（2014，无监督）" class="headerlink" title="Paragraph Vector（2014，无监督）"></a>Paragraph Vector（2014，无监督）</h3><p>论文链接：<a href="https://arxiv.org/pdf/1405.4053.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1405.4053.pdf</a></p><p>论文解读：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/21242559" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21242559</a></li><li><a href="http://klausvon.cn/2019/11/21/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Distributed-Representations-of-Sentences-and-Documents/" target="_blank" rel="noopener">http://klausvon.cn/2019/11/21/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Distributed-Representations-of-Sentences-and-Documents/</a></li></ol><p>句向量中的CBOW：</p><p>已知上文预测后文</p><center><img src="https://s4.ax1x.com/2022/02/12/Hw3rd0.png" alt="screenshot 20220212 112530"></center><p>句向量中的skip-gram：</p><p>Another way is to ignore the context words in the input, but force the model to predict words randomly sampled from the paragraph in the output. In reality, what this means is that at each iteration of stochastic gradient descent, we sample a text window, then sample a random word from the text window and form a classification task given the Paragraph Vector.</p><p>另一种方法是无视上下文，让模型强行预测出段落中随机挑选的单词。在每次随机梯度下降中，采样得到一个文本窗口，将文本窗口中的随机单词作为分类任务的真值训练Paragraph Vector。</p><center><img src="https://s4.ax1x.com/2022/02/12/Hw3oo6.png" alt="screenshot 20220212 112747"></center><h3 id="Skip-Thought-Vectors（NIPS2015，无监督）"><a href="#Skip-Thought-Vectors（NIPS2015，无监督）" class="headerlink" title="Skip-Thought Vectors（NIPS2015，无监督）"></a>Skip-Thought Vectors（NIPS2015，无监督）</h3><p>论文链接：<a href="https://arxiv.org/pdf/1506.06726.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.06726.pdf</a></p><p>论文解读：<a href="https://zhuanlan.zhihu.com/p/21259831" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21259831</a></p><p>Input Sentence -&gt; Encoder -&gt; 2 direction</p><ol><li>Decoder 1：预测前一句话的Token</li><li>Decoder 2：预测后一句话的Token</li></ol><center><img src="https://s4.ax1x.com/2022/02/12/Hw8rXd.png" alt="screenshot 20220212 113429"></center><h3 id="Infersent（EMNLP2017，双塔，迁移学习）"><a href="#Infersent（EMNLP2017，双塔，迁移学习）" class="headerlink" title="Infersent（EMNLP2017，双塔，迁移学习）"></a>Infersent（EMNLP2017，双塔，迁移学习）</h3><p>论文链接：<a href="https://arxiv.org/pdf/1705.02364.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1705.02364.pdf</a></p><p>论文解读：<a href="https://zhuanlan.zhihu.com/p/370619849" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/370619849</a></p><p>双塔结构 + SNLI数据集迁移学习</p><center><img src="https://s4.ax1x.com/2022/02/12/Hw8zjJ.png" alt="screenshot 20220127 150620"></center><p>结论：迁移学习优于无监督的SkipThought。</p><center><img src="https://s4.ax1x.com/2022/02/12/HwGuDA.png" alt="image 20220128151000752"></center><h3 id="BERT（NAACL2019，交互结构，无监督）"><a href="#BERT（NAACL2019，交互结构，无监督）" class="headerlink" title="BERT（NAACL2019，交互结构，无监督）"></a>BERT（NAACL2019，交互结构，无监督）</h3><p>论文链接：<a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1810.04805.pdf</a></p><p>NSP：用前一句话预测下一句话是否与前一句话连续。</p><center><img src="https://s4.ax1x.com/2022/02/12/HwG6v4.png" alt="image 20220127152005211"></center><p>有监督：</p><p>[CLS] Sentence 1 [SEP] Sentence 2 [SEP]</p><p>通过[CLS]接分类器判别句子是否相似。</p><h3 id="Sentence-BERT（EMNLP2019，双塔，迁移学习）"><a href="#Sentence-BERT（EMNLP2019，双塔，迁移学习）" class="headerlink" title="Sentence-BERT（EMNLP2019，双塔，迁移学习）"></a>Sentence-BERT（EMNLP2019，双塔，迁移学习）</h3><p>论文链接：<a href="https://arxiv.org/pdf/1908.10084.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1908.10084.pdf</a></p><p>计算由共享参数的BERT生成的句子表征的相似度。</p><center><img src="https://s4.ax1x.com/2022/02/12/HwGIPK.png" alt="screenshot 20220126 143433"></center><p>结论：双塔结构迁移学习后的SBERT优于BERT原有结构生成的句向量</p><center><img src="https://s4.ax1x.com/2022/02/12/HwJMM4.png" alt="screenshot 20220126 143940"></center><h3 id="Cross-Thought（EMNLP2020，无监督）"><a href="#Cross-Thought（EMNLP2020，无监督）" class="headerlink" title="Cross-Thought（EMNLP2020，无监督）"></a>Cross-Thought（EMNLP2020，无监督）</h3><p>论文链接：<a href="https://arxiv.org/pdf/2010.03652.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2010.03652.pdf</a></p><p>在BERT基础上添加多个上下文token，用MLM方法通过周边其他句子的表示预测当前句子的token_id。</p><p>结论：</p><ol><li>优于MLM</li><li>计算上下文Token Representation算力消耗过大，带来的提升不可观</li></ol><center><img src="https://s4.ax1x.com/2022/02/12/HwJgW8.png" alt="image 20220127152859001"></center><p>Skip-Thought知中间求两边，为Skip。</p><p>Cross-Thought将上下文每句话句向量都考虑在内，为Cross。</p><h3 id="SLM（EMNLP2020，自监督）"><a href="#SLM（EMNLP2020，自监督）" class="headerlink" title="SLM（EMNLP2020，自监督）"></a>SLM（EMNLP2020，自监督）</h3><p>论文链接: <a href="https://arxiv.org/pdf/2010.16249.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2010.16249.pdf</a></p><p>打乱sentence的position_id后，预测顺序。</p><center><img src="https://s4.ax1x.com/2022/02/12/HwJooq.png" alt="screenshot 20220126 151831"></center><p>结论：</p><ol><li>优于MLM</li><li>预测顺序算力消耗过大，带来的提升不可观</li></ol><center><img src="https://s4.ax1x.com/2022/02/12/HwJOlF.png" alt="image 20220127153310852"></center><h3 id="BERT-flow（EMNLP2020，向量空间映射）"><a href="#BERT-flow（EMNLP2020，向量空间映射）" class="headerlink" title="BERT-flow（EMNLP2020，向量空间映射）"></a>BERT-flow（EMNLP2020，向量空间映射）</h3><p>论文链接：<a href="https://arxiv.org/pdf/2011.05864.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2011.05864.pdf</a><br>论文解读：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/337134133" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/337134133</a></li><li><a href="https://kexue.fm/archives/8069" target="_blank" rel="noopener">https://kexue.fm/archives/8069</a></li></ol><p>BERT空间 -&gt; 高斯空间（分布更加均匀）</p><center><img src="https://s4.ax1x.com/2022/02/12/HwJzwR.png" alt="image 20220127153529317"></center><h3 id="BERT-whitening（2021，向量空间映射）"><a href="#BERT-whitening（2021，向量空间映射）" class="headerlink" title="BERT-whitening（2021，向量空间映射）"></a>BERT-whitening（2021，向量空间映射）</h3><p>论文链接：<a href="https://arxiv.org/pdf/2103.15316.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2103.15316.pdf</a><br>论文解读：<a href="https://kexue.fm/archives/8069" target="_blank" rel="noopener">https://kexue.fm/archives/8069</a></p><p>对参数进行白化操作，使得均值变为零，协方差矩阵变换成单位阵，获得一个更好的句向量空间。</p><h3 id="ConSert（ACL2021，无监督学习，数据增强）"><a href="#ConSert（ACL2021，无监督学习，数据增强）" class="headerlink" title="ConSert（ACL2021，无监督学习，数据增强）"></a>ConSert（ACL2021，无监督学习，数据增强）</h3><p>论文链接：<a href="https://arxiv.org/pdf/2105.11741.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2105.11741.pdf</a><br>论文解读：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/378544839" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/378544839</a></li><li><a href="https://tech.meituan.com/2021/06/03/acl-2021-consert-bert.html" target="_blank" rel="noopener">https://tech.meituan.com/2021/06/03/acl-2021-consert-bert.html</a></li></ol><p>通过多种数据增强方法生成正例，缩进正例之间的距离，增加反例句子之间的距离。</p><p>数据增强：</p><ol><li>对抗攻击（Adversarial Attack）：仅适用于有监督？</li><li>打乱顺序：Shuffle Position ID。</li><li>裁剪：将对应Token的Embedding置零 / 将Embedding的某个维度置零</li><li>Dropout：没有Token和Embedding维度的指定，随机置零。</li></ol><center><img src="https://s4.ax1x.com/2022/02/12/HwY36g.png" alt="screenshot 20220126 154832"></center><p>结论：<br>有提升但是不如SimCSE，这篇文章在SimCSE之前发表。</p><h3 id="SimCSE（EMNLP2021，无监督，对比学习）"><a href="#SimCSE（EMNLP2021，无监督，对比学习）" class="headerlink" title="SimCSE（EMNLP2021，无监督，对比学习）"></a>SimCSE（EMNLP2021，无监督，对比学习）</h3><p>论文链接：<a href="https://arxiv.org/pdf/2104.08821.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2104.08821.pdf</a><br>论文解读：</p><ol><li><a href="https://blog.csdn.net/weixin_45839693/article/details/116302914" target="_blank" rel="noopener">https://blog.csdn.net/weixin_45839693/article/details/116302914</a></li><li><a href="https://kexue.fm/archives/8348" target="_blank" rel="noopener">https://kexue.fm/archives/8348</a></li></ol><p>通过dropout生成正例对，与其他句子生成反例对。本质是一种数据扩增。</p><center><img src="https://s4.ax1x.com/2022/02/12/HwYJmj.png" alt="screenshot 20220126 153341"></center><p>结论：</p><ol><li>自监督学习结果优于同量级有监督SBERT版本，对比学习很有效果。</li><li>有监督学习结果优于同量级有监督SBERT版本，可能是数据集/损失函数选择问题。</li></ol><center><img src="https://s4.ax1x.com/2022/02/12/HwY2A1.png" alt="screenshot 20220126 154544"></center><h2 id="数据增强方法"><a href="#数据增强方法" class="headerlink" title="数据增强方法"></a>数据增强方法</h2><p>文本级别的增强：</p><ol><li>回译：将文本翻译为另一种语言再翻译回来。</li><li>CBERT：将文本部分替换为[MASK]，并让BERT恢复这些词。</li><li>意译（Paraphrase）：使用训练好的Paraphrase模型生成同义句。</li></ol><p>Embedding级别的增强：</p><ol><li>对抗攻击（Adversarial Attack）：？仅适用于有监督？</li><li>打乱顺序：Shuffle Position ID。</li><li>裁剪：将对应Token的Embedding置零 / 将Embedding的某个维度置零</li><li>Dropout：没有Token和Embedding维度的指定，随机置零。</li></ol><h2 id="何为对比学习？"><a href="#何为对比学习？" class="headerlink" title="何为对比学习？"></a>何为对比学习？</h2><p>一种通过拉近正例拉远反例方法优化embedding空间的自监督预训练方法</p><h1 id="NLI-数据集"><a href="#NLI-数据集" class="headerlink" title="NLI 数据集"></a>NLI 数据集</h1><h2 id="OCNLI（5万对）"><a href="#OCNLI（5万对）" class="headerlink" title="OCNLI（5万对）"></a>OCNLI（5万对）</h2><p>OCNLI，即原生中文自然语言推理数据集，是第一个非翻译的、使用原生汉语的大型中文自然语言推理数据集。 OCNLI包含5万余训练数据，3千验证数据及3千测试数据。除测试数据外，我们将提供数据及标签。测试数据仅提供数据。</p><p>简介：</p><ol><li><a href="https://github.com/CLUEbenchmark/CLUE" target="_blank" rel="noopener">https://github.com/CLUEbenchmark/CLUE</a></li><li><a href="https://github.com/cluebenchmark/OCNLI" target="_blank" rel="noopener">https://github.com/cluebenchmark/OCNLI</a></li></ol><p>下载地址：<a href="https://storage.googleapis.com/cluebenchmark/tasks/ocnli_public.zip" target="_blank" rel="noopener">https://storage.googleapis.com/cluebenchmark/tasks/ocnli_public.zip</a></p><h2 id="CMNLI（39万对）"><a href="#CMNLI（39万对）" class="headerlink" title="CMNLI（39万对）"></a>CMNLI（39万对）</h2><p>CMNLI数据由两部分组成：XNLI和MNLI。数据来自于fiction，telephone，travel，government，slate等，对原始MNLI数据和XNLI数据进行了中英文转化，保留原始训练集，合并XNLI中的dev和MNLI中的matched作为CMNLI的dev，合并XNLI中的test和MNLI中的mismatched作为CMNLI的test，并打乱顺序。该数据集可用于判断给定的两个句子之间属于蕴涵、中立、矛盾关系。</p><p>简介：<a href="https://github.com/CLUEbenchmark/CLUE" target="_blank" rel="noopener">https://github.com/CLUEbenchmark/CLUE</a></p><p>下载地址：<a href="https://storage.googleapis.com/cluebenchmark/tasks/cmnli_public.zip" target="_blank" rel="noopener">https://storage.googleapis.com/cluebenchmark/tasks/cmnli_public.zip</a></p><h2 id="ChineseTextualInference（42万对）"><a href="#ChineseTextualInference（42万对）" class="headerlink" title="ChineseTextualInference（42万对）"></a>ChineseTextualInference（42万对）</h2><p>目前,关于文本蕴含的研究主要还是集中在英文,如评测中常常使用的SNLI数据集与MultiNIL:</p><ol><li>The Stanford Natural Language Inference (SNLI) 是斯坦福大学NLP组发布的文本蕴含识别的数据集。SNLI由人工标注的，一共包含570K个文本对，其中训练集550K，验证集10K，测试集10K，一共包含三类entailment，contradiction，neutra，上节提到的例子就是出自此数据集</li><li>The Multi-Genre Natural Language Inference (MultiNLI)是一个众包数据集，包含433k个文本对。</li></ol><p>简介：<a href="https://github.com/liuhuanyong/ChineseTextualInference" target="_blank" rel="noopener">https://github.com/liuhuanyong/ChineseTextualInference</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Text-Matching&quot;&gt;&lt;a href=&quot;#Text-Matching&quot; class=&quot;headerlink&quot; title=&quot;Text Matching&quot;&gt;&lt;/a&gt;Text Matching&lt;/h1&gt;&lt;p&gt;主要参考：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href
      
    
    </summary>
    
      <category term="学习" scheme="/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="NLP" scheme="/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>2022年后总结</title>
    <link href="/2022/02/08/2022%20%E5%B9%B4%E5%90%8E%E6%80%BB%E7%BB%93/"/>
    <id>/2022/02/08/2022 年后总结/</id>
    <published>2022-02-08T13:12:00.000Z</published>
    <updated>2023-01-16T17:10:46.339Z</updated>
    
    <content type="html"><![CDATA[<h1 id="2022年后总结"><a href="#2022年后总结" class="headerlink" title="2022年后总结"></a>2022年后总结</h1><p>现在是2022.02.08晚上，去年的这会儿我正在写去年的年前总结，并为即将踏上日本求学之旅而感到紧张。过去的一年大概是人生中最为惊险的一年，疫情之中一个人奔赴日本求学，毕业后一个人在日本吃喝玩。留学生活因为疫情被打断，但好在结果还算不错。过去的时间一直生活在既定的轨道中，因此特别感谢家人给我这样一个体验未知的机会，我也特别珍惜这段充满意外的生活。</p><h2 id="2021-03"><a href="#2021-03" class="headerlink" title="2021.03"></a>2021.03</h2><p>学习：</p><ul><li style="list-style: none"><input type="checkbox" checked> 大概在准备中期答辩</li></ul><p>生活：</p><ul><li style="list-style: none"><input type="checkbox" checked> 结束一个人的宾馆隔离</li><li style="list-style: none"><input type="checkbox" checked> <a href="/2021/03/25/我的炉石卡组/">炉石卡组研究</a>（别人的卡组api好像还得挂代理才能看）</li></ul><h2 id="2021-04"><a href="#2021-04" class="headerlink" title="2021.04"></a>2021.04</h2><p>学习：</p><ul><li style="list-style: none"><input type="checkbox" checked> 中期答辩</li><li style="list-style: none"><input type="checkbox" checked> NLP with GTX1060（文本分类）</li><li style="list-style: none"><input type="checkbox" checked> NLP with GTX1060（完形填空）</li><li style="list-style: none"><input type="checkbox"> NLP with GTX1060（机器翻译）（代码搞好了，文章忘写了，待续）</li><li style="list-style: none"><input type="checkbox" checked> 报了个N2冲刺班（狗头</li></ul><p>生活：</p><ul><li style="list-style: none"><input type="checkbox" checked> 和最好的朋友们去最好的水卷World Buffet（写着写着就哭了🥲</li></ul><h2 id="2021-05"><a href="#2021-05" class="headerlink" title="2021.05"></a>2021.05</h2><p>学习：</p><ul><li style="list-style: none"><input type="checkbox" checked> <a href="/2021/05/26/日语课总结（1）/">N2冲刺</a></li><li style="list-style: none"><input type="checkbox" checked> <a href="/2021/05/08/「天気の子」翻译（人物介绍、序章）/">「天気の子」序章翻译</a></li></ul><p>生活：</p><ul><li style="list-style: none"><input type="checkbox" checked> 和两位帅哥的长期一脚出球训练（但也没练出来</li></ul><h2 id="2021-06"><a href="#2021-06" class="headerlink" title="2021.06"></a>2021.06</h2><p>学习：</p><ul><li style="list-style: none"><input type="checkbox" checked> 大概在准备毕业答辩吧</li><li style="list-style: none"><input type="checkbox" checked> N2一直在长期性摆烂（一天两套题计划最后一共只完成了两套题</li></ul><p>生活：</p><ul><li style="list-style: none"><input type="checkbox" checked> 没生活了</li></ul><h2 id="2021-07"><a href="#2021-07" class="headerlink" title="2021.07"></a>2021.07</h2><p>学习：</p><ul><li style="list-style: none"><input type="checkbox" checked> 毕业答辩</li><li style="list-style: none"><input type="checkbox" checked> N2考试</li><li style="list-style: none"><input type="checkbox" checked> 借助油管生成的低质量字幕预训练神经翻译系统（<a href="/2021/07/11/「先輩とみなみちゃん」JA共済CM合集字幕翻译/">JA共済广告片</a>、<a href="/2021/07/11/「突撃！南島原情報局」南岛原市宣传片字幕翻译/">南岛原宣传片</a>）</li></ul><p>生活：</p><ul><li style="list-style: none"><input type="checkbox" checked> 因为答辩没啥可准备的所以在答辩之前买了相机并一直在玩相机</li></ul><h2 id="2021-08-2021-09"><a href="#2021-08-2021-09" class="headerlink" title="2021.08-2021.09"></a>2021.08-2021.09</h2><ul><li style="list-style: none"><input type="checkbox" checked> 一直在玩，有机会贴一下旧行程表吧，结果很多地方都没去（就当省钱了）。</li></ul><h2 id="2021-10-2021-12"><a href="#2021-10-2021-12" class="headerlink" title="2021.10-2021.12"></a>2021.10-2021.12</h2><p>学习：</p><ul><li style="list-style: none"><input type="checkbox" checked> 笔试-面试循环</li><li style="list-style: none"><input type="checkbox" checked> 间歇性leetcode</li><li style="list-style: none"><input type="checkbox" checked> 借助文档系统学习Python细节（<a href="/2021/11/17/Python的内置类型/">Python的内置类型</a>、<a href="/2021/12/01/Python的语句/">Python的语句</a>、<a href="/2021/12/03/Pytorch环境配置/">Pytorch环境配置</a>、<a href="/2021/12/07/Python的函数/">Python的函数</a>）</li><li style="list-style: none"><input type="checkbox" checked> 复习CS224基础（<a href="/2021/11/23/CS224N - 单词的含义表示/">CS224N - 单词的含义表示</a>、<a href="/2021/11/26/CS224N - 语言模型/">CS224N - 语言模型</a>）</li></ul><p>生活：</p><ul><li style="list-style: none"><input type="checkbox" checked> <a href="/2021/12/01/我的装机单/">我的装机单</a></li></ul><h2 id="2022-01-2022-02"><a href="#2022-01-2022-02" class="headerlink" title="2022.01-2022.02"></a>2022.01-2022.02</h2><p>学习：</p><ul><li style="list-style: none"><input type="checkbox" checked> 带薪学习</li></ul><p>生活：</p><ul><li style="list-style: none"><input type="checkbox" checked> 因为春节在家坐了七天牢导致没有完成文档和总结</li></ul><h1 id="2021总结-amp-2022展望"><a href="#2021总结-amp-2022展望" class="headerlink" title="2021总结&amp;2022展望"></a>2021总结&amp;2022展望</h1><p>去年的总结只总结了干了什么，没有啥价值，因此加了一个环节专门总结有价值的。</p><p>2021有哪些收获：</p><ul><li style="list-style: none"><input type="checkbox" checked> 一个人出国</li><li style="list-style: none"><input type="checkbox" checked> 一个人旅行</li><li style="list-style: none"><input type="checkbox" checked> 一个人生活</li><li style="list-style: none"><input type="checkbox" checked> 顺利毕业（2020目标）</li><li style="list-style: none"><input type="checkbox" checked> 考过N2</li><li style="list-style: none"><input type="checkbox" checked> 尝试完整的翻译日语文章段落和短片</li><li style="list-style: none"><input type="checkbox" checked> Leetcode数据结构与算法（2020目标）</li><li style="list-style: none"><input type="checkbox" checked> GTX1060 深度学习系列（2020目标）</li><li style="list-style: none"><input type="checkbox" checked> 感受到自己为了生计而努力</li></ul><p>2022有哪些需要继续坚持的：</p><ul><li style="list-style: none"><input type="checkbox" checked> 乐观（虽然一直都是因为被某人鼓励）</li><li style="list-style: none"><input type="checkbox" checked> 尝试完整的翻译日语文章段落和短片</li><li style="list-style: none"><input type="checkbox" checked> Leetcode数据结构与算法</li><li style="list-style: none"><input type="checkbox" checked> GTX1060 深度学习系列</li></ul><p>2022有哪些想要开始坚持的：</p><ul><li style="list-style: none"><input type="checkbox"> 锻炼！！！</li><li style="list-style: none"><input type="checkbox"> 把玩垃圾游戏的时间看书（看什么呢？）</li><li style="list-style: none"><input type="checkbox"> 学日语（一直断断续续）</li><li style="list-style: none"><input type="checkbox"> 弹琴（人不能又菜又三分钟热度）</li></ul><p>2022至少要立几个不那么抽象的flag：</p><ul><li style="list-style: none"><input type="checkbox"> 每周锻炼一次</li><li style="list-style: none"><input type="checkbox"> 每天问问自己有没有对爱我的人刻薄</li><li style="list-style: none"><input type="checkbox"> 两次机会呢考考N1吧，万一蒙过了呢</li><li style="list-style: none"><input type="checkbox"> 每周完成一篇文档</li></ul><p>那么在最后，希望2022年我爱的人和爱我的人都可以一切顺利，明年再见咯！</p><h1 id="附：2021全年更新日志"><a href="#附：2021全年更新日志" class="headerlink" title="附：2021全年更新日志"></a>附：2021全年更新日志</h1><p>2020搞的挺稳定的，2021基本没啥更新的地方。</p><h2 id="2022-02-08更新日志"><a href="#2022-02-08更新日志" class="headerlink" title="2022.02.08更新日志"></a>2022.02.08更新日志</h2><ol><li>将©更新为2019-2022，新年新气象！</li></ol><h2 id="2021-11-27更新日志"><a href="#2021-11-27更新日志" class="headerlink" title="2021.11.27更新日志"></a>2021.11.27更新日志</h2><ol><li>为部分时间错乱的post增加date条目</li><li>将Hexo环境迁移至台式电脑</li><li>要注意nodejs版本要用12.xx，否则会出现空html。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;2022年后总结&quot;&gt;&lt;a href=&quot;#2022年后总结&quot; class=&quot;headerlink&quot; title=&quot;2022年后总结&quot;&gt;&lt;/a&gt;2022年后总结&lt;/h1&gt;&lt;p&gt;现在是2022.02.08晚上，去年的这会儿我正在写去年的年前总结，并为即将踏上日本求学之旅
      
    
    </summary>
    
      <category term="生活" scheme="/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
      <category term="总结" scheme="/tags/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>Python的函数</title>
    <link href="/2021/12/07/Python%E7%9A%84%E5%87%BD%E6%95%B0/"/>
    <id>/2021/12/07/Python的函数/</id>
    <published>2021-12-07T04:04:00.000Z</published>
    <updated>2023-01-15T10:31:41.186Z</updated>
    
    <content type="html"><![CDATA[<ul><li>本文参考<br>[1] <a href="https://docs.python.org/zh-cn/3/" target="_blank" rel="noopener">官方 Python3 教程</a><br>[2] <a href="https://www.runoob.com/python3/python3-tutorial.html" target="_blank" rel="noopener">RUNOOB Python3 教程</a></li></ul><h1 id="Python的函数"><a href="#Python的函数" class="headerlink" title="Python的函数"></a>Python的函数</h1><h2 id="函数定义-def"><a href="#函数定义-def" class="headerlink" title="函数定义 def"></a>函数定义 def</h2><pre><code class="python">def f(arg1: str, arg2: int = 0 ) -&gt; str:    ......    return arg1</code></pre><h2 id="函数参数定义"><a href="#函数参数定义" class="headerlink" title="函数参数定义"></a>函数参数定义</h2><h3 id="位置参数和关键字参数"><a href="#位置参数和关键字参数" class="headerlink" title="位置参数和关键字参数"></a>位置参数和关键字参数</h3><pre><code class="python">def parrot(voltage, state=&#39;a stiff&#39;, action=&#39;voom&#39;, type=&#39;Norwegian Blue&#39;):    print(&quot;-- This parrot wouldn&#39;t&quot;, action, end=&#39; &#39;)    print(&quot;if you put&quot;, voltage, &quot;volts through it.&quot;)    print(&quot;-- Lovely plumage, the&quot;, type)    print(&quot;-- It&#39;s&quot;, state, &quot;!&quot;)</code></pre><p>该函数接受一个必选参数（<code>voltage</code>）和三个可选参数（<code>state</code>, <code>action</code> 和 <code>type</code>）。该函数可用下列方式调用：</p><pre><code class="python">parrot(1000)                                          # 1 positional argumentparrot(voltage=1000)                                  # 1 keyword argumentparrot(voltage=1000000, action=&#39;VOOOOOM&#39;)             # 2 keyword argumentsparrot(action=&#39;VOOOOOM&#39;, voltage=1000000)             # 2 keyword argumentsparrot(&#39;a million&#39;, &#39;bereft of life&#39;, &#39;jump&#39;)         # 3 positional argumentsparrot(&#39;a thousand&#39;, state=&#39;pushing up the daisies&#39;)  # 1 positional, 1 keyword</code></pre><p>以下调用函数的方式都无效：</p><pre><code class="python">parrot()                     # required argument missingparrot(voltage=5.0, &#39;dead&#39;)  # non-keyword argument after a keyword argumentparrot(110, voltage=220)     # duplicate value for the same argumentparrot(actor=&#39;John Cleese&#39;)  # unknown keyword argument</code></pre><p>参数设定的原则：</p><ol><li><p>函数定义时，必选参数在前，可选参数（带有默认值的参数）在后。</p></li><li><p>调用函数时，位置参数（基于位置读取）在前，关键词参数（基于调用关键词读取）在后。</p></li><li>不能对同一参数多次赋值（通过位置参数、关键词参数对同一参数赋值）</li></ol><h3 id="当单个参数为多变量或者键值对"><a href="#当单个参数为多变量或者键值对" class="headerlink" title="当单个参数为多变量或者键值对"></a>当单个参数为多变量或者键值对</h3><pre><code class="python">def cheeseshop(kind, *arguments, **keywords):    print(&quot;-- Do you have any&quot;, kind, &quot;?&quot;)    print(&quot;-- I&#39;m sorry, we&#39;re all out of&quot;, kind)    for arg in arguments:        print(arg)    print(&quot;-&quot; * 40)    for kw in keywords:        print(kw, &quot;:&quot;, keywords[kw])</code></pre><p>该函数可以用如下方式调用：</p><pre><code class="python">cheeseshop(&quot;Limburger&quot;, &quot;It&#39;s very runny, sir.&quot;,           &quot;It&#39;s really very, VERY runny, sir.&quot;,           shopkeeper=&quot;Michael Palin&quot;,           client=&quot;John Cleese&quot;,           sketch=&quot;Cheese Shop Sketch&quot;)</code></pre><p>输出结果如下：</p><pre><code class="markdown">-- Do you have any Limburger ?-- I&#39;m sorry, we&#39;re all out of LimburgerIt&#39;s very runny, sir.It&#39;s really very, VERY runny, sir.----------------------------------------shopkeeper : Michael Palinclient : John Cleesesketch : Cheese Shop Sketch</code></pre><p>函数会自动将 <code>&quot;Limburger&quot;</code> 赋给 <code>kind</code>， 将 <code>(&quot;It&#39;s very runny, sir.&quot;, &quot;It&#39;s really very, VERY runny, sir.&quot;)</code> 赋给 <code>*arguments</code>， 将 <code>{shopkeeper=&quot;Michael Palin&quot;, client=&quot;John Cleese&quot;, sketch=&quot;Cheese Shop Sketch&quot;}</code> 赋给 <code>**keywords</code>。</p><p>单个参数为多变量或者键值对的函数设定原则：</p><ol><li><code>*name</code> 必须在 <code>**name</code> 前面</li></ol><h3 id="特殊参数"><a href="#特殊参数" class="headerlink" title="特殊参数"></a>特殊参数</h3><p>默认情况下，参数可以按位置或显式关键字传递给 Python 函数。为了让代码易读、高效，最好限制参数的传递方式，这样，开发者只需查看函数定义，即可确定参数项是仅按位置、按位置或关键字，还是仅按关键字传递。</p><p>函数定义如下：</p><pre><code class="python">def f(pos1, pos2, /, pos_or_kwd, *, kwd1, kwd2):      -----------    ----------     ----------        |             |                  |        |        Positional or keyword   |        |                                - Keyword only         -- Positional only</code></pre><p><code>/</code> 和 <code>*</code> 是可选的。这些符号表明形参如何把参数值传递给函数：位置、位置或关键字、关键字。关键字形参也叫作命名形参。</p><p>特殊参数的函数设定原则：</p><ol><li><code>/</code>之前的参数均为位置限定参数（只能通过位置赋值）</li><li><code>*</code> 之后的参数均为关键词限定参数（只能通过关键词赋值）</li><li><code>/</code> 和 <code>*</code> 之间的参数不限定赋值方式</li></ol><p>函数调用要求独立的位置参数，但实参在列表或元组里时，要执行相反的操作。例如，内置的 <a href="https://docs.python.org/zh-cn/3/library/stdtypes.html#range" target="_blank" rel="noopener"><code>range()</code></a> 函数要求独立的 <em>start</em> 和 <em>stop</em> 实参。如果这些参数不是独立的，则要在调用函数时，用 <code>*</code> 操作符把实参从列表或元组解包出来：</p><pre><code class="python">&gt;&gt;&gt; list(range(3, 6))            # normal call with separate arguments[3, 4, 5]&gt;&gt;&gt; args = [3, 6]&gt;&gt;&gt; list(range(*args))            # call with arguments unpacked from a list[3, 4, 5]</code></pre><p>同样，字典可以用 <code>**</code> 操作符传递关键字参数：</p><pre><code class="python">&gt;&gt;&gt; def parrot(voltage, state=&#39;a stiff&#39;, action=&#39;voom&#39;):...     print(&quot;-- This parrot wouldn&#39;t&quot;, action, end=&#39; &#39;)...     print(&quot;if you put&quot;, voltage, &quot;volts through it.&quot;, end=&#39; &#39;)...     print(&quot;E&#39;s&quot;, state, &quot;!&quot;)...&gt;&gt;&gt; d = {&quot;voltage&quot;: &quot;four million&quot;, &quot;state&quot;: &quot;bleedin&#39; demised&quot;, &quot;action&quot;: &quot;VOOM&quot;}&gt;&gt;&gt; parrot(**d)-- This parrot wouldn&#39;t VOOM if you put four million volts through it. E&#39;s bleedin&#39; demised !</code></pre><h2 id="匿名函数-Lambda"><a href="#匿名函数-Lambda" class="headerlink" title="匿名函数 Lambda"></a>匿名函数 Lambda</h2><pre><code class="python">&gt;&gt;&gt; f = lambda a, b: a+b  &gt;&gt;&gt; f(5, 10) 15</code></pre><p>匿名函数可以用于快速定义一次性函数。</p><pre><code class="python">&gt;&gt;&gt; word_freq = {&#39;apple&#39;: 10, &#39;banana&#39;: 30, &#39;peach&#39;: 20} &gt;&gt;&gt; sorted(word_freq.items(), key=lambda x:x[1], reverse=False) [(&#39;apple&#39;, 10), (&#39;peach&#39;, 20), (&#39;banana&#39;, 30)]</code></pre><p>匿名函数可以用作列表、字典排序时的key值。</p><h2 id="编码风格"><a href="#编码风格" class="headerlink" title="编码风格"></a>编码风格</h2><p>Python 项目大多都遵循 <a href="https://www.python.org/dev/peps/pep-0008" target="_blank" rel="noopener"><strong>PEP 8</strong></a> 的风格指南；它推行的编码风格易于阅读、赏心悦目。Python 开发者均应抽时间悉心研读；以下是该提案中的核心要点：</p><ol><li><strong>缩进</strong>，用 4 个空格，不要用制表符。</li><li><strong>换行</strong>，一行不超过 79 个字符。</li><li>用 <strong>空行</strong> 分隔函数和类，及函数内较大的代码块。</li><li>最好把 <strong>注释放到单独一行</strong>。</li><li>运算符前后、逗号后要用 <strong>空格</strong>，但不要直接在括号内使用： <code>a = f(1, 2) + g(3, 4)</code>。</li><li>类和函数的命名要一致；按惯例，<strong>命名类</strong> 用 <code>UpperCamelCase</code>，<strong>命名函数与方法</strong> 用 <code>lowercase_with_underscores</code>。命名方法中第一个参数总是用 <code>self</code> (类和方法详见 <a href="https://docs.python.org/zh-cn/3/tutorial/classes.html#tut-firstclasses" target="_blank" rel="noopener">初探类</a>)。</li><li>编写用于国际多语环境的代码时，<strong>不要用生僻的编码</strong>。Python 默认的 UTF-8 或纯 ASCII 可以胜任各种情况。同理，就算多语阅读、维护代码的可能再小，也不要在标识符中使用非 ASCII 字符。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;本文参考&lt;br&gt;[1] &lt;a href=&quot;https://docs.python.org/zh-cn/3/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官方 Python3 教程&lt;/a&gt;&lt;br&gt;[2] &lt;a href=&quot;https://www
      
    
    </summary>
    
      <category term="学习" scheme="/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch环境配置</title>
    <link href="/2021/12/03/Pytorch%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    <id>/2021/12/03/Pytorch环境配置/</id>
    <published>2021-12-03T07:41:00.000Z</published>
    <updated>2023-01-14T16:49:32.531Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pytorch环境配置"><a href="#Pytorch环境配置" class="headerlink" title="Pytorch环境配置"></a>Pytorch环境配置</h1><p>硬件环境：AMD5900X+NVIDIA3070</p><p>软件环境配置：windows11+python3.7+pytorch1.8+cuda11.1+cudnn8.2</p><h2 id="Python3-7安装"><a href="#Python3-7安装" class="headerlink" title="Python3.7安装"></a>Python3.7安装</h2><p>python3.7.9 官方下载链接：<a href="https://www.python.org/ftp/python/3.7.9/python-3.7.9-amd64.exe" target="_blank" rel="noopener">https://www.python.org/ftp/python/3.7.9/python-3.7.9-amd64.exe</a></p><p>安装时记得勾选 <code>add python3.7 to path</code></p><p>命令行中输入 <code>python</code> 得到如下回显即为成功：</p><pre><code class="markdown">PS C:\Users\circle&gt; pythonPython 3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)] on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt;</code></pre><h2 id="cuda11-1-cudnn8-2安装"><a href="#cuda11-1-cudnn8-2安装" class="headerlink" title="cuda11.1+cudnn8.2安装"></a>cuda11.1+cudnn8.2安装</h2><p>cuda版本可以查阅：NVIDIA控制面板-系统信息-组件</p><p>理论上安装的cuda版本要低于NVIDIA控制面板显示值并根据pytorch支持的情况进行选择。</p><a href="\img\post\Pytorch环境配置\控制面板信息.png" data-fancybox="images" data-caption="控制面板信息"><img src="\img\post\Pytorch环境配置\控制面板信息.png"></a><p>cuda 安装列表：<a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-toolkit-archive</a></p><p>cuda11.1 官方下载链接：<a href="https://developer.download.nvidia.com/compute/cuda/11.1.0/local_installers/cuda_11.1.0_456.43_win10.exe" target="_blank" rel="noopener">https://developer.download.nvidia.com/compute/cuda/11.1.0/local_installers/cuda_11.1.0_456.43_win10.exe</a></p><p>cudnn版本可以根据cudnn安装列表按照cuda版本进行安装，cudnn下载可能需要注册并登录NVIDIA官网。</p><p>cudnn 安装列表：<a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-archive</a></p><p>cudnn8.2 for cuda11.x 官方下载链接：<a href="https://developer.nvidia.com/compute/machine-learning/cudnn/secure/8.2.0.53/11.3_04222021/cudnn-11.3-windows-x64-v8.2.0.53.zip" target="_blank" rel="noopener">https://developer.nvidia.com/compute/machine-learning/cudnn/secure/8.2.0.53/11.3_04222021/cudnn-11.3-windows-x64-v8.2.0.53.zip</a></p><p>将cudnn解压后得到的文件复制到cuda根目录下即可。</p><a href="\img\post\Pytorch环境配置\文件目录.png" data-fancybox="images" data-caption="文件目录"><img src="\img\post\Pytorch环境配置\文件目录.png"></a><p>命令行中输入 <code>nvidia-smi</code> 得到如下回显即为成功：</p><pre><code class="markdown">PS C:\Users\circle&gt; nvidia-smiFri Dec  3 18:38:11 2021+-----------------------------------------------------------------------------+| NVIDIA-SMI 496.13       Driver Version: 496.13       CUDA Version: 11.5     ||-------------------------------+----------------------+----------------------+| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||                               |                      |               MIG M. ||===============================+======================+======================||   0  NVIDIA GeForce ... WDDM  | 00000000:0A:00.0  On |                  N/A ||  0%   40C    P8     9W / 220W |   1327MiB /  8192MiB |      3%      Default ||                               |                      |                  N/A |+-------------------------------+----------------------+----------------------+</code></pre><p>cuda版本建议安装11.1，其他版本有可能和pytorch兼容性差导致无法正常调用pytorch函数库。</p><h2 id="Pytorch1-8安装"><a href="#Pytorch1-8安装" class="headerlink" title="Pytorch1.8安装"></a>Pytorch1.8安装</h2><p>命令行输入 <code>pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html</code></p><p>安装成功后，命令行执行如下指令：</p><pre><code class="markdown">PS C:\Users\circle&gt; pythonPython 3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)] on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import torch&gt;&gt;&gt; torch.cuda.get_device_name(0)&#39;NVIDIA GeForce RTX 3070&#39;&gt;&gt;&gt;</code></pre><p>显卡名称正常回显即为成功安装pytorch。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Pytorch环境配置&quot;&gt;&lt;a href=&quot;#Pytorch环境配置&quot; class=&quot;headerlink&quot; title=&quot;Pytorch环境配置&quot;&gt;&lt;/a&gt;Pytorch环境配置&lt;/h1&gt;&lt;p&gt;硬件环境：AMD5900X+NVIDIA3070&lt;/p&gt;
&lt;p&gt;软件
      
    
    </summary>
    
      <category term="学习" scheme="/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="/tags/python/"/>
    
      <category term="pytorch" scheme="/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Python的语句</title>
    <link href="/2021/12/01/Python%E7%9A%84%E8%AF%AD%E5%8F%A5/"/>
    <id>/2021/12/01/Python的语句/</id>
    <published>2021-12-01T05:35:00.000Z</published>
    <updated>2023-01-15T10:31:48.122Z</updated>
    
    <content type="html"><![CDATA[<ul><li>本文参考<br>[1] <a href="https://docs.python.org/zh-cn/3/" target="_blank" rel="noopener">官方 Python3 教程</a><br>[2] <a href="https://www.runoob.com/python3/python3-tutorial.html" target="_blank" rel="noopener">RUNOOB Python3 教程</a></li></ul><h1 id="Python的语句"><a href="#Python的语句" class="headerlink" title="Python的语句"></a>Python的语句</h1><h2 id="条件控制语句"><a href="#条件控制语句" class="headerlink" title="条件控制语句"></a>条件控制语句</h2><h3 id="if语句"><a href="#if语句" class="headerlink" title="if语句"></a>if语句</h3><pre><code class="python">if condition_1:    statement_1elif condition_2:    statement_2else:    statement_3</code></pre><h3 id="match语句（3-10新特性）"><a href="#match语句（3-10新特性）" class="headerlink" title="match语句（3.10新特性）"></a>match语句（3.10新特性）</h3><pre><code class="python">match var:    case var1:        statement_1    case var2 if condition:        statement_2    case var3|var4|var5:        statement_3</code></pre><h2 id="循环语句"><a href="#循环语句" class="headerlink" title="循环语句"></a>循环语句</h2><h3 id="for-循环"><a href="#for-循环" class="headerlink" title="for 循环"></a>for 循环</h3><pre><code class="python">for i in range(n):    statement_1else:    statement_2</code></pre><h3 id="while-循环"><a href="#while-循环" class="headerlink" title="while 循环"></a>while 循环</h3><pre><code class="python">while condition:    statement_1else:    statement_2</code></pre><h3 id="break语句和continue语句"><a href="#break语句和continue语句" class="headerlink" title="break语句和continue语句"></a>break语句和continue语句</h3><pre><code class="python">for i in range(n):    if condition:        statement_1        continue    else:        statement_2        break</code></pre><h3 id="pass语句"><a href="#pass语句" class="headerlink" title="pass语句"></a>pass语句</h3><pre><code class="python">while condition:    statement_1    pass</code></pre><h2 id="异常语句"><a href="#异常语句" class="headerlink" title="异常语句"></a>异常语句</h2><h3 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h3><pre><code class="python">try:    statement_1except XxxError:    statement_2else:    statement_3finally:    statement_4</code></pre><h3 id="触发异常"><a href="#触发异常" class="headerlink" title="触发异常"></a>触发异常</h3><pre><code class="python">assert condition</code></pre><pre><code class="python">if not condition:    raise XxxError</code></pre><p>当 XxxError 为 AssertionError 时，上述两者等价。 </p><h2 id="命名空间和作用域控制"><a href="#命名空间和作用域控制" class="headerlink" title="命名空间和作用域控制"></a>命名空间和作用域控制</h2><p>命名空间是变量名到对象的映射。</p><p>命名空间主要有三种：</p><ol><li>Built-in</li><li>Global</li><li>Local</li></ol><p>作用域是可以直接访问到命名空间的区域。</p><p>作用域主要有四种：</p><ol><li>Built-in</li><li>Global</li><li>Enclosing</li><li>Local</li></ol><pre><code class="python">global_var = 0  # Global 作用域def enclosing_fun():    # Enclosing 作用域    enclosing_var = 0    def local_fun():        # Local 作用域        local_var = 0</code></pre><p>对于同一变量，访问顺序为当前作用域由下至上（Local、Enclosing、Global、Built-in）。</p><p>我认为命名空间仅仅是一种逻辑空间的映射，不具有物理意义；而作用域可以体现在代码块的嵌套关系上，也可以体现逻辑空间映射，但具有物理意义。</p><h3 id="global-amp-nonlocal"><a href="#global-amp-nonlocal" class="headerlink" title="global &amp; nonlocal"></a>global &amp; nonlocal</h3><p>这个例子演示了如何引用不同作用域和名称空间：</p><pre><code class="python">def scope_test():    def do_local():        spam = &quot;local spam&quot;    def do_nonlocal():        nonlocal spam        spam = &quot;nonlocal spam&quot;    def do_global():        global spam        spam = &quot;global spam&quot;    spam = &quot;test spam&quot;    do_local()    print(&quot;After local assignment:&quot;, spam)    do_nonlocal()    print(&quot;After nonlocal assignment:&quot;, spam)    do_global()    print(&quot;After global assignment:&quot;, spam)scope_test()print(&quot;In global scope:&quot;, spam)</code></pre><p>这段代码的输出：</p><pre><code>After local assignment: test spamAfter nonlocal assignment: nonlocal spamAfter global assignment: nonlocal spamIn global scope: global spam</code></pre><p>解释：</p><ol><li>输出1没有受到 <code>do_local()</code> 的影响是因为 local 作用域的变量无法直接影响 enclosing 作用域。</li><li>输出2受到了 <code>do_nonlocal()</code> 的影响是因为 <code>nonlocal spam</code> 语句赋予了 local 作用域的 spam 影响 enclosing 作用域的能力。</li><li>输出4受到了 <code>do_global()</code> 的影响是因为 <code>global spam</code> 语句赋予了 local 作用域的 spam 影响 global 作用域的能力，而输出3没有受到 <code>do_global()</code> 的影响是因为 <code>global spam</code> 语句没有赋予其直接影响 enclosing 作用域的能力，而 enclosing 作用域的优先级别高于 global 作用域。</li></ol><p>不使用 global &amp; nonlocal 可以用如下方式控制变量的流动：</p><pre><code class="python">def fun(x) -&gt; int:    x = x + 1    return xx = 0x = fun(x)print(x)</code></pre><p>使用 global &amp; nonlocal 可以用如下方式控制变量的流动：</p><pre><code class="python">def fun():    global x    x = x + 1x = 0fun()print(x)</code></pre><h2 id="del-amp-with-amp-yield（待续）"><a href="#del-amp-with-amp-yield（待续）" class="headerlink" title="del&amp;with&amp;yield（待续）"></a>del&amp;with&amp;yield（待续）</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;本文参考&lt;br&gt;[1] &lt;a href=&quot;https://docs.python.org/zh-cn/3/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官方 Python3 教程&lt;/a&gt;&lt;br&gt;[2] &lt;a href=&quot;https://www
      
    
    </summary>
    
      <category term="学习" scheme="/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>我的装机单</title>
    <link href="/2021/12/01/%E6%88%91%E7%9A%84%E8%A3%85%E6%9C%BA%E5%8D%95/"/>
    <id>/2021/12/01/我的装机单/</id>
    <published>2021-12-01T04:28:00.000Z</published>
    <updated>2023-01-14T15:12:50.541Z</updated>
    
    <content type="html"><![CDATA[<h1 id="我的装机单"><a href="#我的装机单" class="headerlink" title="我的装机单"></a>我的装机单</h1><table><thead><tr><th style="text-align:center">配件</th><th style="text-align:center">型号</th><th style="text-align:center">价格</th></tr></thead><tbody><tr><td style="text-align:center">CPU</td><td style="text-align:center">AMD 5900X</td><td style="text-align:center">3000</td></tr><tr><td style="text-align:center">GPU</td><td style="text-align:center">影驰 金属大师 RTX3070</td><td style="text-align:center">3900</td></tr><tr><td style="text-align:center">主板</td><td style="text-align:center">技嘉 X570 AORUS PRO WIFI</td><td style="text-align:center">1500</td></tr><tr><td style="text-align:center">内存</td><td style="text-align:center">金士顿 FURY 16GB DDR4 3200 * 2</td><td style="text-align:center">400 * 2</td></tr><tr><td style="text-align:center">SSD</td><td style="text-align:center">三星 980 1T * 2</td><td style="text-align:center">750 * 2</td></tr><tr><td style="text-align:center">CPU散热</td><td style="text-align:center">九州风神 大霜塔 PRO</td><td style="text-align:center">150</td></tr><tr><td style="text-align:center">电源</td><td style="text-align:center">酷冷至尊 GX750 金牌全模组</td><td style="text-align:center">500</td></tr><tr><td style="text-align:center">机箱</td><td style="text-align:center">酷冷至尊 S600 清风侠</td><td style="text-align:center">320</td></tr><tr><td style="text-align:center">晒单返E卡</td><td style="text-align:center"></td><td style="text-align:center">-300</td></tr><tr><td style="text-align:center">合计</td><td style="text-align:center"></td><td style="text-align:center">11370</td></tr></tbody></table><p><del>给孩子买来上网课的，孩子用了很喜欢。</del></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;我的装机单&quot;&gt;&lt;a href=&quot;#我的装机单&quot; class=&quot;headerlink&quot; title=&quot;我的装机单&quot;&gt;&lt;/a&gt;我的装机单&lt;/h1&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:center&quot;&gt;配件&lt;/th&gt;
&lt;t
      
    
    </summary>
    
      <category term="生活" scheme="/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
      <category term="彩音之选" scheme="/tags/%E5%BD%A9%E9%9F%B3%E4%B9%8B%E9%80%89/"/>
    
  </entry>
  
  <entry>
    <title>CS224N - 语言模型</title>
    <link href="/2021/11/26/CS224N%20-%20%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    <id>/2021/11/26/CS224N - 语言模型/</id>
    <published>2021-11-26T11:50:00.000Z</published>
    <updated>2023-01-14T17:12:21.362Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CS224N-语言模型"><a href="#CS224N-语言模型" class="headerlink" title="CS224N - 语言模型"></a>CS224N - 语言模型</h1><p>## </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CS224N-语言模型&quot;&gt;&lt;a href=&quot;#CS224N-语言模型&quot; class=&quot;headerlink&quot; title=&quot;CS224N - 语言模型&quot;&gt;&lt;/a&gt;CS224N - 语言模型&lt;/h1&gt;&lt;p&gt;## &lt;/p&gt;

      
    
    </summary>
    
      <category term="学习" scheme="/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="NLP" scheme="/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>CS224N - 单词的含义表示</title>
    <link href="/2021/11/23/CS224N%20-%20%E5%8D%95%E8%AF%8D%E7%9A%84%E5%90%AB%E4%B9%89%E8%A1%A8%E7%A4%BA/"/>
    <id>/2021/11/23/CS224N - 单词的含义表示/</id>
    <published>2021-11-23T11:00:00.000Z</published>
    <updated>2023-01-15T12:23:54.474Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Lecture-1"><a href="#Lecture-1" class="headerlink" title="Lecture 1"></a>Lecture 1</h1><h2 id="1-单词的含义表示"><a href="#1-单词的含义表示" class="headerlink" title="1 单词的含义表示"></a>1 单词的含义表示</h2><h3 id="1-1-单词的数据库-WordNet"><a href="#1-1-单词的数据库-WordNet" class="headerlink" title="1.1 单词的数据库 - WordNet"></a>1.1 单词的数据库 - WordNet</h3><p>待解决问题：如何表示单词的含义</p><p>描述：WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. </p><p>优点：</p><ol><li>词性标注</li><li>同义归类</li><li>词间关系</li></ol><p>缺点：</p><ol><li>缺少语境</li><li>缺少新的词汇和已有词汇的新含义</li><li>依赖人工标注</li><li>无法计算单词相似度（无法向量化）</li></ol><h3 id="1-2-单词的稀疏向量表示-One-hot-Vector"><a href="#1-2-单词的稀疏向量表示-One-hot-Vector" class="headerlink" title="1.2 单词的稀疏向量表示 - One-hot Vector"></a>1.2 单词的稀疏向量表示 - One-hot Vector</h3><p>待解决问题：如何表示单词的含义</p><p>描述：在传统NLP中，我们将不同的单词视为独立的个体。我们可以用 one-hot 编码的形式去表示他们，如下所示。向量的维度将会是词汇量的大小。</p><p>motel = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]<br>hotel = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]</p><p>缺点：</p><ol><li>one-hot 编码的向量都是正交的，无法计算相似度。</li></ol><h3 id="1-3-单词的稠密向量表示-Word-Vector"><a href="#1-3-单词的稠密向量表示-Word-Vector" class="headerlink" title="1.3 单词的稠密向量表示 - Word Vector"></a>1.3 单词的稠密向量表示 - Word Vector</h3><p>待解决问题：如何表示单词的含义-如何计算单词相似度</p><p>分布式语义：假设一个单词的含义取决于其高频出现的上下文。</p><p>描述：通过多个含有单词 w 的上下文建立起单词 w 的表示。这个表示是一个稠密向量，可被称为词表示或者词嵌入，这是一种分布式表示。</p><p>优点：</p><ol><li>在这种分布式语义中，所得到的稠密向量可以用于计算单词相似度，即在向量空间上同义词距离近，反义词距离远。</li></ol><h3 id="1-4-Word-Vector-的具体实现-word2vec"><a href="#1-4-Word-Vector-的具体实现-word2vec" class="headerlink" title="1.4 Word Vector 的具体实现 - word2vec"></a>1.4 Word Vector 的具体实现 - word2vec</h3><p>待解决问题：如何表示单词的含义-如何计算单词相似度</p><p>描述：</p><ol><li>根据 word vector 的思想，我们将基于上下文用稠密向量表示一个单词，这样可以在表示单词含义的同时保留单词之间的相关性信息。</li><li>对于我们的语料库中任意位置的单词 c（center），有上下文单词集合 o（outside）。</li><li>用单词 c 以及上下文单词集合 o 中的单词的 word vector 的相似度去计算 p(c | o) 以及 p(o | c)。同一单词，对于出现在上下文中的该单词以及出现在 center 位的该单词，我们用两个向量 <strong>*1</strong> 分别表示。</li><li>调整各单词的词向量使概率最大。</li></ol><p><strong>*1</strong> 可以使用单个向量表示，但对于同一单词同时出现在 center 和上下文，会出现 $ x^{T}x $ 的情况，会增加梯度下降难度。</p><p>损失函数：<br>$$<br>\begin{equation}<br>J(\theta)=-\frac{1}{T} \sum_{t=1}^{T} \sum_{-m \leq j \leq m \atop j \neq 0} \log P\left(w_{t+j} \mid w_{t} ; \theta\right)<br>\end{equation}<br>$$<br>损失函数会计算每一个位置 t 周围的 2m 个单词出现的概率。损失函数越小，概率越大。</p><p>条件概率：<br>$$<br>\begin{equation}<br>P(o \mid c)=\frac{\exp \left(u_{o}^{T} v_{c}\right)}{\sum_{w \in V} \exp \left(u_{w}^{T} v_{c}\right)}<br>\end{equation}<br>$$</p><p>条件概率的计算中，对上下文单词和中心单词词向量的点乘做了softmax正则化，即对于同一个中心单词，上下文单词词向量与中心单词词向量的点乘结果越大，其条件概率占整个上下文单词概率空间的比重越多。</p><p>优点：</p><ol><li>模型可以继承 <strong>1.3 单词的稠密向量表示 - Word Vector</strong> 的思路进行自我迭代。</li><li>迭代后得到的词向量能够计算单词相似度，且单词之间一定程度上具有自然语言的逻辑叠加性质，即经典示例 $ V_{king} - V_{man} + V_{woman} = V_{queen} $</li></ol><h1 id="Lecture-2"><a href="#Lecture-2" class="headerlink" title="Lecture 2"></a>Lecture 2</h1><h2 id="1-单词的含义表示-1"><a href="#1-单词的含义表示-1" class="headerlink" title="1 单词的含义表示"></a>1 单词的含义表示</h2><h3 id="1-5-word2vec-的两种形式-Skip-grams-和-Continuous-Bag-of-Words"><a href="#1-5-word2vec-的两种形式-Skip-grams-和-Continuous-Bag-of-Words" class="headerlink" title="1.5 word2vec 的两种形式 - Skip-grams 和 Continuous Bag of  Words"></a>1.5 word2vec 的两种形式 - Skip-grams 和 Continuous Bag of  Words</h3><p>待解决问题：如何表示单词的含义-如何计算单词相似度</p><p>word2vec 的两种形式：</p><ol><li>Skip-grams：给出中心单词预测上下文单词。（常见）</li><li>CBoW：给出上下文预测中心单词。</li></ol><h3 id="1-6-word2vec-的高效实现-Negative-Sampling"><a href="#1-6-word2vec-的高效实现-Negative-Sampling" class="headerlink" title="1.6 word2vec 的高效实现 - Negative Sampling"></a>1.6 word2vec 的高效实现 - Negative Sampling</h3><p>待解决问题：如何表示单词的含义-如何计算单词相似度-如何提高 word2vec 损失函数计算效率</p><p>描述：</p><p>在 <strong>1.4 Word Vector 的具体实现 - word2vec</strong> 中给出的条件概率计算方式在单词维度很大的时候，计算开销极大。在实际操作中使用随机负采样单词和中心单词配对，提升上下文单词词向量与中心单词词向量的点乘大小，降低随机采样单词与中心单词的点乘大小。</p><p>损失函数：<br>$$<br>J(\theta)=\frac{1}{T} \sum_{t=1}^{T} J_{t}(\theta)<br>$$<br>其中，<br>$$<br>J_{t}(\theta)=-\log \sigma(u_{o}^{T} v_{c})-\sum_{i=1}^{k} E_{j \sim P(w)}[\log \sigma(-u_{j}^{T} v_{c})]<br>$$</p><p>$$<br>\sigma(x)=\frac{1}{1+e^{-x}}<br>$$</p><p>损失函数的第一部分，上下文单词词向量与中心单词词向量点乘越大，损失函数越小。损失函数第二部分，随机负采样的k个单词的词向量与中心单词词向量点乘越小损失函数越小。</p><p>优点：</p><ol><li>相比原先的softmax需要在每次条件概率中计算整个单词表，负采样后的损失函数只需要计算上下文单词和经过随机负采样选出的单词，计算开销大幅下降。</li></ol><h3 id="1-7-Word-Vector-的简单实现-共现矩阵"><a href="#1-7-Word-Vector-的简单实现-共现矩阵" class="headerlink" title="1.7 Word Vector 的简单实现 - 共现矩阵"></a>1.7 Word Vector 的简单实现 - 共现矩阵</h3><p>待解决问题：如何表示单词的含义-如何计算单词相似度</p><p>描述：</p><p>通过单词间相邻次数（出现在对方相邻 n 个单词中的次数）构建整个词汇表中单词间的共现矩阵，并用矩阵的 行向量/列向量 表示该单词。</p><p>示例（语料库）：</p><ol><li>I like deep learning. </li><li>I like NLP.</li><li>I enjoy flying.</li></ol><table><thead><tr><th style="text-align:center">出现次数</th><th style="text-align:center">I</th><th style="text-align:center">like</th><th style="text-align:center">enjoy</th><th style="text-align:center">deep</th><th style="text-align:center">learning</th><th style="text-align:center">NLP</th><th style="text-align:center">flying</th><th style="text-align:center">.</th></tr></thead><tbody><tr><td style="text-align:center">I</td><td style="text-align:center">0</td><td style="text-align:center">2</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">like</td><td style="text-align:center">2</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">enjoy</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">deep</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">learning</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">NLP</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">flying</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">.</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td></tr></tbody></table><p>各单词对应词向量：</p><table><thead><tr><th style="text-align:center">单词</th><th style="text-align:center">向量</th></tr></thead><tbody><tr><td style="text-align:center">I</td><td style="text-align:center">[0, 2, 1, 0, 0, 0, 0, 0]</td></tr><tr><td style="text-align:center">like</td><td style="text-align:center">[2, 0, 0, 1, 0, 1, 0, 0]</td></tr><tr><td style="text-align:center">enjoy</td><td style="text-align:center">[1, 0, 0, 0, 0, 0, 1, 0]</td></tr><tr><td style="text-align:center">deep</td><td style="text-align:center">[0, 1, 0, 0, 1, 0, 0, 0]</td></tr><tr><td style="text-align:center">learning</td><td style="text-align:center">[0, 0, 0, 1, 0, 0, 0, 1]</td></tr><tr><td style="text-align:center">NLP</td><td style="text-align:center">[0, 1, 0, 0, 0, 0, 0, 1]</td></tr><tr><td style="text-align:center">flying</td><td style="text-align:center">[0, 0, 1, 0, 0, 0, 0, 1]</td></tr><tr><td style="text-align:center">.</td><td style="text-align:center">[0, 0, 0, 0, 1, 1, 1, 0]</td></tr></tbody></table><p>最终，根据上下文单词出现的频率，相似的单词（词法/含义 相似）会具有高相似度的词向量。</p><p>优点：</p><ol><li>实现逻辑简单，不需要训练。</li></ol><p>缺点：</p><ol><li>空间占用高，随着词汇量增大而增大。（解决方法：数学降维方法 / 预处理词库）</li></ol><h3 id="1-8-GloVe-共现矩阵和word2vec的结合"><a href="#1-8-GloVe-共现矩阵和word2vec的结合" class="headerlink" title="1.8 GloVe - 共现矩阵和word2vec的结合"></a>1.8 GloVe - 共现矩阵和word2vec的结合</h3><p>待解决问题：如何表示单词的含义-如何计算单词相似度-如何建立一个时空高效且效果好的模型</p><p>现有模型优缺点：</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">共现矩阵</th><th style="text-align:center">word2vec</th></tr></thead><tbody><tr><td style="text-align:center">特点</td><td style="text-align:center">基于单词出现次数</td><td style="text-align:center">基于概率预测</td></tr><tr><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">优点</td><td style="text-align:center">基于统计学，无需训练，时间高效</td><td style="text-align:center">能够捕捉单词相似度之外的复杂特征</td></tr><tr><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">能在 downstream 任务中获得更好的效果</td></tr><tr><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">缺点</td><td style="text-align:center">仅可用于捕捉单词相似度</td><td style="text-align:center">随着语料库的增加，训练过程耗时加长，效率降低</td></tr><tr><td style="text-align:center"></td><td style="text-align:center">对高频词汇分配了过多的权重</td></tr></tbody></table><p>GloVe核心思想：</p><ol><li>通过共现概率之比编码一些有用信息</li><li>用词向量点乘拟合共现概率</li></ol><table><thead><tr><th style="text-align:center">$ x =  $</th><th style="text-align:center">$ solid $</th><th style="text-align:center">$ gas $</th><th style="text-align:center">$ water $</th><th style="text-align:center">$ random $</th></tr></thead><tbody><tr><td style="text-align:center">$ \begin{equation}P(x&#124;ice)\end{equation} $</td><td style="text-align:center">高</td><td style="text-align:center">低</td><td style="text-align:center">高</td><td style="text-align:center">低</td></tr><tr><td style="text-align:center">$ P(x&#124;steam) $</td><td style="text-align:center">低</td><td style="text-align:center">高</td><td style="text-align:center">高</td><td style="text-align:center">低</td></tr><tr><td style="text-align:center">$ \frac{P(x&#124;ice)}{P(x&#124;steam)} $</td><td style="text-align:center">高</td><td style="text-align:center">低</td><td style="text-align:center">~1</td><td style="text-align:center">~1</td></tr></tbody></table><ul><li><code>&amp;#124;</code> 可用于转义 <code>|</code> 。</li></ul><p>共现概率之比可以编码一些成分信息（类比），如 $ V_{king} - V_{man} + V_{woman} = V_{queen} $</p><p>为了在训练中使得向量具有这种可叠加的特点，我们让词向量点乘拟合共现概率。<br>$$<br>w_{i} \cdot w_{j}=\log P(i \mid j) \tag{1}<br>$$</p><p>$$<br>w_{x} \cdot\left(w_{a}-w_{b}\right)=\log \frac{P(x \mid a)}{P(x \mid b)}\tag{2}<br>$$</p><p>通过 <strong>公式1</strong> 对于共现概率的拟合，词向量将拥有 <strong>公式2</strong> 的可叠加性，并将共现概率之比的信息保留在词向量中。</p><p>损失函数：<br>$$<br>J=\sum_{i, j=1}^{V} f(X_{i j})(w_{i}^{T} w_{j}+b_{i}+b_{j}-\log X_{i j})^{2}<br>$$</p><p>$ f(x) $ 是一个权重函数：<br>$$<br>\begin{equation}<br>f(x) =<br>\begin{cases}<br>(x / x_{\max })^{\alpha} &amp; \text { if } x&lt;x_{\max }<br>\newline 1 &amp; \text { otherwise }<br>\end{cases}<br>\end{equation}<br>$$<br>其中，$ x_{\max } $ 是共现次数的最大值，$ \alpha $ 的典型值为 $ \frac{3}{4} $ 。我们希望给予高频共现单词对更高的权重，但不像 <strong>1.7 Word Vector 的简单实现 - 共现矩阵</strong> 中高频共现词汇对那样过高的权重。</p><h3 id="1-9-如何评价-Word-Vector-的好坏？"><a href="#1-9-如何评价-Word-Vector-的好坏？" class="headerlink" title="1.9 如何评价 Word Vector 的好坏？"></a>1.9 如何评价 Word Vector 的好坏？</h3><ol><li><p>Intrinsic Evaluation：基于直接的词汇对，如 <strong>比较级 / 过去式</strong>。</p></li><li><p>Extrinsic Evaluation：基于真实的任务场景，如 <strong>命名实体识别</strong>。</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Lecture-1&quot;&gt;&lt;a href=&quot;#Lecture-1&quot; class=&quot;headerlink&quot; title=&quot;Lecture 1&quot;&gt;&lt;/a&gt;Lecture 1&lt;/h1&gt;&lt;h2 id=&quot;1-单词的含义表示&quot;&gt;&lt;a href=&quot;#1-单词的含义表示&quot; class
      
    
    </summary>
    
      <category term="学习" scheme="/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="NLP" scheme="/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Python的内置类型</title>
    <link href="/2021/11/17/Python%E7%9A%84%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/"/>
    <id>/2021/11/17/Python的内置类型/</id>
    <published>2021-11-17T03:46:00.000Z</published>
    <updated>2023-01-15T10:31:46.581Z</updated>
    
    <content type="html"><![CDATA[<ul><li>本文参考<br>[1] <a href="https://docs.python.org/zh-cn/3/" target="_blank" rel="noopener">官方 Python3 教程</a><br>[2] <a href="https://www.runoob.com/python3/python3-tutorial.html" target="_blank" rel="noopener">RUNOOB Python3 教程</a></li></ul><h1 id="Python的内置类型"><a href="#Python的内置类型" class="headerlink" title="Python的内置类型"></a>Python的内置类型</h1><h2 id="逻辑值"><a href="#逻辑值" class="headerlink" title="逻辑值"></a>逻辑值</h2><ol><li>被定义为假值的常量: <code>None</code> 和 <code>False</code>。</li><li>任何数值类型的零: <code>0</code>, <code>0.0</code>, <code>0j</code>, <code>Decimal(0)</code>, <code>Fraction(0, 1)</code></li><li>空的序列和多项集: <code>&#39;&#39;</code>, <code>()</code>, <code>[]</code>, <code>{}</code>, <code>set()</code>, <code>range(0)</code></li></ol><h2 id="逻辑运算"><a href="#逻辑运算" class="headerlink" title="逻辑运算"></a>逻辑运算</h2><table><thead><tr><th style="text-align:center">逻辑运算</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">x or y</td><td style="text-align:center">或</td></tr><tr><td style="text-align:center">x and y</td><td style="text-align:center">与</td></tr><tr><td style="text-align:center">not x</td><td style="text-align:center">非</td></tr></tbody></table><h2 id="比较运算"><a href="#比较运算" class="headerlink" title="比较运算"></a>比较运算</h2><table><thead><tr><th style="text-align:center">比较运算</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">&lt;</td><td style="text-align:center">严格小于</td></tr><tr><td style="text-align:center">&lt;=</td><td style="text-align:center">小于等于</td></tr><tr><td style="text-align:center">&gt;</td><td style="text-align:center">严格大于</td></tr><tr><td style="text-align:center">&gt;=</td><td style="text-align:center">大于等于</td></tr><tr><td style="text-align:center">==</td><td style="text-align:center">等于</td></tr><tr><td style="text-align:center">!=</td><td style="text-align:center">不等于</td></tr></tbody></table><h2 id="成员运算及身份运算"><a href="#成员运算及身份运算" class="headerlink" title="成员运算及身份运算"></a>成员运算及身份运算</h2><table><thead><tr><th style="text-align:center">运算</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">in</td><td style="text-align:center">包含于</td></tr><tr><td style="text-align:center">not in</td><td style="text-align:center">不包含于</td></tr><tr><td style="text-align:center">is</td><td style="text-align:center">相同对象</td></tr><tr><td style="text-align:center">is not</td><td style="text-align:center">不相同对象</td></tr></tbody></table><pre><code class="python">&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = a&gt;&gt;&gt; b == aTrue&gt;&gt;&gt; b is aTrue&gt;&gt;&gt; b = a[:]&gt;&gt;&gt; b == aTrue&gt;&gt;&gt; b is aFalse</code></pre><ol><li><code>b = a</code> 中，列表的赋值采用的是引用赋值，两者不光数值相同，指针指向也相同。其中之一的列表元素发生改变，另一个也会跟着改变。此时，a 和 b 为相同对象。</li><li><code>b = a[:]</code> 中，<code>[:]</code> 表示只取值，此时列表的赋值只是单纯的复制了数值，两者的数值虽然相同，但指针指向不同。其中之一的列表元素发生改变，另一个不会跟着改变。此时，a 和 b 为不同对象。</li></ol><h2 id="数字类型及运算符"><a href="#数字类型及运算符" class="headerlink" title="数字类型及运算符"></a>数字类型及运算符</h2><h3 id="数字类型"><a href="#数字类型" class="headerlink" title="数字类型"></a>数字类型</h3><ol><li>int<br><code>x = 5</code></li><li>float<br><code>x = 5.0</code></li><li>complex<br><code>x = 5 + 5.0j</code></li></ol><pre><code class="python">&gt;&gt;&gt; x = []&gt;&gt;&gt; x.append(5);x.append(5.1);x.append(5+5.1j)&gt;&gt;&gt; x[5, 5.1, (5+5.1j)]</code></pre><h3 id="数字运算"><a href="#数字运算" class="headerlink" title="数字运算"></a>数字运算</h3><table><thead><tr><th style="text-align:center">数字运算</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">x + y</td><td style="text-align:center">加</td></tr><tr><td style="text-align:center">x - y</td><td style="text-align:center">减</td></tr><tr><td style="text-align:center">x * y</td><td style="text-align:center">乘</td></tr><tr><td style="text-align:center">x / y</td><td style="text-align:center">除</td></tr><tr><td style="text-align:center">x // y</td><td style="text-align:center">整除（向下取整）</td></tr><tr><td style="text-align:center">x % y</td><td style="text-align:center">取余</td></tr><tr><td style="text-align:center">x ** y</td><td style="text-align:center">x 的 y 次幂</td></tr><tr><td style="text-align:center">pow(x, y)</td><td style="text-align:center">x 的 y 次幂</td></tr><tr><td style="text-align:center">abs(x)</td><td style="text-align:center">取绝对值</td></tr><tr><td style="text-align:center">int()</td><td style="text-align:center">整数强制类型转换</td></tr><tr><td style="text-align:center">float()</td><td style="text-align:center">浮点数强制类型转换</td></tr><tr><td style="text-align:center">complex(real, imag)</td><td style="text-align:center">复数</td></tr><tr><td style="text-align:center">divmod(x, y)</td><td style="text-align:center">(x // y, x % y)</td></tr><tr><td style="text-align:center">round(x[, n])</td><td style="text-align:center">将x保留n位小数（四舍五入）</td></tr><tr><td style="text-align:center">math.floor(x)</td><td style="text-align:center">&lt;=x的最大整数</td></tr><tr><td style="text-align:center">math.ceil(x)</td><td style="text-align:center">&gt;=x的最小整数</td></tr></tbody></table><h3 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a>位运算</h3><table><thead><tr><th style="text-align:center">位运算</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">x</td><td style="text-align:center">y</td></tr><tr><td style="text-align:center">x ^ y</td><td style="text-align:center">按位异或</td></tr><tr><td style="text-align:center">x &amp; y</td><td style="text-align:center">按位与</td></tr><tr><td style="text-align:center">x &lt;&lt; n</td><td style="text-align:center">左移n位</td></tr><tr><td style="text-align:center">x &gt;&gt; n</td><td style="text-align:center">右移n位</td></tr><tr><td style="text-align:center">~x</td><td style="text-align:center">按位取反</td></tr></tbody></table><h2 id="序列类型"><a href="#序列类型" class="headerlink" title="序列类型"></a>序列类型</h2><h3 id="通用序列操作"><a href="#通用序列操作" class="headerlink" title="通用序列操作"></a>通用序列操作</h3><table><thead><tr><th style="text-align:center">通用序列操作</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">x in s</td><td style="text-align:center">如果 s 中的某项等于 x 则结果为 True，否则为 False</td></tr><tr><td style="text-align:center">x not in s</td><td style="text-align:center">如果 s 中的某项等于 x 则结果为 False，否则为 True</td></tr><tr><td style="text-align:center">s + t</td><td style="text-align:center">s 与 t 相拼接</td></tr><tr><td style="text-align:center">s * n</td><td style="text-align:center">相当于 s 与自身进行 n 次拼接</td></tr><tr><td style="text-align:center">s[i]</td><td style="text-align:center">s 的第 i 项，起始为 0</td></tr><tr><td style="text-align:center">s[i:j]</td><td style="text-align:center">s 从 i 到 j 的切片</td></tr><tr><td style="text-align:center">s[i:j:step]</td><td style="text-align:center">s 从 i 到 j 步长为 k 的切片</td></tr><tr><td style="text-align:center">len(s)</td><td style="text-align:center">s 的长度</td></tr><tr><td style="text-align:center">min(s)</td><td style="text-align:center">s 的最小项</td></tr><tr><td style="text-align:center">max(s)</td><td style="text-align:center">s 的最大项</td></tr><tr><td style="text-align:center">s.index(x[, i[, j]])</td><td style="text-align:center">x 在 s 中首次出现项的索引号（索引号在 i 或其后且在 j 之前）</td></tr><tr><td style="text-align:center">s.count(x)</td><td style="text-align:center">x 在 s 中出现的总次数</td></tr></tbody></table><h3 id="可变序列操作"><a href="#可变序列操作" class="headerlink" title="可变序列操作"></a>可变序列操作</h3><table><thead><tr><th style="text-align:center">可变序列操作</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">s[i] = x</td><td style="text-align:center">将 s 的第 i 项替换为 x</td></tr><tr><td style="text-align:center">s[i:j] = t</td><td style="text-align:center">将 s 从 i 到 j 的切片替换为可迭代对象 t 的内容</td></tr><tr><td style="text-align:center">del s[i:j]</td><td style="text-align:center">等同于 s[i:j] = []</td></tr><tr><td style="text-align:center">s.append(x)</td><td style="text-align:center">将 x 添加到序列的末尾 (等同于 s[len(s):len(s)] = [x])</td></tr><tr><td style="text-align:center">s.clear()</td><td style="text-align:center">从 s 中移除所有项 (等同于 del s[:])</td></tr><tr><td style="text-align:center">s.copy()</td><td style="text-align:center">创建 s 的浅拷贝 (等同于 s[:])</td></tr><tr><td style="text-align:center">s.extend(t)</td><td style="text-align:center">用 t 的内容扩展 s (基本上等同于 s[len(s):len(s)] = t)</td></tr><tr><td style="text-align:center">s.insert(i, x)</td><td style="text-align:center">在由 i 给出的索引位置将 x 插入 s (等同于 s[i:i] = [x])</td></tr><tr><td style="text-align:center">s.pop(*[, i])</td><td style="text-align:center">提取在 i 位置上的项，并将其从 s 中移除</td></tr><tr><td style="text-align:center">s.remove(x)</td><td style="text-align:center">删除 s 中第一个 s[i] 等于 x 的项目。</td></tr><tr><td style="text-align:center">s.reverse()</td><td style="text-align:center">就地将列表中的元素逆序。</td></tr></tbody></table><h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><p>列表是可变序列，通常用于存放同类项目的集合（其中精确的相似程度将根据应用而变化）。</p><p><code>class list([iterable])</code><br>可以用多种方式构建列表：</p><ol><li>使用一对方括号来表示空列表: <code>[]</code></li><li>使用方括号，其中的项以逗号分隔: <code>[a]</code> / <code>[a, b, c]</code></li><li>使用列表推导式: <code>[x for _ in iterable]</code></li><li>使用类型的构造器: <code>list()</code> / <code>list(iterable)</code></li></ol><p><code>list.sort(\*, key=None, reverse=False)</code><br>此方法会对列表进行原地排序，保证稳定。</p><pre><code class="python">&gt;&gt;&gt; list = [1, 5, 3, 4, 6, 2]&gt;&gt;&gt; list.sort()&gt;&gt;&gt; list[1, 2, 3, 4, 5, 6]&gt;&gt;&gt; list = [[1, 5, 3], [4, 6, 2]]&gt;&gt;&gt; list.sort(key=lambda x:x[2])&gt;&gt;&gt; list[[4, 6, 2], [1, 5, 3]]</code></pre><h3 id="元组"><a href="#元组" class="headerlink" title="元组"></a>元组</h3><p>元组是不可变序列，通常用于储存异构数据的多项集（例如由 <code>enumerate()</code> 内置函数所产生的二元组）。 元组也被用于需要同构数据的不可变序列的情况（例如允许存储到 set 或 dict 的实例）。</p><p><code>class tuple([iterable])</code><br>可以用多种方式构建元组：</p><ol><li>使用一对圆括号来表示空元组: <code>()</code></li><li>使用一个后缀的逗号来表示单元组: <code>a,</code> / <code>(a,)</code></li><li>使用以逗号分隔的多个项: <code>a, b, c</code> / <code>(a, b, c)</code></li><li>使用内置的tuple(): <code>tuple()</code> / <code>tuple(iterable)</code></li></ol><h3 id="range-对象"><a href="#range-对象" class="headerlink" title="range 对象"></a>range 对象</h3><p>range 类型表示不可变的数字序列，通常用于在 for 循环中循环指定的次数。</p><p><code>class range(stop)</code><br><code>class range(start, stop[, step])</code><br>range 构造器的参数必须为整数。 </p><ol><li>如果省略 step 参数，其默认值为 1。 </li><li>如果省略 start 参数，其默认值为 0。</li><li>如果 step 为零则会引发 ValueError。</li></ol><pre><code class="python">&gt;&gt;&gt; list(range(10))[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&gt;&gt;&gt; list(range(1, 11))[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]&gt;&gt;&gt; list(range(0, 30, 5))[0, 5, 10, 15, 20, 25]&gt;&gt;&gt; list(range(0, 10, 3))[0, 3, 6, 9]&gt;&gt;&gt; list(range(0, -10, -1))[0, -1, -2, -3, -4, -5, -6, -7, -8, -9]&gt;&gt;&gt; list(range(0))[]&gt;&gt;&gt; list(range(1, 0))[]</code></pre><h3 id="文本序列类型"><a href="#文本序列类型" class="headerlink" title="文本序列类型"></a>文本序列类型</h3><p>在 Python 中处理文本数据是使用 str 对象，也称为 字符串。<br>字符串是由 Unicode 码位构成的不可变序列。</p><p><code>class str(object=&#39;&#39;)</code><br><code>class str(object=b&#39;&#39;, encoding=&#39;utf-8&#39;, errors=&#39;strict&#39;)</code><br>可以用多种方式构建：</p><ol><li>使用引号：<code>&#39;&#39;</code> /<code>&quot;&quot;</code> /<code>&#39;&#39;&#39; &#39;&#39;&#39;</code> / <code>&quot;&quot;&quot; &quot;&quot;&quot;</code></li><li>使用内置的str()：<code>str()</code> / <code>str(object)</code></li><li>使用raw禁止转义：<code>r&#39;\n&#39;</code></li></ol><table><thead><tr><th style="text-align:center">str的方法</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">str.capitalize()</td><td style="text-align:center">将str首字母大写</td></tr><tr><td style="text-align:center">str.find(s)</td><td style="text-align:center">检测字符串s是否在str中，真返回True，假返回False</td></tr><tr><td style="text-align:center">str.is?()</td><td style="text-align:center">检查str是否为?，真返回True，假返回False</td></tr><tr><td style="text-align:center">str.join(s)</td><td style="text-align:center">以str为分割，将s中的元素合并为一个新的字符串</td></tr><tr><td style="text-align:center">str.lower() / str.upper()</td><td style="text-align:center">将str全部字符转为小写/大写</td></tr><tr><td style="text-align:center">str.lstrip(s) / str.rstrip(s) / str.strip(s)</td><td style="text-align:center">将str 左侧/右侧/两侧 的字符串s去掉</td></tr><tr><td style="text-align:center">str.replace(old, new[, max])</td><td style="text-align:center">将str中的字符串old替换为new，最大次数为max</td></tr><tr><td style="text-align:center">str.split(sep, maxsplit=-1)</td><td style="text-align:center">将str以sep中的元素为基准进行分割，最多分割maxsplit次，得到maxsplit段</td></tr><tr><td style="text-align:center">str.startwith(startstr, beg=0, end=len(string))</td><td style="text-align:center">检查str是否在(beg, end)区间以startstr作为开头</td></tr><tr><td style="text-align:center">str.swapcase()</td><td style="text-align:center">将str的 大/小 写字母转换为 小/大 写字母</td></tr><tr><td style="text-align:center">str.title()</td><td style="text-align:center">将str中所有单词首字母大写</td></tr></tbody></table><p><code>str.is?()</code>：</p><table><thead><tr><th style="text-align:center">str.is?() 中的 ?</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">alnum</td><td style="text-align:center">是否只包含字母或数字</td></tr><tr><td style="text-align:center">alpha</td><td style="text-align:center">是否只包含字母</td></tr><tr><td style="text-align:center">digit</td><td style="text-align:center">是否只包含数字</td></tr><tr><td style="text-align:center">lower</td><td style="text-align:center">是否只包含小写字母（只考虑区分大小写的）</td></tr><tr><td style="text-align:center">numeric</td><td style="text-align:center">是否只包含数字字符</td></tr><tr><td style="text-align:center">space</td><td style="text-align:center">是否只包含空格</td></tr><tr><td style="text-align:center">upper</td><td style="text-align:center">是否只包含大写字母（只考虑区分大小写的）</td></tr></tbody></table><h3 id="字符串格式化"><a href="#字符串格式化" class="headerlink" title="字符串格式化"></a>字符串格式化</h3><p><a href="https://docs.python.org/zh-cn/3/library/string.html#formatstrings" target="_blank" rel="noopener">str.format()</a></p><h2 id="二进制序列类型（待补充）"><a href="#二进制序列类型（待补充）" class="headerlink" title="二进制序列类型（待补充）"></a>二进制序列类型（待补充）</h2><h2 id="集合类型-set-frozen"><a href="#集合类型-set-frozen" class="headerlink" title="集合类型 - set, frozen"></a>集合类型 - set, frozen</h2><p>set 对象是由具有唯一性的 hashable 对象所组成的无序多项集。常见的用途包括成员检测、从序列中去除重复项以及数学中的集合类计算，例如交集、并集、差集与对称差集等等。<br>目前有两种内置集合类型：set 和 frozenset。</p><ol><li>set 类型是可变的。其内容可以使用 <code>add()</code> 和 <code>remove()</code> 这样的方法来改变。由于是可变类型，它没有哈希值，且不能被用作字典的键或其他集合的元素。</li><li>frozenset 类型是不可变并且为 hashable 。其内容在被创建后不能再改变；因此它可以被用作字典的键或其他集合的元素。</li></ol><p><code>class set([iterable])</code><br><code>class frozenset([iterable])</code><br>返回一个新的 set 或 frozenset 对象，其元素来自于 iterable。集合的元素必须为 hashable 要表示由集合对象构成的集合，所有的内层集合必须为 frozenset 对象。如果未指定 iterable，则将返回一个新的空集合。<br>集合可用多种方式来创建：</p><ol><li>使用花括号内以逗号分隔元素的方式:<br><code>{&#39;jack&#39;, &#39;sjoerd&#39;}</code></li><li>使用集合推导式:<br><code>{c for c in &#39;abracadabra&#39; if c not in &#39;abc&#39;}</code></li><li>使用类型构造器:<br><code>set()</code> / <code>set(&#39;foobar&#39;)</code> / <code>set([&#39;a&#39;, &#39;b&#39;, &#39;foo&#39;])</code></li></ol><table><thead><tr><th style="text-align:center">set的操作</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">len(set)</td><td style="text-align:center">返回set中的元素数量</td></tr><tr><td style="text-align:center">element in set / element not in set</td><td style="text-align:center">判断element是否出现在set中</td></tr><tr><td style="text-align:center">set.add(element)</td><td style="text-align:center">将element加入set</td></tr><tr><td style="text-align:center">set.clear()</td><td style="text-align:center">清空set</td></tr><tr><td style="text-align:center">set.copy()</td><td style="text-align:center">返回set的浅拷贝</td></tr><tr><td style="text-align:center">setx.difference(sety)</td><td style="text-align:center">返回包含在setx但不包含在sety的元素</td></tr><tr><td style="text-align:center">set.discard(element)</td><td style="text-align:center">移除set中的指定element（element不存在不报错）</td></tr><tr><td style="text-align:center">set.intersection(set)</td><td style="text-align:center">返回两个集合的交集</td></tr><tr><td style="text-align:center">set.isjoint(set)</td><td style="text-align:center">判断两个集合是否有交集</td></tr><tr><td style="text-align:center">setx.issubset(sety)</td><td style="text-align:center">判断setx是否为sety的子集</td></tr><tr><td style="text-align:center">setx.issuperset(sety)</td><td style="text-align:center">判断setx是否为sety的父集</td></tr><tr><td style="text-align:center">set.pop()</td><td style="text-align:center">随机移除set中的元素，返回该元素</td></tr><tr><td style="text-align:center">set.remove(element)</td><td style="text-align:center">移除set中的指定element（element不存在报错）</td></tr><tr><td style="text-align:center">setx.symmetric_difference(sety)</td><td style="text-align:center">返回setx和sety中不重复的元素</td></tr><tr><td style="text-align:center">set.union(set)</td><td style="text-align:center">返回两个集合的并集</td></tr><tr><td style="text-align:center">setx.update(sety)</td><td style="text-align:center">将setx置为两个集合的并集</td></tr></tbody></table><h2 id="映射类型"><a href="#映射类型" class="headerlink" title="映射类型"></a>映射类型</h2><p>mapping 对象会将 hashable 值映射到任意对象。 映射属于可变对象。目前仅有一种标准映射类型 字典。<br>字典的键几乎可以是任何值。非 hashable 的值，即包含列表、字典或其他可变类型的值（此类对象基于值而非对象标识进行比较）不可用作键。数字类型用作键时遵循数字比较的一般规则：如果两个数值相等 (例如 1 和 1.0) 则两者可以被用来索引同一字典条目。</p><p><code>class dict(\*\*kwarg)</code><br><code>class dict(mapping, \*\*kwarg)</code><br><code>class dict(iterable, \*\*kwarg)</code><br>返回一个新的字典，基于可选的位置参数和可能为空的关键字参数集来初始化。<br>字典可用多种方式来创建：</p><ol><li>使用花括号内以逗号分隔 键: 值 对的方式: <code>{&#39;jack&#39;: 4098, &#39;sjoerd&#39;: 4127}</code> or <code>{4098: &#39;jack&#39;, 4127: &#39;sjoerd&#39;}</code></li><li>使用字典推导式: <code>{}</code>, <code>{x: x ** 2 for x in range(10)}</code></li><li>使用类型构造器: <code>dict()</code> / <code>dict([(&#39;foo&#39;, 100), (&#39;bar&#39;, 200)])</code> / <code>dict(foo=100, bar=200)</code></li></ol><table><thead><tr><th style="text-align:center">dict的操作</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center">list(dict)</td><td style="text-align:center">返回dict所有key的列表</td></tr><tr><td style="text-align:center">len(dict)</td><td style="text-align:center">返回dict的长度</td></tr><tr><td style="text-align:center">dict[key]</td><td style="text-align:center">返回dict中key的value，若key不存在则报错</td></tr><tr><td style="text-align:center">dict[key] = value</td><td style="text-align:center">将key增加至dict中并赋值value</td></tr><tr><td style="text-align:center">del dict[key]</td><td style="text-align:center">移除dict中的key，若key不存在则报错</td></tr><tr><td style="text-align:center">key in dict / key not in dict</td><td style="text-align:center">判断key是否在dict中</td></tr><tr><td style="text-align:center">dict.clear()</td><td style="text-align:center">清空dict</td></tr><tr><td style="text-align:center">dict.copy()</td><td style="text-align:center">返回dict的浅拷贝</td></tr><tr><td style="text-align:center">dict.fromkeys(iterable[, value])</td><td style="text-align:center">通过iterable类型的元素建立列表</td></tr><tr><td style="text-align:center">dict.get(key[, default])</td><td style="text-align:center">如果key存在则返回key对应的value，如果key不存在则返回default的值，default默认为None</td></tr><tr><td style="text-align:center">dict.items()</td><td style="text-align:center">以((key, value), ( , ), …)的形式返回dict的键值对</td></tr><tr><td style="text-align:center">dict.keys()</td><td style="text-align:center">以视图对象dict_keys([key1, key2, …])的形式返回dict的keys</td></tr><tr><td style="text-align:center">dict.pop(key)</td><td style="text-align:center">返回dict中key的value，并将该key移除（key不存在则报错）</td></tr><tr><td style="text-align:center">dict.popitem()</td><td style="text-align:center">以LIFO（Last In First Out）顺序返回dict中key的value，并将其移除</td></tr><tr><td style="text-align:center">dict.values()</td><td style="text-align:center">以视图对象dict_values([value1, value2, …])的形式返回dict的values</td></tr></tbody></table><h3 id="字典视图对象"><a href="#字典视图对象" class="headerlink" title="字典视图对象"></a>字典视图对象</h3><p>由 <code>dict.keys()</code>, <code>dict.values()</code> 和 <code>dict.items()</code> 所返回的对象是 视图对象。该对象提供字典条目的一个动态视图，这意味着当字典改变时，视图也会相应改变。<br>可以使用 <code>list(dict.keys())</code> 将字典视图对象转换为list。</p><h2 id="上下文管理器类型（待补充）"><a href="#上下文管理器类型（待补充）" class="headerlink" title="上下文管理器类型（待补充）"></a>上下文管理器类型（待补充）</h2><p>Python 的 with 语句支持通过上下文管理器所定义的运行时上下文这一概念。此对象的实现使用了一对专门方法，允许用户自定义类来定义运行时上下文，在语句体被执行前进入该上下文，并在语句执行完毕时退出该上下文。<br>详情请参考 <a href="\">with 语句</a>。</p><h2 id="union类型（待补充）"><a href="#union类型（待补充）" class="headerlink" title="union类型（待补充）"></a>union类型（待补充）</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;本文参考&lt;br&gt;[1] &lt;a href=&quot;https://docs.python.org/zh-cn/3/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官方 Python3 教程&lt;/a&gt;&lt;br&gt;[2] &lt;a href=&quot;https://www
      
    
    </summary>
    
      <category term="学习" scheme="/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>「突撃！南島原情報局」南岛原市宣传片字幕翻译</title>
    <link href="/2021/07/11/%E3%80%8C%E7%AA%81%E6%92%83%EF%BC%81%E5%8D%97%E5%B3%B6%E5%8E%9F%E6%83%85%E5%A0%B1%E5%B1%80%E3%80%8D%E5%8D%97%E5%B2%9B%E5%8E%9F%E5%B8%82%E5%AE%A3%E4%BC%A0%E7%89%87%E5%AD%97%E5%B9%95%E7%BF%BB%E8%AF%91/"/>
    <id>/2021/07/11/「突撃！南島原情報局」南岛原市宣传片字幕翻译/</id>
    <published>2021-07-11T08:30:38.000Z</published>
    <updated>2023-01-14T16:42:36.117Z</updated>
    
    <content type="html"><![CDATA[<h1 id="「突撃！南島原情報局」"><a href="#「突撃！南島原情報局」" class="headerlink" title="「突撃！南島原情報局」"></a>「突撃！南島原情報局」</h1><h2 id="作品简介"><a href="#作品简介" class="headerlink" title="作品简介"></a>作品简介</h2><p>日语宣传片的个人翻译练习。看到朋友搬运了这个视频感觉很有趣，就当作训练素材了（神经翻译系统训练ing）。这个宣传视频主要介绍了日本长崎县南岛原市的风光以及特产。</p><h2 id="突撃！南島原情報局【神回】"><a href="#突撃！南島原情報局【神回】" class="headerlink" title="突撃！南島原情報局【神回】"></a>突撃！南島原情報局【神回】</h2><div style="position: relative; width: 95%; height: 0; left: 2.5%; padding-bottom: 65%;"><br>    <iframe src="https://www.bilibili.com/blackboard/html5mobileplayer.html?aid=714134464&bvid=BV13X4y157GW&cid=362713060&page=1&high_quality=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;"> </iframe><br></div><h2 id="勘误"><a href="#勘误" class="headerlink" title="勘误"></a>勘误</h2><p>有些地方把 <strong>一揆（いっき）</strong> 听成了 <strong>域（いき）</strong>。因为之前并不知道 <strong>一揆（いっき）</strong> 这个词，导致字幕里面有的地方翻译成了 <strong>地区</strong> 忘了改成 <strong>起义</strong>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;「突撃！南島原情報局」&quot;&gt;&lt;a href=&quot;#「突撃！南島原情報局」&quot; class=&quot;headerlink&quot; title=&quot;「突撃！南島原情報局」&quot;&gt;&lt;/a&gt;「突撃！南島原情報局」&lt;/h1&gt;&lt;h2 id=&quot;作品简介&quot;&gt;&lt;a href=&quot;#作品简介&quot; class=&quot;
      
    
    </summary>
    
      <category term="学习" scheme="/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="日语" scheme="/tags/%E6%97%A5%E8%AF%AD/"/>
    
      <category term="翻译" scheme="/tags/%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>「先輩とみなみちゃん」JA共済CM合集字幕翻译</title>
    <link href="/2021/07/11/%E3%80%8C%E5%85%88%E8%BC%A9%E3%81%A8%E3%81%BF%E3%81%AA%E3%81%BF%E3%81%A1%E3%82%83%E3%82%93%E3%80%8DJA%E5%85%B1%E6%B8%88CM%E5%90%88%E9%9B%86%E5%AD%97%E5%B9%95%E7%BF%BB%E8%AF%91/"/>
    <id>/2021/07/11/「先輩とみなみちゃん」JA共済CM合集字幕翻译/</id>
    <published>2021-07-11T07:31:46.000Z</published>
    <updated>2023-01-14T16:42:51.164Z</updated>
    
    <content type="html"><![CDATA[<h1 id="「先輩とみなみちゃん」（前辈和美波酱）"><a href="#「先輩とみなみちゃん」（前辈和美波酱）" class="headerlink" title="「先輩とみなみちゃん」（前辈和美波酱）"></a>「先輩とみなみちゃん」（前辈和美波酱）</h1><h2 id="作品简介"><a href="#作品简介" class="headerlink" title="作品简介"></a>作品简介</h2><p>日语广告片的个人翻译练习。合集主要介绍了JA共済的相关保障服务，类似于保险业务。</p><h2 id="1-JA共済とは（JA共济是什么）"><a href="#1-JA共済とは（JA共济是什么）" class="headerlink" title="# 1 ＪＡ共済とは（JA共济是什么）"></a># 1 ＪＡ共済とは（JA共济是什么）</h2><div style="position: relative; width: 95%; height: 0; left: 2.5%; padding-bottom: 65%;"><br>    <iframe src="https://www.bilibili.com/blackboard/html5mobileplayer.html?aid=418566655&bvid=BV1xV41147yw&page=1&high_quality=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;"> </iframe><br></div><h2 id="2-JA共済の保障とは（JA共济的保障是什么）"><a href="#2-JA共済の保障とは（JA共济的保障是什么）" class="headerlink" title="# 2 ＪＡ共済の保障とは（JA共济的保障是什么）"></a># 2 ＪＡ共済の保障とは（JA共济的保障是什么）</h2><div style="position: relative; width: 95%; height: 0; left: 2.5%; padding-bottom: 65%;"><br>    <iframe src="https://www.bilibili.com/blackboard/html5mobileplayer.html?aid=418566655&bvid=BV1xV41147yw&page=2&high_quality=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;"> </iframe><br></div><h2 id="3-ライフアドバイザーとは（生活顾问是什么）"><a href="#3-ライフアドバイザーとは（生活顾问是什么）" class="headerlink" title="# 3 ライフアドバイザーとは（生活顾问是什么）"></a># 3 ライフアドバイザーとは（生活顾问是什么）</h2><div style="position: relative; width: 95%; height: 0; left: 2.5%; padding-bottom: 65%;"><br>    <iframe src="https://www.bilibili.com/blackboard/html5mobileplayer.html?aid=418566655&bvid=BV1xV41147yw&page=3&high_quality=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;"> </iframe><br></div><h2 id="4-医療共済（医疗共济）"><a href="#4-医療共済（医疗共济）" class="headerlink" title="# 4 医療共済（医疗共济）"></a># 4 医療共済（医疗共济）</h2><div style="position: relative; width: 95%; height: 0; left: 2.5%; padding-bottom: 65%;"><br>    <iframe src="https://www.bilibili.com/blackboard/html5mobileplayer.html?aid=418566655&bvid=BV1xV41147yw&page=4&high_quality=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;"> </iframe><br></div><h2 id="5-そなエール（防范准备）"><a href="#5-そなエール（防范准备）" class="headerlink" title="# 5 そなエール（防范准备）"></a># 5 そなエール（防范准备）</h2><div style="position: relative; width: 95%; height: 0; left: 2.5%; padding-bottom: 65%;"><br>    <iframe src="https://www.bilibili.com/blackboard/html5mobileplayer.html?aid=418566655&bvid=BV1xV41147yw&page=5&high_quality=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;"> </iframe><br></div><h2 id="6-終身共済（终身共济）"><a href="#6-終身共済（终身共济）" class="headerlink" title="# 6 終身共済（终身共济）"></a># 6 終身共済（终身共济）</h2><div style="position: relative; width: 95%; height: 0; left: 2.5%; padding-bottom: 65%;"><br>    <iframe src="https://www.bilibili.com/blackboard/html5mobileplayer.html?aid=418566655&bvid=BV1xV41147yw&page=6&high_quality=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;"> </iframe><br></div><h2 id="7-メディフル（一時金）（Mediful-一次性补贴）"><a href="#7-メディフル（一時金）（Mediful-一次性补贴）" class="headerlink" title="# 7 メディフル（一時金）（Mediful 一次性补贴）"></a># 7 メディフル（一時金）（Mediful 一次性补贴）</h2><div style="position: relative; width: 95%; height: 0; left: 2.5%; padding-bottom: 65%;"><br>    <iframe src="https://www.bilibili.com/blackboard/html5mobileplayer.html?aid=418566655&bvid=BV1xV41147yw&page=7&high_quality=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;"> </iframe><br></div><h2 id="8-メディフル（祝金）（Mediful-健康红包）"><a href="#8-メディフル（祝金）（Mediful-健康红包）" class="headerlink" title="# 8 メディフル（祝金）（Mediful 健康红包）"></a># 8 メディフル（祝金）（Mediful 健康红包）</h2><div style="position: relative; width: 95%; height: 0; left: 2.5%; padding-bottom: 65%;"><br>    <iframe src="https://www.bilibili.com/blackboard/html5mobileplayer.html?aid=418566655&bvid=BV1xV41147yw&page=8&high_quality=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;"> </iframe><br></div><h2 id="9-My家財プラス（我的家产plus）"><a href="#9-My家財プラス（我的家产plus）" class="headerlink" title="# 9 My家財プラス（我的家产plus）"></a># 9 My家財プラス（我的家产plus）</h2><div style="position: relative; width: 95%; height: 0; left: 2.5%; padding-bottom: 65%;"><br>    <iframe src="https://www.bilibili.com/blackboard/html5mobileplayer.html?aid=418566655&bvid=BV1xV41147yw&page=9&high_quality=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;"> </iframe><br></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;「先輩とみなみちゃん」（前辈和美波酱）&quot;&gt;&lt;a href=&quot;#「先輩とみなみちゃん」（前辈和美波酱）&quot; class=&quot;headerlink&quot; title=&quot;「先輩とみなみちゃん」（前辈和美波酱）&quot;&gt;&lt;/a&gt;「先輩とみなみちゃん」（前辈和美波酱）&lt;/h1&gt;&lt;h2 i
      
    
    </summary>
    
      <category term="学习" scheme="/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="日语" scheme="/tags/%E6%97%A5%E8%AF%AD/"/>
    
      <category term="翻译" scheme="/tags/%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>日语课总结（1）</title>
    <link href="/2021/05/26/%E6%97%A5%E8%AF%AD%E8%AF%BE%E6%80%BB%E7%BB%93%EF%BC%881%EF%BC%89/"/>
    <id>/2021/05/26/日语课总结（1）/</id>
    <published>2021-05-26T15:31:42.000Z</published>
    <updated>2021-05-26T15:31:42.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="日语课总结（1）"><a href="#日语课总结（1）" class="headerlink" title="日语课总结（1）"></a>日语课总结（1）</h1><h2 id="表記（ひょうき）"><a href="#表記（ひょうき）" class="headerlink" title="表記（ひょうき）"></a>表記（ひょうき）</h2><ol><li>どんなことも、まずやってみようという精神（せいしん）が大事だ。</li><li>ごみの処理（しょり）にはお金がかかる。</li><li>みんな同じコップを使いますから、自分のコップに印（しるし）を付けてください。</li><li>胸に記（しる）して忘れない。</li><li>日記（にっき）</li><li>今日の夜、テレビでサッカーの試合を放送します。</li><li>私の趣味はお菓子を作ることです。</li><li>彼は三十七歳のとき、始めて小説を著（あらわ）した。</li><li>悲しみの感情を表（あらわ）した音楽。</li><li>危険（きけん）な症状を現す。</li><li>父の財産を兄と私で半分ずつ相続（そうぞく）した。</li><li>この腕時計（うでどけ）は一日に一分ずつ進む。</li><li>大雨でサッカーの決勝戦（けっしょうせん）は延期されることになった。</li><li>現場から犯人のものと思われる足跡（あしあと）が発見された。</li><li>このゲームは非常（ひじょう）に集中力が要（い）る。</li></ol><h2 id="言い換え類義"><a href="#言い換え類義" class="headerlink" title="言い換え類義"></a>言い換え類義</h2><ol><li>母親は息子の部屋のドアを　<strong>そっと／静かに</strong>　閉めた</li><li><strong>こっそり／こそこそ</strong></li><li>きちんと 整洁、恰当</li><li>ちゃんと 认真、规矩、整齐</li><li>確り（しっかり） 可靠、牢固</li><li>地震（じしん）の時は　<strong>冷静／落ち着いて</strong>　に行動しなければならない。</li><li><strong>急いで／慌（あわ）てて</strong></li><li>今年の梅雨（ばいう）が　<strong>長引（ながび）ている／なかなか終わらない</strong>　。</li><li>土地の問題をめぐる住民（じゅうみん）の対立の　<strong>根（ね）／原因</strong>　は深い。</li><li>専門用語（せんもんようご）の多い英語（えいご）を翻訳（ほんやく）するのは　<strong>厄介（やっかい）／面倒（めんどう）</strong>　なので、専門家に頼むんだ。</li><li>この争（あらそ）いの　<strong>根（ね）／原因</strong>　は一つだけではない。</li><li>からかうのは　<strong>よして／やめて</strong>　ください。</li><li>車を止める。</li><li>道路（どうろ）の石を退（ど）ける。</li><li>結婚式（けっこんしき）の招待状（しょうたいじょう）を送ったら、　<strong>続々（ぞくぞく）と／次々（つぎつぎ）に</strong>　返事が届いた。</li><li>順々（じゅんじゅん）に仕事をかたづける。</li><li>転々（てんてん）と各地（かくち）を巡業（じゅんぎょう）する。</li><li>度々注意したが聞き入れない。</li><li>選手（せんしゅ）たちはゴールに向かって　<strong>徐々（じょじょ）に／少しずつ</strong>　走るスピードを上げていった。</li><li>工事は　<strong>着々（ちゃくちゃく）と／順調に</strong>　進んでいる。</li><li>さんまがどっと市場（いちば）へ出回（でまわ）る。</li><li>彼女は　<strong>穏（おだ）やかな／静かな</strong>　表情（ひょうじょう）で話している。</li></ol><h2 id="語形成（ごけいせい）"><a href="#語形成（ごけいせい）" class="headerlink" title="語形成（ごけいせい）"></a>語形成（ごけいせい）</h2><ol><li>おいしいと評判（ひょうばん）のケーキを買いに行ったが、売り　<u><strong>切れて</strong></u>　いて買えなかった。</li><li>日本で　<u><strong>少子化（しょうしか）</strong></u>　が進んでいる原因の一つとして、結婚の時期が遅くなっていることがあげられる。</li><li>今回のエベントは、この会社に入って初めての　<u><strong>大</strong></u>　仕事だから、みんな張り切っている。</li><li>自転車で走っていたら、横道から子供が飛び　<u><strong>出して</strong></u>　きて、びっくりした。</li><li>母はいつも、肉、魚、野菜をバランスよく取り　<u><strong>入れて</strong></u>　料理を作ってくれる。</li><li>もう少しで頂上（ちょうじょう）というところまで行ったが、急に天気が悪くなったので仕方なく引き　<u><strong>返した</strong></u>。</li><li>子どもを引き　<u><strong>寄せる</strong></u>。</li><li>以前は英語なんて話せないと思い　<u><strong>込んで</strong></u>　いたが、海外からの客が増えて、どうしても話さないわけにはいかなくなった。</li><li>地球（ちきゅう）　<u><strong>温暖化（おんだんか）</strong></u>　の影響（えいきょう）だろうか、異常（いじょう）気象（きしょう）が続いている。</li><li>この仏像（ぶつぞう）は国宝（こくほう）だが、一般的（いっぱんてき）には　<u><strong>非</strong></u>　公開になっている。</li><li>タバコはやめたはずなのに、無　<u><strong>意識</strong></u>　に灰皿（はいざら）のある場所を探してしまう。</li><li>この会社では、新製品を発売するための準備が　<u><strong>着々と</strong></u>　進んでいる。</li></ol><h2 id="文脈規定（ぶんみゃくきてい）"><a href="#文脈規定（ぶんみゃくきてい）" class="headerlink" title="文脈規定（ぶんみゃくきてい）"></a>文脈規定（ぶんみゃくきてい）</h2><ol><li>呉呉（くれぐれ）もお大事に。</li><li>栄養（えいよう）の　<u><strong>バランス</strong></u>　を取るには、野菜をたくさん食べることです。</li><li>朝飯の　<u><strong>献立（こんだて）／メニュー</strong></u>　はみそ汁・たまご焼き・のり・香の物だ。</li><li>世界の何処（どこ）かで戦争（せんそう）が起こっている。平和（へいわ）を　<u><strong>維持</strong></u>　するのは難（むずか）しいことだ。</li><li>結論（けつろん）を出すために、話し合いの　<u><strong>焦点（しょうてん）</strong></u>　を絞（しぼ）りましょう。</li><li>首相（しゅしょう）はインタビューで政府の重要（じゅうよう）課題（かだい）に対する　<u><strong>見解</strong></u>　を明（あき）らかにする。</li><li>会費は5万円くらいと　<u><strong>見当をつける／を推測（すいそく）する</strong></u>。</li><li>事故（じこ）は　<u><strong>思いがけない</strong></u>　ところで起こることが多いから、一瞬の注意も怠（おこた）ることができない。</li><li>手入れを怠（おこた）ると故障（こしょう）する。</li><li>この国の女性の　<u><strong>平均寿命（へいきんじゅみょう）</strong></u>　は八十歳である。</li><li>この料理は野菜が　<u><strong>たっぷり</strong></u>　入っているので、体にいいですよ。</li><li>青空に山が　<u><strong>くっきり／はっきり</strong></u>　見える。</li><li>こっそり跡をつける。</li><li>ぐっすりと眠っている。</li><li>彼女は、この仕事では二十年のキャリアがある　<u><strong>ベテラン（veteran）</strong></u>　だ。</li><li>テレビのアンテナ（antenna）を取り付ける。</li><li>この道は狭いから、車が　<u><strong>擦（す）れ違う</strong></u>　のは難（むずか）しい。</li><li>レポートを書くために、世界の人口の　<u><strong>分布（ぶんぷ）</strong></u>　を調べてみた。</li><li>社会人は自分の　<u><strong>行動（こうどう）</strong></u>　に責任を持たなげればならない。</li><li>この荷物（にもつ）は上下（じょうげ）が　<strong>逆（さか）さま</strong>　にならないように注意して運（はこ）んでください。</li><li>入選作は　<u><strong>各々（おのおの）／銘々（めいめい）／それぞれ</strong></u>　優（すぐ）れている。</li><li>試験の前日はしっかり睡眠（すいみん）をとったほうがいい。</li></ol><h2 id="文法（ぶんぽう）"><a href="#文法（ぶんぽう）" class="headerlink" title="文法（ぶんぽう）"></a>文法（ぶんぽう）</h2><ol><li><strong>Vる + ほかない</strong><br>含义：除此 <strong>该动作</strong> 之外没别的办法。「Vる + しかない」的official版本。<br>例句：だれにも頼めないから、自分で <u><strong>やる</strong></u> ほかない。</li><li><strong>Vる + ものだ／ものではない</strong><br>含义：认为 <strong>该动作</strong> 为一般常识，这样的动作更好。语气较为强烈。<br>例句1： 学生は <u><strong>勉強する</strong></u> ものです。遊んでばかりいてはいけません。<br>例句2：日本では、女性に年齢を <u><strong>聞く</strong></u> ものではない。</li><li><strong>V／いAい／なAな + わけではない</strong><br>含义：认为 <strong>该动作/形容</strong> 不完全正确。<br>例句：日本人が全員、敬語が上手に <u><strong>話せる</strong></u> わけではない。</li><li><strong>Vる + ことはない</strong><br>含义：认为 <strong>该动作</strong> 没有必要。用于主观性的建议。<br>例句：彼のほうが悪いのだから、君が <u><strong>謝る</strong></u> ことはない。</li><li><strong>Vる／N + どころではない</strong><br>含义：表示现在不是 <strong>该动作/名词</strong> 发生的时候。语气较为强烈，带有责难或质疑。<br>例句：<br>A：こんばん、映画に行かない？<br>B：え？明日、テストなんだから、 <u><strong>行く／映画</strong></u> どころではないよ！</li><li><strong>Vる、Vない + ことだ</strong><br>含义：认为 <strong>该动作</strong> 比较好、重要。用于主观性建议。<br>例句：日本語が上手になりたかったら、毎日、日本人と <u><strong>練習する／会話する</strong></u> ことだ。</li><li><strong>V／いAい／なAな／Nの + わけがない</strong><br>含义：表示对于 <u><strong>该动作/形容/名词</strong></u> 的强烈否定。语气带有强烈怀疑、质疑。<br>例句：<br>A：旅行に行かない？<br>B：えっ？今、コロナなんだから、旅行に <u><strong>行く</strong></u> わけがないでしょう。</li><li><strong>Vる、Vない + ことになっている</strong><br>含义：表示 <strong>该动作</strong> 是被规则、计划等决定好的。<br>例句1：日本では、二十歳（はたち）未満でお酒を飲んでは <u><strong>いけない</strong></u> ことになっている。<br>例句2：能力試験は、7月は中止になったら、12月は <u><strong>行（おこな）う</strong></u> ことになっている。</li><li><strong>N + からして</strong><br>含义：表示仅仅看 <strong>该名词</strong> ，不看其他方面，就可以得出一些结论。用于表达 <strong>该名词</strong> 的重要性亦或是做事的最低限度。<br>例句：あのコンビニのバイト、新人だね。 <u><strong>あいさつ</strong></u> からして、きちんとしていないよ。</li><li><strong>V／いAい／なAな／Nの + あまり</strong><br>含义：表示 该动作/形容/名词 过分到超过了一般情况。常用于表示 <strong>该动作</strong> 做的过了， <strong>该名词（情感）</strong> 过剩。<br>*余（あま）り。<br>例句：田中さんは、彼にふられて、 <u><strong>悲しみ</strong></u> のあまり、ご飯を食べなくなった。</li><li><strong>V／いAい／なAである／Nである + ことから</strong><br>含义：表示 <strong>该动作/形容/名词</strong> 是后文的原因。<br>例句：ここから、よく星が <u><strong>見える</strong></u> ことから、ここは星見町という名前になった。</li><li><strong>V／いAい／なAである／Nである + からには</strong><br>含义：表示 <strong>该动作/形容/名词</strong> 是后文的原因（当然でしょう）。语气中包含着强烈的 決意（けつい），可理解为「既然 … 就一定要 … 」。<br>例句：<u><strong>約束した</strong></u> からには、守らなければならない／守るべきだ。</li><li><strong>Nの + かわりに</strong><br>含义：表示后文动作本应由 <strong>该名词</strong> 完成，作为  <strong>该名词</strong> 的替代，主语完成了该动作。<br>例句：学会（がっかい）には、忙しい先輩のかわりに私が出席（しゅっせき）した。</li><li><strong>Nの + ことだから</strong><br>含义：在较为了解 <strong>该名词</strong> 的情况下，推测 <strong>该名词</strong> 很有可能导致后文的动作发生。<br>例句：よく遅（おく）れる <u><strong>田中さん</strong></u> のことだから、今日も、遅（おく）れるだろう。</li><li><strong>Vます／N + がちだ</strong><br>含义：表示 <strong>该动作/名词</strong> 发生的次数很多或者很容易发生。<br>例句：彼女は小さいときからよく病気する。→彼女は <u><strong>病気</strong></u> がちだ。</li><li><strong>N + だらけ</strong><br>含义：表示 <strong>该名词</strong> 大量出现。使用场景仅限于眼见的实体。<br>例句：この作文は間違いがたくさんある。→この作文は <u><strong>まちがい</strong></u> だらけだ。</li><li><strong>Vて／N + ばかり</strong><br>含义：表示只会一直进行 <strong>该动作/名词</strong> 不做别的。常用于责难的时候，带有不满的语气。<br>例句：彼はいつもゲームを <u><strong>して</strong></u> ばかりいる。＝彼はいつも <u><strong>ゲーム</strong></u> ばかりしている。</li><li><strong>Vる + べきだ</strong><br>含义：表示 <strong>该动作</strong> 是当然应该做的。常用于提出强烈的主张和忠告。<br>例句1：子供でも、悪いことをしたら、 <u><strong>謝る</strong></u> べきだ。<br>例句2：結婚式に白い服を <u><strong>着る</strong></u> べきではない。</li><li><strong>V／いAい／なAな + わけだ</strong><br>含义：表示 <strong>该动作</strong> 是有理由的。常用于表示前后文关系的合理性。<br>例句：<br>A：ジョンさんはアメリカ人だけど、お母さんが日本人なんだって。<br>B：それで、日本語が <u><strong>わかる／話せる</strong></u> わけだ。</li><li><strong>N1をN2として</strong><br>含义：把 <strong>名词1</strong> 当作是 <strong>名词2</strong>。<br>例句：今日は、 <u><strong>ベトさん</strong></u> を <u><strong>先生</strong></u> として、ベトナム語を勉強しましょう。</li><li><strong>Vる、Vている、Vない／いAい／なAな／Nの + うちに</strong><br>含义：趁着 <strong>该动作/形容/名词</strong> 的时间，去做后文的动作。<br>* の間に<br>例句1：子供が <u><strong>ねている</strong></u> うちに、そうじしよう／買い物に行こう。<br>例句2：<u><strong>覚えている／忘れない</strong></u> うちに、メモしよう。</li><li><strong>Vる、Vた／Nの + ついでに</strong><br>含义：<strong>该动作/名词</strong> 的顺便做了后文的动作。<br>例句：スーパーへ <u><strong>行く</strong></u> ついでに、銀行／郵便局に寄った。</li><li><strong>V／いAい／なAなである／Nである + わりに</strong><br>含义：表示在 该动作/形容/名词 的前提下，后文的事件与预想不符。通常用于前后文的正负相关性与预想不符。<br>例句：あのレストランは、値段が <u><strong>安い</strong></u> わりに、量が多い。</li><li><strong>Vます形 + 得る</strong><br>含义：表示 该动作是可能发生的。official版本。不能用于表示能力。<br>例句1：コロナは、だれでも、 <u><strong>なり／かかり</strong></u> うる病気だ。<br>例句2：私は1キロ（<strong>泳げる</strong>・<del>泳ぎうる</del>）。</li><li><strong>Vます形 + っこない</strong><br>含义：表示 该动作 绝对不可能发生。口语。<br>例句：こんなにたくさんの言葉、一日じゃ、 <u><strong>覚えられ</strong></u> っこないよ。</li><li><strong>Vる／Nの + おそれがある</strong><br>含义：表示 <strong>该动作</strong> 恐怕有可能发生。通常 <strong>该动作</strong> 为负面结果。<br>例句：地震の後は、津波（つなみ）が <u><strong>くる</strong></u> おそれがあります。</li><li><strong>Vます形 + つつある</strong><br>含义：表示正在逐渐向 <strong>该动作</strong> 的方向变化。<strong>该动作</strong> 通常是具有改变、成为等含义的动词。<br>例句：台風で、風が強くなりつつあります。</li><li><strong>Vた + すえに</strong><br>含义：表示后文是 <strong>该动作</strong> 的结果。这个结果的发生通常需要 <strong>该动作</strong> 经过较长时间。<br>例句：<u><strong>考えた</strong></u> すえに、国に帰ることに決めました。</li><li><strong>Vる、Vている、Vた + ところ</strong><br>含义：表示 <strong>即将要、正在、刚刚</strong> 做了 <strong>该动作</strong> 的时候，就发生了后文的动作。</li></ol><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>这一节课不会的东西也太多了，全记成电子笔记的话工程量有点遭不住啊。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;日语课总结（1）&quot;&gt;&lt;a href=&quot;#日语课总结（1）&quot; class=&quot;headerlink&quot; title=&quot;日语课总结（1）&quot;&gt;&lt;/a&gt;日语课总结（1）&lt;/h1&gt;&lt;h2 id=&quot;表記（ひょうき）&quot;&gt;&lt;a href=&quot;#表記（ひょうき）&quot; class=&quot;head
      
    
    </summary>
    
      <category term="学习" scheme="/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="日语" scheme="/tags/%E6%97%A5%E8%AF%AD/"/>
    
  </entry>
  
</feed>
